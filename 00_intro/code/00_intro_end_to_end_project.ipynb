{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un projet Data en 8 √©tapes, ma cl√© de lecture :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Bien d√©finir le probl√®me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que vous ayez un profil technique, la gestion de projet fera partie de votre m√©tier.\n",
    "\n",
    "Chaque projet se doit d'avoir un probl√®me bien cadr√© sinon vous allez dans le mur!\n",
    "Si vous n'√™tes pas en mesure de d√©finir le probl√®me vous ne serez pas √† quoi r√©pondre et donc vous ne pourrez rien d√©velopper ou alors vous r√©pondez √† cot√© dans 90% des cas!\n",
    "\n",
    "Pour ce faire vous pouvez suivre les recommandations suivantes :\n",
    "\n",
    "- Explorer la probl√©matique qui vous est soumise.\n",
    "   Qu'elle est le probl√®me?\n",
    "   Pourquoi le probl√®me existe, qu'est ce que cela engendre?\n",
    "   Quelles sont les solutions pour y r√©pondre aujourd'hui?\n",
    "   Mesure-t-on le probl√®me aujourd'hui? Si pas de donn√©es pour le mesurer il sera compliqu√© pour vous de prouver √† post√©riorio que votre am√©liore am√©liore quoique ce soit.\n",
    "   Qui est impact√© par ce probl√®me?\n",
    "\n",
    "   Int√©rogez les personnes qui gravitent autour de probl√®me et ne vous lancez pas directement dans les donn√©es!\n",
    "\n",
    "\n",
    "   A l'image d'un architecte, plus les plans de votre projet seront pr√©cis plus il sera facile de le d√©velopper apr√®s.\n",
    "\n",
    "\n",
    "   A la fin de l'√©tape de d√©finition du probl√®me vous serez en mesure :\n",
    "\n",
    "   D√©finir le probl√®me, de le mesure, d'expliquer votre solution et ses impactes.\n",
    "\n",
    "   N'h√©sitez pas √† prototyper vos solutions avant de lancer un d√©vellopement. \n",
    "   Parfois une feuille et un crayon permettent de tester votre solution avant de la d√©velloper.\n",
    "\n",
    "   Pensez √©galement √† consulter r√©guli√©rement la personne qui vous a expos√© le probl√®me afin de ne pas trop vous √©loigner de ses attentes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Trouver les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que vous avez un 'plan', vous savez comment r√©pondre th√©oriquement au probl√®me mais il va falloir se confronter √† la r√©alit√©.\n",
    "\n",
    "Vous allez devoir trouver les donn√©es dont vous avez besoin pour r√©pondre √† votre probl√®matique:\n",
    "\n",
    "- Lister les donn√©es dont vous avez besoin\n",
    "- Trouver un interlocteur ou un document vous explicant o√π sont les donn√©es/ comment elles sont g√©n√©r√©es \n",
    "- Cr√©er vous un nouvel espace de travail( un espace par projet)\n",
    "- V√©rifier les obligations l√©gales relatives √† vos donn√©es (RGDP, Techniques, fuites de donn√©es, ...)\n",
    "- Demander des autorisations (si besoin)\n",
    "- Commencer √† regarder le type des donn√©es dont vous avez besoin (Image, texte, tabulaire, temporelle, g√©ographique,...)\n",
    "- Cr√©er un code automatisable pour r√©cup√©rer vos donn√©es\n",
    "- Structurer votre jeu de donn√©es pour que ce soit simple par la suite\n",
    "    - Format des donn√©es\n",
    "    - Nom des colonnes\n",
    "    - Restriction sur votre p√©rim√®tre\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Explorer les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie vous allez essayer de faire ressortir les *insights* de vos donn√©es  \n",
    "üìù *Pensez automatisation, si vous rajouter des nouvelles donn√©es vous ne devez pas recoder l'analyse*\n",
    "\n",
    "- Cr√©er une copie de votre dataset pour travailler dessus (diminuer le taille si il est trop volumineux)\n",
    "- Pour de l'exploration jupyternotebook est tr√®s bien! (on l'oubliera pour le passage en production)\n",
    "- Analyser vos donn√©es de fa√ßon descripive\n",
    "    - .describe()\n",
    "    - html report pandas\n",
    "- Modifier le type de vos donn√©es si n√©cessaire\n",
    "- Pour une analyse supervis√©e, identifier la variable cible (target)\n",
    "- Visualiser les donn√©es\n",
    "- Etudier les corr√©lations\n",
    "- R√©fl√©chissez √† comment vous pourriez le r√©soudre le probl√®me en tant que humain sans coder.  \n",
    "  Quelles informations utiliseriez-vous? Comment le feriez-vous?  \n",
    "  Apr√®s l'avoir fait, essayer de transposer ca en code  \n",
    "- Commencer votre *feature engineering* pour cr√©er des nouvelles features\n",
    "- Retourner √† l'√©tape 2 s'il vous manque des donn√©es\n",
    "- Documenter vos trouvailles, documenter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Pr√©parer le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Travailler sur une copie du data set \n",
    "\n",
    "- Ecrivez des functions et pas du code non r√©utilisable\n",
    "    - Ca vous servira pour vos prochains projets\n",
    "    - Ca rend le code plus fonctionnel\n",
    "    - Ca facilite le traitement des donn√©es pour la prochaine mise √† jour des donn√©es  \n",
    "    \n",
    "\n",
    "- 1) **Data cleaning** (outliers, NA value, ...)  \n",
    "\n",
    "\n",
    "- 2) **Feature selection**(si besoin)  \n",
    "\n",
    "    - Etudes des corr√©lations\n",
    "    - Variable d'importances\n",
    "    - M√©thode de Shapley\n",
    "    - Stats descriptives\n",
    "\n",
    "\n",
    "- 3) **Feature engineering** adatp√© √† vos besoins  \n",
    "\n",
    "    - Discretiser vos donn√©es continues\n",
    "    - Recoder variables cat√©gorielles\n",
    "    - Ajouter des transformations de features\n",
    "    - Aggr√©ger des features  \n",
    "\n",
    "\n",
    "- 4) **Feature scaling** \n",
    "\n",
    "    - Standardiser ou normaliser vos features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Explorer des mod√®les et d√©terminer une short-list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Entrainer des mod√®les avec les hyperparam√®tres par d√©faut  \n",
    "      (Si  dataset avec n tr√®s grand, utiliser des √©chantillons)\n",
    "     *Des mod√®les avec des paradigmes diff√©rents (regressions, arbres, svm, neural net, xgboost,...*\n",
    "     \n",
    "     \n",
    "- 2) Mesurer les performances de chaque mod√®le\n",
    "     *Utiliser une cross-validation avec n-fold*\n",
    "     \n",
    "     \n",
    "- 3) Analyser les variables d'importances pour chaque mod√®les\n",
    "\n",
    "\n",
    "- 4) Analyser les erreurs du mod√®les\n",
    "\n",
    "\n",
    "- 5) R√©aliser une liste des features pertinents\n",
    "\n",
    "\n",
    "- 6) Essayer de changer d'am√©liorer rapidement vos pr√©c√©dents mod√®les\n",
    "\n",
    "\n",
    "- 7) Garder une liste des 3 meilleurs mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Tuner les mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous allez maintenant utilisez l'ENSEMBLE des vos donn√©es pour obtenir le meilleur mod√®le possible\n",
    "\n",
    "- 1) Tunez vos mod√®les en utilisant une crossvalidation\n",
    "\n",
    "    - Par exp√©rience, je vous conseil de traiter votre feature engineering comme un hyperparam√®tre.\n",
    "      Surtout si vous n'√™tes pas s√ªr de votre strat√©gie (ie, imputation NA, r√©unification Data, ...)\n",
    "      \n",
    "    - Random grid, search, bayesian grid search\n",
    "    \n",
    "- 2) Si vos mod√®les offres des performances faibles, testez les mod√®les ensemblistes\n",
    "\n",
    "- 3) Quand votre mod√®le est suffisament performant sur le training test, mesurer sa performance avec le test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Pr√©senter votre solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Documentez votre projet\n",
    "     *Pensez bien √† expliquer les choix que vous avez fait*\n",
    "\n",
    "- 2) Cr√©er une pr√©sentation sympa (pas de word SVP)\n",
    "     *Mettez en avant les informations importantes*\n",
    "     \n",
    "- 3) Expliquer concr√®tement comment votre projet r√©pond au besoin business (besoin de d√©part)\n",
    "\n",
    "- 4) Pensez √† comment vous aller vendre votre projet!\n",
    "     Si vous n'√™tes pas dans une entreprise tech, il sera parfois compliqu√© de prouver que votre mod√®le est utile.\n",
    "     Faites de la com, soyez imaginatif\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Automatiser votre mod√®le, monitorer votre mod√®le et maintenir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Pr√©parer votre code pour passer en production \n",
    "\n",
    "- 2) Pr√©parer un monitoring de votre code\n",
    "\n",
    "    - Suivre la performance de votre mod√®le (KPI)\n",
    "    - Suivre que votre mod√®le s'excute bien\n",
    "    - V√©rifier que le mod√®le ne se d√©gragde pas\n",
    "    - Mesurer qu'il n'y a pas de d√©rive sur vos donn√©es\n",
    "    \n",
    " - 3) Faites r√©guli√®rement des points avec le business pour prouver que votre solution am√©liore la situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept utile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le cas le plus comment de donn√©es d√©siquilibr√©e est une classification binaire.\n",
    "\n",
    "Prenons l'exemple d'un fraude √† la carte bancaire.  \n",
    "Nous avons un data set contenant 1 million d'op√©ration bancaire. La fraude √©tant un √©l√©ment rare (heuresement) notre data set ne contient que 10 000 fraude pour 990 000 non fraude.\n",
    "\n",
    "Si nous entrainons un mod√®le de machine learning pour une classifcation binaire sur ce projet, il sera incapable d'apprendre ce qu'est une fraude car nous ne lui pr√©senterons pas suffisament d'exemple pour qu'il arrive √† d√©finir une fraude.\n",
    "\n",
    "Imaginons tout de m√™me que nous entrainions tout de m√™me un mod√®le logit sur ce data set.  \n",
    "**Le mod√®le donne une accuracy de 89%** \n",
    "> Est-ce une bonne nouvelle, le mod√®le est-il pertinent?\n",
    "\n",
    "On aurait tendance √† dire oui car 89% de bonne pr√©diction semble √™tre une valeure √©lev√©e mais 89% est plus faible qu'une pr√©diction na√Øve...\n",
    "\n",
    "Si on cr√©er un algo qui dit syst√©matiquement qu'un op√©ration n'est pas une fraude il aura raison √† 99% du temps 99000/1000000.\n",
    "\n",
    "\n",
    "Si vous √™tes confronter √† ce genre de probl√®me, vous pouvez utiliser les m√©thodos suivantes :\n",
    "\n",
    "- Upsampling : Augmenter le nombre de l'√©v√©nement rare avec un tirage al√©atoire avec replacement\n",
    "- Downsampling : Diminuer le nombre de l'√©v√©nement non rare en retirant des cas\n",
    "- Oversampling : Algorithme ROSE ou SMOTE cr√©ant artificielment de nouveaux cas rare\n",
    "\n",
    "> Lien pour smote et rose https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le scaling feature (mettre vos donn√©es √† la m√™me √©chelle) permet d'exprimer diff√©rentes features avec diff√©rentes grandeurs num√©riques dans une m√™me unit√©s.\n",
    "\n",
    "\n",
    "Il exite 2 grandes familles pour le feature scaling :\n",
    "\n",
    "- La normalisation\n",
    "\n",
    "- La standardisation\n",
    "\n",
    "Les 2 permettent d'exprimer les colonnes num√©riques dans une m√™me unit√©s, am√©liorer le temps de calcul des mod√®les et pour certains mod√®les avec de meilleur performance.\n",
    "\n",
    "#### Normalisation\n",
    "\n",
    "La normalisation est le fait de transformation vos features dans une √©chelle [0,1]. On l'appelle parfois *min-max scaling*.\n",
    "Sa formule est la suivante\n",
    "\n",
    "$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Exmple\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler() \n",
    "print(scaler.fit(data))\n",
    "```\n",
    "\n",
    "#### Standardisation\n",
    "\n",
    "La standardisation est une technique qui permet quant √† elle de transformer nos colonnes en variable avec une moyenne de 0 et un √©cart type de 1. Les colonnes transform√©es auront donc les m√™mes param√®tres de distribution.\n",
    "La standardisation pr√©sente des avantages quand il existe des outliers, comme on utilise pas la valeur min et max, la technique y est moins sensible!\n",
    "\n",
    "$z = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "```python \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Quand vous calculez les param√®tres de votre mod√®les vous avez 2 possibilit√©s :\n",
    "\n",
    "\n",
    "- Utiliser une r√©solution math√©matique pour obtenir la solution optimale (exemple, r√©solution MCO reg lin√©aire)  \n",
    "\n",
    "*Equation normale*\n",
    "\n",
    "- Utiliser une r√©solution d'optimitsation successive appell√©e **Descent de Gradient** qui va chercher it√©rativement les param√®tres qui minise la fonction de co√ªt du mod√®le\n",
    "\n",
    "\n",
    "> Plus d'information ici https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach\n",
    "\n",
    "\n",
    "Concr√©tement vous commencez avec un param√®tr $\\theta$ donn√© et vous allez le faire varier it√©rativement en fonction de la valeur de sa d√©riv√©e.  \n",
    "On peut l'observer graphiquement sur le graphique N¬∞1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞1 :Descente de gradient</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/master/00_intro/img/descente_gradient_1.png?token=GHSAT0AAAAAABZOBGASQJQ5WDKPI3YBYA3IY3CYWCA\" alt=\"fig_2_intuition_svm.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Chaque point rouge repr√©sente une it√©ration de descente de gradient et converge vers le minimun global de la fonction de perte.  \n",
    "Nous obtenons en ce point pour un param√®tre $\\theta$ dont la valeur minise notre fonction de perte. \n",
    "\n",
    "\n",
    "<u>Graphique N¬∞2 :Descente de gradient, learning rate trop faible</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/master/00_intro/img/descente_gradient_2.png?token=GHSAT0AAAAAABZOBGATYCCCAVANYEDMAHTCY3CYY6Q\" alt=\"fig_2_intuition_svm.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Il est important que la taille de 'saut' de mise √† jour de la valeur de votre param√®tre $\\theta$ ne soit pas trop faible.\n",
    "On appelera le param√®tre qui contr√¥le le 'saut' **LEARNING RATE**.  \n",
    "Si celui-ci est trop faible vos 'sauts' seront petits il faudra beaucoup d'it√©rations avant de trouver le param√®tre optimal.\n",
    "\n",
    "\n",
    "<u>Graphique N¬∞3 :Descente de gradient, learning rate trop haut</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/master/00_intro/img/descente_gradient_3.png?token=GHSAT0AAAAAABZOBGASJVGZSIW23ZBCCRT6Y3CY5OA\" alt=\"fig_2_intuition_svm.png\" style=\"width:600px;\"/>\n",
    "\n",
    "A l'inverse si le *LEARNING RATE* est trop √©lev√© vous pourriez ne jamais trouver l'optimal de votre fonction.  \n",
    "Le calcul divergera et ne trouvera jamais de minimun local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un peu de math pour comprendre comment la descente de gradient marche.\n",
    "C'est un concept fondamental pour les algorithmes de machine learning!!\n",
    "\n",
    "Exemple descente de gradient avec fonction de co√ªt MSE pour un mod√®le lin√©aire:\n",
    "\n",
    "On d√©finit un fonction lin√©aire avec un vecteur de param√®tre $\\theta$  \n",
    "$\\widehat{y} = \\theta_0 + \\theta_1x1+...+\\theta_nxn$\n",
    "\n",
    "o√π  :\n",
    "\n",
    "- $\\theta_0 : Biais\\space du\\space modele $\n",
    "- $\\theta_n : Param√®tre \\space du \\space mod√®le$\n",
    "- $\\widehat{y} : Valeure\\space pr√©dite$\n",
    "- $ n : Nombre \\space de \\space features$\n",
    "\n",
    "\n",
    "En forme vectorielle ca donne l'√©quation suivante\n",
    "\n",
    "$ \\widehat{y} = h_\\theta(x) = \\theta.X$\n",
    "\n",
    "On d√©finit la fonction de perte de ce mod√®le comme :\n",
    "\n",
    "$MSE(X, h_\\theta) = \\frac{1}{N}\\sum_{i=1}^N (y_i-\\widehat{y_i})¬≤$\n",
    "\n",
    "Pour impl√©menter la descente de gradient, vous devez calculer le gradient de la fonction de co√ªt MSE en fonction de ses param√®tres $\\theta$\n",
    "On doit donc calculer toutes les d√©riv√©es partielles de la fonction MSE\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} MSE(\\theta) = \\frac{2}{N}\\sum_{i=1}^N (\\theta^Tx^{(i)}-y^{(i)})x^{(i)}_j$\n",
    "  \n",
    "ou au format vectoriel\n",
    "\n",
    "$\\nabla_\\theta MSE(\\theta) = \\begin{pmatrix}  \\frac{\\partial}{\\partial \\theta_0} \\\\ \\frac{\\partial}{\\partial \\theta_1} \\\\ . \\\\\\frac{\\partial}{\\partial \\theta_n}  \\end{pmatrix} =\\frac{2}{N}X^T (X\\theta-y)$  \n",
    "\n",
    "Une fois que vous avez le vecteur de descente de gradient, vous devez simplement mettre √† jour vos param√®tre $\\theta$ jusqu'√† atteindre le minimun de votre fonction.\n",
    "\n",
    "$\\theta^{(next)} = \\theta - \\eta\\nabla_\\theta MSE(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple Descente de gradient\n",
    "Un exemple en dimension 1 pour mieux comprendre üòÄ\n",
    "\n",
    "Nous avons une fonction  $f(x) = 3x^2 -2x +5 $ et nous souhaitons minimiser cette fonction\n",
    "\n",
    "<u>Graphique N¬∞4 :Exemple descente de gradient</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/master/00_intro/img/exemple_grad_1D_4.png?token=GHSAT0AAAAAABZOBGATOM24LDELJ53HLCDQY3C4SIQ\" alt=\"fig_2_intuition_svm.png\" style=\"width:500px;\"/>\n",
    "\n",
    "Etape 1 : On calcule son vecteur gradient \n",
    "\n",
    "En dimension le vecteur est de taille 1, donc on calcul uniquement une d√©riv√©e\n",
    "\n",
    "$f'(x) = 6x -2x $\n",
    " \n",
    "Etape 2 : On initialise une valeur de $x$ par d√©faut et une valeur pour le learning rate\n",
    "\n",
    "On pose $x_0 = 5$ et $\\eta = 0.05$\n",
    "\n",
    "La formule pour les √©tapes de descente de gradient en D1 est donc :\n",
    "$x_{n+1} = x_n -\\eta*f'(x_n)$\n",
    "\n",
    "Etape 3 : It√©ration sucessive d√©scente de gradient\n",
    "\n",
    "<u>Graphique N¬∞5 :Exemple descente de gradient</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/master/00_intro/img/descente_grad_exemple_5.png?token=GHSAT0AAAAAABZOBGATDNVWXSKSYDGE55KQY3C5XZA\" alt=\"fig_2_intuition_svm.png\" style=\"width:500px;\"/>\n",
    "\n",
    "\n",
    "Successivement la valeur de $\\theta$ se rapproche de la valeur de $x=\\frac{1}{3}$ qui minise la fonction.\n",
    "Quand vous utiliserez l'hyperparam√®tre **learning rate** pour un algo de machine learning c'est exactement ca qui se passera en back.\n",
    "\n",
    "> Vous savez maintenant ce qu'est la descente de gradient, bravo !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS function VS Metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liste √† voir avec eux\n",
    "\n",
    "Gradient Descent  XX\n",
    "Learning rate  XX\n",
    "Hyperparameter  \n",
    "Loss function  \n",
    "Computational complexity  \n",
    "Grid search  \n",
    "Scaling features  X\n",
    "Evaluation metrics  \n",
    "Imbalanced dataset X\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
