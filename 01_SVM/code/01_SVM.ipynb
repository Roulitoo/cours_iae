{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>SVM sous Python</h1></center>\n",
    "<img src=\"https://github.com/Roulitoo/cours_iae/blob/master/00_intro/img/Logo_IAE_horizontal.png\" alt=\"Logo IAE.png\" style=\"width:200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "[1. Pr√©ambule](#Etapes-d'un-projet-Data)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1 Iris flower ](#)<br>\n",
    "\n",
    "[2. Support Vecteur Machine](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.1 Presentation intuitive d'un SVM](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2 Calcul de la marge ](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3 Maximisation de la marge](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4 SVM LINEAIRE, HARDS MARGING VS SOFT MARGIN CLASSIFCATION](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.5 SVM non li√©naire](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.5.1 Polynomial Kernel](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.5.2 Similarity Features](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.6 Classification Multiclass pour les SVM](#)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.6.1 Application classication multiclass, dataset MNIST ](#)<br>\n",
    "\n",
    "[3. SVM pour la r√©gression](#)<br>\n",
    "[4. R√©capitulatif](#)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Pr√©ambule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connaissez-vous le fameux jeu de donn√©es IRIS produit par Ronald Fisher en 1936??  \n",
    "Si non, on nous allons y rem√©dier!  \n",
    "\n",
    "Nous pouvons acc√©der √† ce dataset depuis sklearn avec le code suivant :\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "# Array to Pandas DataFrame\n",
    "iris = pd.DataFrame(np.c_[data['data'],\n",
    "                          data['target']\n",
    "                         ],\n",
    "                    columns = data.feature_names + ['species']\n",
    "                   )\n",
    "\n",
    "iris['species'] = iris['species'].astype('int')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de donn√©es Iris est un jeu de donn√©es regroupant 3 esp√®ces de plantes :\n",
    "\n",
    "- La Setosa\n",
    "- La Versicolore\n",
    "- La Virginica\n",
    "\n",
    "Pour chaque plante nous avons mesur√© en cm 4 caract√©ristiques.\n",
    "La longueur de ses p√©tales et s√©pales (visible sous la photo ci-dessous) ainsi que leur largeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-Iris flower\n",
    "\n",
    "    \n",
    "![iris_photo](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le jeu de donn√©es Iris pour voir comment les informations sont repr√©sent√©es.  \n",
    "Les 3 esp√®ces ont √©t√© recod√©es de la mani√®re suivante :\n",
    "\n",
    "**0 = setos**\n",
    "\n",
    "**1 = versicolor** \n",
    "\n",
    "**2 = virginica**\n",
    "\n",
    "On compte 50 lignes par esp√®ces pour un jeu de donn√©es au format (150,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Tableau N¬∞1 :  5 premieres lignes du jeu de donn√©es IRIS </u>\n",
    "\n",
    "| sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | species |\n",
    "|------------------:|-----------------:|------------------:|-----------------:|--------:|\n",
    "|               5.1 |              3.5 |               1.4 |              0.2 |       0 |\n",
    "|               4.9 |              3.0 |               1.4 |              0.2 |       0 |\n",
    "|               4.7 |              3.2 |               1.3 |              0.2 |       0 |\n",
    "|               4.6 |              3.1 |               1.5 |              0.2 |       0 |\n",
    "|               5.0 |              3.6 |               1.4 |              0.2 |       0 |\n",
    "    \n",
    "\n",
    "Le tableau comporte 4 features(colonnes) ainsi que l'esp√®ce de la fleur.  \n",
    "\n",
    "Regardons √©galement sur le graphique N¬∞1 la dispersion de la longueur des p√©tales en fonction de leur largeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞1 :  R√©partition des Iris en fonction de la longueur et largueur de ses p√©tales</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_1_iris_scatter_y3.png?token=GHSAT0AAAAAABZOBGASBLUFH4KFDPKMDBLCY2ZS7VA\" alt=\"fig_1_iris_scatter_y3.png\" style=\"width:800px;\"/>\n",
    "\n",
    "On remarque que les points rouges ont une distribution tr√®s diff√©rente des autres.  \n",
    "Pour les points gris et oranges on voit √©galement qu'ils appartiennent √† 2 distributions distinctes mais la fronti√®re entre les 2 est plus mince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>2-Support Vecteur Machine</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1-Presentation intuitive d'un SVM\n",
    "\n",
    "\n",
    "Prenons un exemple de classification binaire avec le data set Iris.  \n",
    "Pour faire simple nous utiliserons seulement 2 features (plus simple √† repr√©senter)\n",
    "\n",
    "Nous d√©cidons de conserver la longueur du petal et la largeur du petal( colonnes 3 & 4) \n",
    "\n",
    "ü§î  \n",
    "Imaginons maintenant qu'on nous demande de tracer une fonction permettant de s√©parer nos 2 nuages de point par une fronti√®re.  \n",
    "**Comment feriez-vous?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous cherchons une fonction de la forme :<br>\n",
    "$f(x) = X^T\\beta = x1\\beta1 + x2\\beta2 + ... + \\beta0 =0$\n",
    "\n",
    "Cette fonction de permettre de d√©terminer :\n",
    "\n",
    "$f(x)>0 : classe: 1$<br>\n",
    "$f(x)<0 : classe:0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞2 :Fronti√®re de d√©cision pour classification binaire</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_2_intuition_svm.png?token=GHSAT0AAAAAABZOBGATTNP5G5DXIMM2AD72Y2ZTZPQ\" alt=\"fig_2_intuition_svm.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Pour ce probl√®me, il existe une **infinit√© de solutions**. Comment en d√©terminer une optimale??\n",
    "\n",
    "Une premi√®re piste est qu'en machine learning, notre objectif est de r√©alisation une pr√©diction.  \n",
    "Pour cela, il faut donc que notre mod√®le soit **g√©n√©ralisable**. Un point qui ne figure pas dans les donn√©es d'entrainement, devra √™tre bien classifi√©.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc apporter une contrainte √† notre fronti√®re de d√©cision $f(x)$.\n",
    "Elle ne devra pas √™tre trop proche des points en distance.  \n",
    "Plus elle sera proche plus on aura de chance qu'un nouveau point issu de la distribution 0 ou 1 soit mal classifi√©.\n",
    "\n",
    "**Autrement dit, nous devons trouver une contrainte √† notre fonction afin d'apporter une solution unique.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Gprahique N¬∞3 : Calculer une distance </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/03_calcul_distance.png?token=GHSAT0AAAAAABZOBGATQDJTU72UB5OMPXQ4Y2ZT5LA\" alt=\"03_calcul_distance\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rappel la distance entre un point et une droite se calcule de la mani√®re suivante pour un espace √† 2 dimensions\n",
    "\n",
    "<u>Equation N¬∞1 : Distance dans d'un point √† une droite dans R¬≤</u><br>\n",
    "\n",
    "La distance entre une droite $D$ d'√©quation $ax+by+c = 0$ et un point $P$ de coordonn√©es $(x1,y1)$ est\n",
    "\n",
    "$\\normalsize d(D,P) = \\frac{\\vert{ax+by+c}\\vert}{\\sqrt{a¬≤+b¬≤}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut g√©n√©raliser ce calcul de distance pour trouver les marges du SVM.\n",
    "\n",
    "On va **chercher le ou les points** qui permettent de **maximiser l'√©cart entre la marge et la fronti√®re de d√©cision** (hyperplan) tout en **minimisant l'√©cart entre la marge √† un des points d'entra√Ænement**\n",
    "\n",
    "Si on le formalise cela donne la formule suivante:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2-Calcul de la marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Equation N¬∞2 : Formule de calcul SVM </u>\n",
    "\n",
    "Si on transpose notre exemple de R¬≤ √† notre probl√®me de SVM on obtient :\n",
    "\n",
    "L'√©quation de la fronti√®re de d√©cision not√© $H$ est : $f(x) = \\lt\\beta.x \\gt= \\beta^Tx+b$ \n",
    "\n",
    "L'√©cart entre la fronti√®re de d√©cision et une marge est alors not√©e:$\\large\\frac{(\\beta^Tx+b)}{\\vert\\vert\\beta\\vert\\vert_2}$<br>\n",
    "Comme il y a 2 marges on obtient la formule suivante :\n",
    "\n",
    "$\\normalsize Marge = 2d(x,H) =2\\frac{(\\omega^T\\beta+b)}{\\vert\\vert\\omega\\vert\\vert_2}$\n",
    "\n",
    "\n",
    "o√π :<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\beta :$ param√®tre du mod√®le <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\vert\\vert\\beta\\vert\\vert_2$ d√©signe la norme euclidienne de $\\beta$ : $\\sqrt{\\beta_1¬≤+\\beta_2¬≤+\\beta_3¬≤+...+\\beta_n¬≤}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°Pour rappel, la marge est la distance minimale de l‚Äôhyperplan √† un des points d‚Äôentra√Ænement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3-Maximisation de la marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut trouver l‚Äôhyperplan de support qui permet de maximiser cette marge, c‚Äôest-√†-dire qu‚Äôon veut trouver un hyperplan avec la plus grande marge possible.\n",
    "<br>\n",
    "Puisque l‚Äôon cherche l‚Äôhyperplan qui maximise la marge, on cherche l‚Äôunique hyperplan dont les param√®tres $(\\beta,b)$ sont donn√©s par la formule‚ÄØ:\n",
    "<br>\n",
    "\n",
    "$\\large arg max_{\\beta,b} min_k \\frac{l_k(\\beta^Tx+b)}{\\vert\\vert\\beta\\vert\\vert_2}$\n",
    "\n",
    "o√π $l_k$ est le label de la donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞4 : Fronti√®re de d√©cision et marges, SVM lin√©aire</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_4_visualisation_svm_lineaire.png?token=GHSAT0AAAAAABZOBGASGEY5ZSO6KNSISS36Y2ZUGFQ\" alt=\"fig_4_visualisation_svm_lineaire\" style=\"width:600px;\"/>\n",
    "    \n",
    "Sur le graphique N¬∞4, on peut observer les points fronti√®res qui maximisent l'√©cart entre la marge et la fronti√®re de d√©cision.\n",
    "Ici on peut dire que la fronti√®re de d√©cision est bonne.  \n",
    "Elle est suffisamment large pour que la probabilit√© qu'un point soit mal classifi√© est peu probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù  **Pour info**\n",
    "- On appelle les points prochent des marges ==> **Points edges** ou **points supports**\n",
    "- La fonction qui s√©pare nos 2 ensembles de points est une **fronti√®re de d√©cision**\n",
    "- Les droites proche des points edges sont les **marges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì**Questions**\n",
    "\n",
    "Que se passe-t-il si je rajoute une observation? \n",
    "Ma fronti√®re de d√©cision change-t-telle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è**Attention**\n",
    "\n",
    "Notez qu'il est important de toujours standardiser vos donn√©es lorsque vous utilisez des SVM.  \n",
    "De mani√®re g√©n√©rale quand vous utilisez des mod√®les avec calcul de distance, pensez √† standardiser vos donn√©es.\n",
    "   \n",
    "   Pour cela vous avez plusieurs op√©rations math√©matiques pour mettre vos donn√©es √† la m√™me √©chelle, voici les principaux √† retenir :\n",
    "   \n",
    "   - **Min Max Scaling**\n",
    "   - **Normalization** \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "Prenons le jeu de donn√©es suivant avec $x_1$ et $x_2$ des features associ√© √† un label {0,1}.  \n",
    "On a √©galement rajout√© la standardisation de nos 2 features $x_1scaled$ et $x_2scaled$\n",
    "\n",
    "Nous lan√ßons un SVM lin√©aire pour tenter de classifier les labels 0 et 1 en fonction de ces features.\n",
    "Un premier avec les features brutes et l'autre avec les features standardis√©s\n",
    "\n",
    "Regardons maintenant sur le graphique N¬∞ comment la standardisation impact la fronti√®re de d√©cision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Tableau N¬∞2 : Feature scaling</u>\n",
    "\n",
    "\n",
    "\n",
    "| x1 | x2 | x1_scaled | x2_scaled | label |\n",
    "|----|----|-----------|-----------|-------|\n",
    "| 1  | 50 | -1.507    | -0.115    | 0     |\n",
    "| 5  | 20 | 0.904     | -1.5010   | 0     |\n",
    "| 3 | 80  | -0.301    | 1.270     | 1     |\n",
    "| 5  | 60 | 0.904     | 0.346     | 1     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞5 : Influence de l'√©chelle des donn√©es sur le mod√®le </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_5_scaling_data.png?token=GHSAT0AAAAAABZOBGATNCBW3H74K63FTNESY2ZUQGA\" alt=\"fig_5_scaling_data\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.red { background-color:#ff000020; border-radius: 5px; padding: 20px;}\n",
    "</style>\n",
    "<div class = \"red\">\n",
    "üìù  <strong>Pour info</strong>\n",
    "    \n",
    "Les SVM sont sensibles √† l'√©chelle des donn√©es. Il est important de standardiser ses donn√©es avant d'entrainer le mod√®le.\n",
    "\n",
    "$$\n",
    "  Xscale = \\frac{X-\\mu}{\\sigma}\\ \n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4-SVM LINEAIRE, HARDS MARGING VS SOFT MARGIN CLASSIFCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade tout se passe bien et la classification d'un SVM semble √™tre parfaite pour notre jeu de donn√©es.\n",
    "\n",
    "Cependant il est tr√®s rare qu'un jeu de donn√©es soit lin√©airement s√©parable... pour ainsi dire jamais avec des donn√©es d'entreprise.\n",
    "Pour l'instant nous avons vu ce qu'on appelle un SVM √† *hard margin classification*. Chaque individu doit √™tre d'un cot√© de la zone de d√©cision.\n",
    "\n",
    "Autrement dit, on ne peut pas trouver une versicolore du cot√© d'une setosa\n",
    "\n",
    "**Mais cel√† pose 2 probl√®mes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons l'exemple du graphique N¬∞6.\n",
    "\n",
    "Il est impossible de r√©aliser une classification lin√©aire √† l'aide d'un SVM.\n",
    "L'outlier emp√™che de trouver une fronti√®re de d√©cision qui permettrait de classifier parfaitement nos 2 groupes.\n",
    "\n",
    "Dans le 2nd cas, visible sur le graphique N¬∞6, l'outlier est tellement √©loign√© du point moyen de son groupe qu'il r√©duit drastiquement l'√©cart des marges du SVM.\n",
    "<br>\n",
    "La fronti√®re de d√©cision ne sera pas optimale et il sera compliqu√© de g√©n√©raliser ce mod√®le √† d'autres individus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞6 : Donn√©es non lin√©aire et SVM </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_6_svm_linear_problem.png?token=GHSAT0AAAAAABZOBGAS6PBYQVT6TKJJQGXIY2ZUTXQ\" alt=\"fig_6_svm_linear_problem\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour √©viter ce genre de probl√®me, les statisticiens ont d√©velopp√© un mod√®le plus flexible.\n",
    "Son objectif est de trouver un √©quilibre entre la maximisation des marges et le nombre de fois o√π l'on peut ignorer un point.\n",
    "(nombre de points du mauvais cot√©, mal classifi√©)\n",
    "\n",
    "Le mod√®le s'appelle le *soft margin classification* en opposition au *hard margin classification* vu plus haut.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La soft margin classification fait intervenir un nouveau param√®tre dans le mod√®le. On l'appelle param√®tre de r√©gularisation $C$.\n",
    "Il est √† valeurs dans $]0,\\infty[$.\n",
    "On parle ici d'hyperparam√®tre car sa valeur optimale n'est pas fix√©e mais d√©pend du jeu de donn√©es. C'est √† vous de le trouver pour optimiser votre mod√®le\n",
    "\n",
    "Regardons comment faire avec le code python suivant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Import Package\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "# Code\n",
    "\n",
    "#Standardiser nos donn√©es\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Entrainement mod√®le\n",
    "# Loss='hinge' permet de dire qu'on utile un SVM classique\n",
    "# Vous pouvez regarder la doc pour avoir + d'infos\n",
    "svm_clf= LinearSVC(loss='hinge', C=1)\n",
    "svm_clf.fit(X_scaled, y)\n",
    "\n",
    "#Prediction pour un nouveau point\n",
    "svm_clf.predict([[1,1]])\n",
    "#==> array([1.])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plus on augmente la valeur de $C$ plus le mod√®le va avoir tendance √† produire des marges proches de la fronti√®re de d√©cision et √† l'inverse plus $C$ est petit plus la fronti√®re sera grande.**\n",
    "<br>\n",
    "Pour mieux le comprendre, regardons le graphique N¬∞ . Cela repr√©sente 3 SVM entrain√©s avec 3 valeurs diff√©rentes de $C$ √† savoir 1, 50 et 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code suivant\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "svm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "svm_clf2 = LinearSVC(C=50, loss=\"hinge\", random_state=42)\n",
    "svm_clf3 = LinearSVC(C=500, loss=\"hinge\", random_state=42)\n",
    "\n",
    "scaled_svm_clf1 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf1),\n",
    "    ])\n",
    "scaled_svm_clf2 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf2),\n",
    "    ])\n",
    "\n",
    "scaled_svm_clf3 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf3),\n",
    "    ])\n",
    "\n",
    "scaled_svm_clf1.fit(X, y)\n",
    "scaled_svm_clf2.fit(X, y)\n",
    "scaled_svm_clf3.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞7 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_7_regularisation_critere.png?token=GHSAT0AAAAAABZOBGASPSRSPNQDAEM3QKSOY2ZS3KA\" alt=\"fig_7_regularisation_critere\" style=\"width:1400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les autres hyperparamtres fix√©s, une augmentation de C permet de diminuer la taille des marges. \n",
    "Plus la taille de la marge sera faible plus il sera compliqu√© de g√©n√©raliser pour le mod√®le\n",
    "<br>\n",
    "‚ÑπÔ∏è Le SVM lin√©aire est √©galement disponible √† travers la fonction **SGDClassifier** de sklearn. La diff√©rence essentielle provient de l'optimiseur utilis√©. Ici la fonction utilise une descente de gradient qui peut s'av√©rer utile dans le cas de dataset avec beaucoup de ligne (n grand)\n",
    "\n",
    "On l'utilise de la m√™me mani√®re\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, loss='hinge', alpha=0.001))\n",
    "#alpha=1/(n*C)\n",
    "clf.fit(X, Y)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5-SVM non li√©naire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classification lin√©aire est tr√®s utile et peut s'av√©rer pr√©cise pour de nombreux dataset. Malheureusement quand on se confronte √† des donn√©es r√©elles et non pas un dataset kaggle, les donn√©es sont rarement lin√©airement s√©parables.\n",
    "\n",
    "Les mod√®les lin√©aires fournissent des performances assez faibles et ne permettent pas de r√©pondre √† votre probl√®me.\n",
    "Heureusement, il existe des techniques pour faire √©voluer les SVM et traiter les cas o√π les donn√©es ne sont pas lin√©airement s√©parables.\n",
    "\n",
    "Prenons l'exemple intuitif suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞8 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_8_SVM_non_lineaire.png?token=GHSAT0AAAAAABZOBGASMJJWQECQ3ULPKKDGY2ZS37A\" alt=\"fig_8_SVM_non_lineaire\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce probl√®me est un cas r√©current en machine learning. Nous cherchons √† classifier des donn√©es mais les features disponibles ne permettent pas de le faire.\n",
    "\n",
    "C'est un cas d'√©cole de **feature engineering**.\n",
    "Ici il faut transformer nos donn√©es brutes de telle sorte qu'on puisse classifier nos donn√©es lin√©airement apr√®s transformation.\n",
    "\n",
    "Une transformation possible est d'ajouter un feature qui serait $X_2 = X_1¬≤$\n",
    "\n",
    "Regardons graphiquement le r√©sultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞9 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_9_svm_separation_lineaire.png?token=GHSAT0AAAAAABZOBGATM6V77DASKHUIZVZYY2ZS4HA\" alt=\"fig_9_svm_separation_lineaire\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant une transformation qui nous fait passer d'un probl√®me √† 1D √† 2D, on trouve un espace o√π nos donn√©es sont lin√©airement s√©parables.\n",
    "**Ce type de transformation est tr√®s utile pour les SVM mais s'applique √† tous les mod√®les de machine learning.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.red { background-color:#ff000020; border-radius: 5px; padding: 20px;}\n",
    "</style>\n",
    "<div class = \"blue\">\n",
    "\n",
    "Si vos donn√©es brutes offres des performances m√©diocres, pensez √† faire du feature engineering sur vos donn√©es.\n",
    "         \n",
    "- Appliquer des fonctions sur vos donn√©es pour en cr√©er des nouvelles (log, puissance, sigmoide, loi normale, ...)\n",
    "- Combiner des donn√©es $X_{new} = X_1*X_2$\n",
    "- Cr√©er de nouveau feature avec des mod√®les (ACP, ACM, AUTOENCODER, KNN)\n",
    "- Tester, soyez cr√©atif ;)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour impl√©menter ce type d'approche sklearn offre des fonctions toutes faites.\n",
    "Vous pouvez utiliser la fonction *PolynomiaFeatures* dans le module sklearn.preprocessing qui permet de faire des transformations polynomiales pour chaque feature num√©rique.\n",
    "\n",
    "```Python\n",
    "#Import package\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC\n",
    "#Function sklearn qui genere donn√©es en forme de lune\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "#Transformation polynomiale de nos features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_degr3 = poly.fit_transform(X)\n",
    "\n",
    "#Standardiser les donn√©es\n",
    "scaler = StandardScaler()\n",
    "X_degr3_scaled = scaler.fit_transform(X_degr3)\n",
    "\n",
    "#SVM classification\n",
    "\n",
    "polynomial_svm_clf_test= LinearSVC(C=1, loss='hinge', random_state=42)\n",
    "polynomial_svm_clf_test.fit(X_degr3_scaled,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞10 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_10_classification_non_lineaire.png?token=GHSAT0AAAAAABZOBGASDXBX7YBJM23ZMKN6Y2ZS4NA\" alt=\"fig_10_classification_non_lineaire\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1-Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'approche par transformation des features convient parfaitement pour des datasets de petite taille et peu complexe.\n",
    "En effet, une transformation polynomiale avec des degr√©s faibles ne pourra pas s'adapter √† un dataset complexe et des degr√©s trop √©lev√©s cr√©eront un dataset trop grand pour √™tre entrain√© dans un laps de temps int√©ressant.\n",
    "\n",
    "Rappel :\n",
    "\n",
    "Plus on augmente la taille d'un dataset (features ou lignes) plus le mod√®le devient complexe √† entrainer et plus le temps de calcul sera long...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heureusement, les math√©maticiens ont pens√© √† tout et il existe une astuce avec les SVM.\n",
    "On l'appelle le *kernel trick*. Cela permet d'obtenir les m√™mes r√©sultats qu'en ajoutant des *polynomial features* sans faire exploser la taille du dataset.\n",
    "\n",
    "Sans passer par la d√©monstration math√©matique, il faut retenir qu'utiliser le *kernel trick* permet d'avoir un mod√®le de complexit√© $O(n^d)$ vs $O(n)$.\n",
    "\n",
    "Cette technique est impl√©ment√©e directement dans sklearn avec la fonction suivante\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel=\"poly\", degree=3, C=50,coef0=1)\n",
    "svm.fit(X, y)\n",
    "\n",
    "#Degree ==> Degre polinomiale\n",
    "#Kernel ==> Le type de noyau\n",
    "# C ==> Param√®tre de tol√©rance (r√©gularisation)\n",
    "# coef0 ==> Contr√¥le l'infulence des polynomes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞11 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_11_svm_no_lineaire_hyperpara.png?token=GHSAT0AAAAAABZOBGASCWXYK4BIUEU5PLXIY2ZS4WQ\" alt=\"fig_11_svm_no_lineaire_hyperpara\" style=\"width:1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La facon la plus simple de trouver les hyperparam√®tres ad√©quats et de r√©aliser un *grid search*. Nous verrons en TD comment l'impl√©menter avec sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2-Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe de nombreux *kernel trick* utile pour trouver un espace √† plus grande dimension o√π nos donn√©es sont lin√©airement s√©parables.\n",
    "Vous pouvez aller voir celles qui sont impl√©ment√©es avec [sklearn](https://scikit-learn.org/stable/modules/svm.html#kernel-functions)\n",
    "\n",
    "La derni√®re que nous allons voir est une des plus populaires pour les SVM est la fonction de similarit√© *Gaussian Radial Basis Function*. Elle se d√©finit formellement de la fa√ßon suivante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation N¬∞\n",
    "\n",
    "$\\phi_\\gamma(x,x') = exp(-\\gamma\\vert\\vert x-x'\\vert\\vert¬≤)$\n",
    "\n",
    "o√π\n",
    "$x'$ = Un point rep√®re que nous choisisons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple :\n",
    "\n",
    "Prenons le cas du graphique 1D N¬∞...\n",
    "Nous prenons $x'$ = {-2,1} comme rep√®res et $x$ = -1 pour un $\\gamma =0.3$\n",
    "\n",
    "Nous obtenons donc les fonctions de similirat√©s suivantes pour 2 nouveaux features $x_2$ et $x_3$\n",
    "\n",
    "$x_2 = exp(-0.3*1¬≤) \\simeq  0.74 $\n",
    "\n",
    "$x_3 = exp(-0.3*2¬≤) \\simeq  0.3 $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞12 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_12_similarity_features.png?token=GHSAT0AAAAAABZOBGASD7UVYX4EJLOEUOFGY2ZS45A\" alt=\"fig_12_similarity_features\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les *polynomials features*, vous pouvez cr√©er √† la main vous-m√™mes les features que vous souhaitez rajouter dans votre dataset avec cette technique.\n",
    "Choissier autant de 'rep√®res' que vous avez de ligne dans votre dataset pour cr√©er de nouveaux features.\n",
    "\n",
    "**Probl√®me**, cette technique peut rapidement **faire exploser la taille de votre Dataset si $n$ est grand**. Vous obtiendrez √† la fin un Dataframe de taille $n*n$ (en supposant que vous supprimez les features de bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heureusement pour nous sklearn propose une impl√©mentation optimis√©e dans ses fonctions pour cette approche. Exactement comme pour les *polynomial features*.\n",
    "\n",
    "On retrouve le *kernel trick* dans la fonction  SVC de sklearn avec comme noyau(kernel) 'rbf' pour Radial Basis Function kernel vu au-dessus.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel=\"rbf\",  gamma=5, C=0.001)\n",
    "svm.fit(X, y)\n",
    "\n",
    "#gamma rend la distribution plus √©troite ce qui donne des fronti√®res de d√©cisions plus irr√©guli√®res\n",
    "#chaque observations influences plus la fronti√®re de d√©cision\n",
    "\n",
    "# C ici ne change pas. Il est toujours un cri√®tre de tol√©rance\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez √©galement que gamma comme C est un hyperparam√®tre permettant de r√©gulariser le mod√®le.\n",
    "Si votre mod√®le est en *overfitting* pensez √† r√©duire gamma/C et inversement s'il est en *underfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞12 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/13_rbf_kernel.png?token=GHSAT0AAAAAABZOBGAT6JHMTZ72HVIGNAP6Y2ZS5GA\" alt=\"13_rbf_kernel\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.blue { background-color: rgba(117, 190, 218, 0.5); border-radius: 5px; padding: 20px;}\n",
    "</style>\n",
    "<div class = \"blue\">\n",
    "\n",
    "Tips :\n",
    "Parfois vos donn√©es ne sont pas au format num√©rique et le *kernel trick* n√©cessite un noyau sp√©cifique.\n",
    "Il faut savoir qu'il existe des noyaux adapt√©s pour diff√©rentes structures de donn√©es :\n",
    "    \n",
    "- String kernel pour la classification de text par exemple ( cf *string subsequence kernel* ou *Levenshtein distance*)\n",
    "    \n",
    "    \n",
    "Comment choisir son *kernel trick* parmis ceux disponibles?\n",
    "Il n'y a pas de r√®gle √©crite qui permet de choisir directement. La meilleur r√©ponse est ca d√©pend de vos donn√©es.\n",
    "    \n",
    "Mais le sch√©ma suivant marche g√©n√©ralement bien:\n",
    "    \n",
    "- 1, commencer par un SVM lin√©aire dispo avec la fonction LinearSVC\n",
    "- 2, si le training set n'est tr√®s grand vous pouvez utiliser le noyau *Gaussian RBF kernel*\n",
    "- 3, si vous avez du temps, testez d'autres noyaux mais 1 et 2 est g√©n√©ralement suffisant pour voir si les SVM sont adapt√©s au probl√®me\n",
    "\n",
    "‚ö†Ô∏è Pensez bien √† *tuner* votre mod√®le avec un *gridsearch* et une *cross-validation* avant de comparer vos mod√®les avec votre test set\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6-Classification Multiclass pour les SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreux mod√®les permettent nativement de r√©aliser des classifications multiclasse (Random Forest, SGBDclassifiers, Naives Bayes, ...).\n",
    "Malheureusement les SVM n'en font pas partie.\n",
    "\n",
    "Pour les ustiliser lors de classifications multiclasse, nous devons trouver une parade!\n",
    "\n",
    "#### Exemple, classification de chiffre manuscrit\n",
    "\n",
    "##### Contexte\n",
    "Imaginons qu'on nous donne un dataset contenant des images. Chaque image repr√©sente un chiffre manuscrit entre 0 et 9.\n",
    "On nous demande de cr√©er un mod√®le bas√© sur un SVM afin de classifier ces chiffres manuscrits (l'humain qui le fait habituellement en √† marre de le faire).\n",
    "\n",
    "Le data scientist en charge du projet √† bien compris la probl√©matique mais sait aussi qu'un SVM ne permet pas de faire de la classification multiclasse...\n",
    "Il cherche alors une strat√©gie pour r√©pondre parfaitement √† la commande.\n",
    "\n",
    "##### Solution\n",
    "Sa **premiere intuition** est de **d√©couper le probl√®me en 10 probl√®mes distincts.**\n",
    "10 classifications binaires o√π il va chercher √† identifier les 1 VS les autres puis les 2 VS les autres etc.\n",
    "Apr√®s les avoir entrain√©s, il obtiendra le score de d√©cision gr√¢ce √† sklearn et prendra celui qui le maximise.\n",
    "\n",
    "Un coll√®gue lui souffle √©galement **une autre id√©e.**\n",
    "**R√©aliser des mod√®les par pair. Un mod√®le 1vs2, 1vs3, 1vs4,...2vs3,, ... etc**\n",
    "Il essaye cette approche mais obtient **45 mod√®les** diff√©rents pour ce probl√®me.\n",
    "\n",
    "\n",
    "C'est 2 approches sont appel√©s **OVR(one versus rest)** pour la premiere et **OVO(one versus one)** pour la seconde.\n",
    "Elles permettent d'approcher un probl√®me multiclasse avec une classification binaire.\n",
    "Leur principal d√©faut est de faire exploser le nombre de mod√®les √† entrainer.\n",
    "\n",
    "- **One versus rest** : On ram√®ne un probl√®me avec $N_{class}$ √† $N$ classification binaire. On compare une classe √† $N-1$ classe.<br>\n",
    "<br>\n",
    "- **One versus one** : On ram√®ne le probl√®me avec $N_{class}$ √† $N\\times\\frac{N-1}{2}$ classification binaire. Cette fois on teste toutes les combinaisons de N possibles divis√©es par 2. (Classification $N_1 | N_2$ est la m√™me que $N_2 | N_1$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'impl√©mentation avec sklearn est encore une fois chose facile\n",
    "##### OVR\n",
    "\n",
    "```python\n",
    "#Import OVO, OVR\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#Import SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "OvR_SVC_clf = OneVsRestClassifier(SVC())\n",
    "\n",
    "OvR_SVC_clf.fit(trainX, trainY)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1-Application classication multiclass, dataset MNIST\n",
    "\n",
    "Pour exemple, nous pouvons utiliser ces strat√©gies pour le jeu de donn√©es MNIST.\n",
    "C'est un jeu de donn√©es c√©l√®bre qui comprend 70 000 images de chiffre √©crit √† la main. Chaque image a √©t√© classifi√© et le jeu de donn√©es est parfait pour s'entrainer √† ce type de donn√©es\n",
    "\n",
    "Pour le charger utilisez la commande suivante sur python\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#Import SVM\n",
    "from sklearn.svm import SVC\n",
    "##########\n",
    "#GET DATA#\n",
    "##########\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "# dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',\n",
    "# 'categories', 'url'])\n",
    "\n",
    "X , y = mnist[\"data\"] , mnist['target']\n",
    "X.shape\n",
    "#(70000, 784)\n",
    "# Chaque image contient 784 features qui correspodent √† la distribution de ses pixels en nuance de gris.\n",
    "# Sa valeur est entre 0 et 255\n",
    "\n",
    "#############\n",
    "#Train model#\n",
    "#############\n",
    "# Suivant votre quantit√© de RAM, attention √† combien de ligne vous prenez pour entrainer votre mod√®le!!\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000],y[60000:]\n",
    "\n",
    "\n",
    "OvR_SVC_clf = OneVsRestClassifier(SVC())\n",
    "OvO_SVC_clf = OneVsOneClassifier(SVC())\n",
    "\n",
    "OvR_SVC_clf.fit(X_train, y_train) \n",
    "OvO_SVC_clf.fit(X_train , y_train)    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞ 14:  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_14_mnist.png?token=GHSAT0AAAAAABZOBGAT3NHVR4YEHYDD47TQY2ZS5OQ\" alt=\"fig_14_mnist\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>************************Demo avec le code 01_SVM_DEMO************************</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-SVM pour la r√©gression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derni√®re partie consacr√©e au SVM sera celle sur la r√©gression.\n",
    "Jusqu'√† maintenant nous avons vu les SVM pour la classification et il n'est pas n√©cessairement intuitif de voir comment l'appliquer √† la r√©gression (du moins th√©oriquement, ca vous prendra une ligne avec sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globalement les SVM pour la r√©gression sont aussi flexibles. Ils permettent de faire de la r√©gression *lin√©aire* et non *li√©naire* avec les m√™mes techniques vues pr√©c√©demment.\n",
    "La nuance est que nous devons inverser notre objectif! Ici on ne cherche plus √† maximiser la marge entre 2 classes tout en limitant le nombre de violations.\n",
    "\n",
    "Le mod√®le cherche √† inclure le maximum d'observation √† l'int√©rieur de ses marges tout en limitant le nombre d'observations √† l'ext√©rieur. La largeur de la fronti√®re de d√©cision sera contr√¥l√©e par un nouvel hyperparam√®tre $\\epsilon$.\n",
    "\n",
    "Regardons comme il agit √† travers 2 exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞15 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_15_svm_reg.png?token=GHSAT0AAAAAABZOBGATB75PQJFS25GWZEAIY2ZS5ZQ\" alt=\"fig_15_svm_reg\" style=\"width:700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez utilisez le code suivant pour l'impl√©menter sous python.\n",
    "La logique est la m√™me que pour la classfication üòâ\n",
    "\n",
    "```python \n",
    "#SVM Regression li√©naire\n",
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)\n",
    "\n",
    "#SVM Regression non li√©naire\n",
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-R√©capitulatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Tableau N¬∞3 : R√©sum√© des fonction utilis√©es</u>\n",
    "\n",
    "| Sklearn class | Scaling | Kernel trick | Hyperparameter                                          | Computational complexity | Multiclass                                | Sklearn import                                 |\n",
    "|--------------:|---------|-------------:|---------------------------------------------------------|-------------------------:|-------------------------------------------|------------------------------------------------|\n",
    "| SVC           | OUI     | OUI          | kernel,C                                            |          O(m^3n)         | OneVsRestClassifier<br>OneVsOneClassifier | from sklearn.svm import SVC                    |\n",
    "| LinearSVC     | OUI     | NON          | loss ='hinge',<br>C=> Tol√©rance                         |          O(m*n)          | OneVsRestClassifier<br>OneVsOneClassifier | from sklearn.svm import LinearSVC              |\n",
    "| SGDClassifier | OUI     | NON          | loss = 'hinge',<br>max_iter,<br>Alpha ==> Learning rate |          O(m*n)          | OneVsRestClassifier<br>OneVsOneClassifier | from sklearn.linear_model import SGDClassifier |\n",
    "| LinearSVR     | OUI     | NON          |  Epsilon , C                                                      |          O(m*n)          |                                           | from sklearn.svm import LinearSVR              |\n",
    "| SVR           | OUI     | OUI          |Epsilon,  C , kernel                                                        |          O(m^3n          |                                           | from sklearn.svm import SVR                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avantages des SVM**<br>\n",
    "- Ils marchent relativement bien sur les dataset avec peu de features<br>\n",
    "- Ils fournissent une s√©paration clair de vos donn√©es. On peut interpr√©ter le mod√®le.<br>\n",
    "- Ils s'adaptent tr√®s bien au dataset avec plus de features que de data points.<br>\n",
    "- On peut sp√©cifier diff√©rent $kernel$ pour trouver une meilleur fronti√®re de d√©cision<br>\n",
    "\n",
    "**D√©savantages des SVM** \n",
    "- Leur entrainement n√©cessite beaucoup de temps de calul et de ressource.<br> \n",
    "- Ce n'est pas recommand√© de les utilis√© si vous avez un dataset grand (millions de lignes)<br>\n",
    "- Ils sont tr√®s sensibles aux outliers<br>\n",
    "- Ne supportent pas nativement le multiclass<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Si vous souhaitez un suppl√©ment de cours plus math√©matique vous pouvez consulter le lien [suivant](https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "> La page wikip√©dia de [Vapnik](https://fr.wikipedia.org/wiki/Vladimir_Vapnik) l'inventeur du SVM\n",
    "\n",
    "> Une video avec Vladmir Vapnik sur le [l'apprentissage automatique](https://www.youtube.com/watch?v=STFcvzoxVw4&ab_channel=LexFridman) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
