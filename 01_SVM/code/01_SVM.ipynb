{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "chdir('C://Users/roulB/data_science/cours/01_SVM/img/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><h1>SVM sous Python</CENTER>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table des mati√®res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©ambule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connaissez-vous le fameux jeu de donn√©es IRIS produit par Ronald Fisher en 1936??  \n",
    "Si non, on nous allonsy rem√©dier tout de suite!  \n",
    "\n",
    "Nous pouvons acc√®der √† ce dataset depuis sklearn avec le code suivant :\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "# Array to Pandas DataFrame\n",
    "iris = pd.DataFrame(np.c_[data['data'],\n",
    "                          data['target']\n",
    "                         ],\n",
    "                    columns = data.feature_names + ['species']\n",
    "                   )\n",
    "\n",
    "iris['species'] = iris['species'].astype('int')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de donn√©es Iris est un jeu de donn√©es regroupant 3 esp√®ces de plantes :\n",
    "\n",
    "- La Setosa\n",
    "- La Versicolore\n",
    "- La Virginica\n",
    "\n",
    "Pour chaque plante nous avons mesur√© en cm 4 caract√©ristiques.\n",
    "La longueur de ses p√©tales et s√©pales (visible sous la photo ci-dessous) ainsi que leur largeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris flower\n",
    "<center> Exemple d'une fleur Iris\n",
    "    \n",
    "![iris_photo](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le jeu de donn√©es Iris pour voir comment les informations sont repr√©sent√©es.  \n",
    "Les 3 esp√©ces ont √©t√© recod√©es de la mani√®re suivantes :\n",
    "\n",
    "**0 = setos**\n",
    "\n",
    "**1 = versicolor** \n",
    "\n",
    "**2 = virginica**\n",
    "\n",
    "On compte 50 lignes par esp√®ces pour un jeu de donn√©es au format (150,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Tableau N¬∞1 :  5 premieres lignes du jeu de donn√©es IRIS </u>\n",
    "\n",
    "| sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | species |\n",
    "|------------------:|-----------------:|------------------:|-----------------:|--------:|\n",
    "|               5.1 |              3.5 |               1.4 |              0.2 |       0 |\n",
    "|               4.9 |              3.0 |               1.4 |              0.2 |       0 |\n",
    "|               4.7 |              3.2 |               1.3 |              0.2 |       0 |\n",
    "|               4.6 |              3.1 |               1.5 |              0.2 |       0 |\n",
    "|               5.0 |              3.6 |               1.4 |              0.2 |       0 |\n",
    "    \n",
    "\n",
    "Le tableau comporte 4 features(colonnes) ainsi que l'esp√®ce de la fleur.  \n",
    "\n",
    "Regardons √©galement sur le graphique N¬∞1 la dispersion de la longueur des p√©tales en fonction de leur largeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞1 :  R√©partition des Iris en fonction de la longueur et largueur de ses p√©tales</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_1_iris_scatter_y3.png?token=GHSAT0AAAAAABZOBGASBLUFH4KFDPKMDBLCY2ZS7VA\" alt=\"fig_1_iris_scatter_y3.png\" style=\"width:800px;\"/>\n",
    "\n",
    "On remarque que les points rouges ont une distribution tr√®s diff√©rentes des autres.  \n",
    "Pour les points gris et oranges on voit √©galement qu'ils appartiennent √† 2 distributions distinctes mais la fronti√®re entre les 2 est plus minces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation intuitive d'un SVM\n",
    "\n",
    "\n",
    "Prenons un exemple de classification binaire avec le data set Iris.  \n",
    "Pour faire simple nous utiliserons seulement 2 features (plus simple √† repr√©senter)\n",
    "\n",
    "Nous d√©cidons de conserver la longueur du petal et la largeur du petal( colonnes 3 & 4) \n",
    "\n",
    "ü§î  \n",
    "Imaginons maintenant qu'on nous demande de tracer une fonction permettant de s√©parer nos 2 nuages de point par une fronti√®re.  \n",
    "**Comment feriez vous?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous cherchons une fonction de la forme :\n",
    "$$\n",
    "f(x) = X^T\\beta = x1\\beta1 + x2\\beta2 + ... + \\beta0 =0\n",
    "$$\n",
    "\n",
    "Cette fonction de permettre de d√©terminer :\n",
    "\n",
    "$$  \n",
    "f(x)>0 : classe: 1\n",
    "$$\n",
    "$$f(x)<0 : classe:0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞2 :Fronti√®re de d√©cision pour classification binaire</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_2_intuition_svm.png?token=GHSAT0AAAAAABZOBGATTNP5G5DXIMM2AD72Y2ZTZPQ\" alt=\"fig_2_intuition_svm.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Pour ce probl√®me, il existe une **infinit√© de solution**. Comment en d√©terminer une optimale??\n",
    "\n",
    "Une premi√®re piste est qu'en machine learning, notre objectif est de r√©alisation une pr√©diction.  \n",
    "Pour cela, il faut donc que notre mod√®le soit **g√©n√©ralisable**. Un point qui ne figure pas dans les donn√©es d'entrainement, devra √™tre bien classifi√©.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc apporter une contrainte √† notre fronti√®re de d√©cision $f(x)$.\n",
    "Elle ne devra pas √™tre trop proche des points en terme de distance.  \n",
    "Plus elle sera proche plus on aura de chance qu'un nouveau point issu de la distribution 0 ou 1 soit mal classifi√©.\n",
    "\n",
    "**Autrement dit, nous devons trouver une contrainte √† notre fonction afin d'apporter une solution unique.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Gprahique N¬∞3 : Calculer une distance </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/03_calcul_distance.png?token=GHSAT0AAAAAABZOBGATQDJTU72UB5OMPXQ4Y2ZT5LA\" alt=\"03_calcul_distance\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rappel la distance entre un point et une droite se calcule de la mani√®re suivante pour un espace √† 2 dimensions\n",
    "\n",
    "\n",
    "$$\n",
    "d = \\frac{\\vert{ax+by+c}\\vert}{\\sqrt{a¬≤+b¬≤}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut g√©n√©raliser ce calcul de distance pour trouver les marges du SVM.\n",
    "\n",
    "On va **chercher le ou les points** qui permettent de **maximiser l'√©cart entre la marge et la fronti√®re de d√©cision** (hyperplan) tout en **minimisant l'√©cart entre la marge √† un des points d'entra√Ænement**\n",
    "\n",
    "Si on le formalise cela donne la formule suivante:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul de la marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{l_k(\\omega^T\\beta+b)}{\\vert\\vert\\omega\\vert\\vert_2} \n",
    "$$\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "$$\n",
    "\\vert\\vert\\omega\\vert\\vert_2 : \\sqrt{\\omega_1¬≤+\\omega_2¬≤+\\omega_3¬≤+...+\\omega_n¬≤}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O√π $\\vert\\vert\\omega\\vert\\vert_2$ designe la norme euclidienne de $\\omega$ et $l_k$ le label associ√© √† chaque $x_k$.\n",
    "<br>\n",
    "La marge d‚Äôun hyperplan de param√®tres $(\\omega,b)$ par rapport √† un ensemble de points $x_k$ est donc \n",
    "\n",
    "\n",
    "$$min(k)\\frac{l_k(\\omega^T\\beta+b)}{\\vert\\vert\\omega\\vert\\vert_2} $$\n",
    "\n",
    "\n",
    "Pour rappel, la marge est la distance minimale de l‚Äôhyperplan √† un des points d‚Äôentra√Ænement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximisation de la marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut trouver l‚Äôhyperplan de support qui permet de maximiser cette marge, c‚Äôest-√†-dire qu‚Äôon veut trouver un hyperplan avec la plus grande marge possible.\n",
    "<br>\n",
    "Puisque l‚Äôon cherche l‚Äôhyperplan qui maximise la marge, on cherche l‚Äôunique hyperplan dont les param√®tres $(\\omega,b)$ sont donn√©s par la formule‚ÄØ:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "arg max_{\\omega,b} min_k \\frac{l_k(\\omega^T\\beta+b)}{\\vert\\vert\\omega\\vert\\vert_2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞4 : Fronti√®re de d√©cision et marges, SVM lin√©aire</u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_4_visualisation_svm_lineaire.png?token=GHSAT0AAAAAABZOBGASGEY5ZSO6KNSISS36Y2ZUGFQ\" alt=\"fig_4_visualisation_svm_lineaire\" style=\"width:600px;\"/>\n",
    "    \n",
    "Sur le graphique N¬∞4, on peut observer les points fronti√®res qui maximise l'√©cart entre la marge et la fronti√®re de d√©cision.\n",
    "Ici on peut dire que la fronti√®re de d√©cision est bonne.  \n",
    "Elle est suffisament large pour que la probabilit√© qu'un point soit mal classifi√© est peu probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù  **Pour info**\n",
    "- On appelle les points coll√©s aux marges ==> **Points edges** ou **points supports**\n",
    "- La fonction qui s√©pare nos 2 ensembles de points est une **fronti√®re de d√©cision**\n",
    "- Les droites coll√®s aux point edges sont les **marges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì**Questions**\n",
    "\n",
    "Que se passe-t-il si je rajoute une observation? \n",
    "Ma fronti√®re de d√©cision change-t-telle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è**Attention**\n",
    "\n",
    "Notez qu'il est important de toujours standardiser vos donn√©es lorsque vous utilisez des SVM.  \n",
    "De mani√®re g√©n√©rale quand vous utilisez des mod√®les avec calcul de distance, pensez √† standardiser vos donn√©es.\n",
    "   \n",
    "   Pour cela vous avez plusieurs op√©rations math√©matiques pour mettre vos donn√©es √† la m√™me √©chelle, voici les principaux √† retenir :\n",
    "   \n",
    "   - **Min Max Scaling**\n",
    "   - **Normalization** \n",
    "   - **Standardization**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "Prenons le jeu de donn√©es suivant avec $x_1$ et $x_2$ des features associ√© √† un label {0,1}.\n",
    "On √©galement rajout√© la standardisation de nos 2 feature $x_1scaled$ et $x_2scaled$\n",
    "\n",
    "Nous lan√ßons un SVM lin√©aire pour tenter de classifier les labels 0 et 1 en fonction de ces features.\n",
    "Un premier avec les features brutes et l'autre avec les features standardis√©s\n",
    "\n",
    "Regardons maintenant sur le graphique N¬∞ comment la standardisation impact la fronti√®re de d√©cision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Tableau N¬∞2 : Feature scaling</u>\n",
    "\n",
    "| x1 | x2 | x1_scaled | x2_scaled | label |\n",
    "|----|----|-----------|-----------|-------|\n",
    "| 1  | 50 | -1.507    | -0.115    | 0     |\n",
    "| 5  | 20 | 0.904     | -1.5010   | 0     |\n",
    "| 3 | 80  | -0.301    | 1.270     | 1     |\n",
    "| 5  | 60 | 0.904     | 0.346     | 1     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞5 : Influence de l'√©chelle des donn√©es sur le mod√®le </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_5_scaling_data.png?token=GHSAT0AAAAAABZOBGATNCBW3H74K63FTNESY2ZUQGA\" alt=\"fig_5_scaling_data\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.red { background-color:#ff000020; border-radius: 5px; padding: 20px;}\n",
    "</style>\n",
    "<div class = \"red\">\n",
    "üìù  <strong>Pour info</strong>\n",
    "    \n",
    "Les SVM sont sensibles √† l'√©chelle des donn√©es. Il est important de standardiser ses donn√©es avant d'entrainer le mod√®le.\n",
    "\n",
    "$$\n",
    "  Xscale = \\frac{X-\\mu}{\\sigma}\\ \n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM LINEAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HARDS MARGING VS SOFT MARGIN CLASSIFCATION\n",
    "\n",
    "A ce stade tout se passe bien et la classification d'un SVM semble √™tre parfaite pour notre jeu de donn√©es.\n",
    "\n",
    "Cependant il est tr√®s rare qu'un jeu de donn√©es soit lin√©airement s√©parable... pour ainsi dire jamais avec des donn√©es d'entreprise.\n",
    "Pour l'instant nous avons vu ce qu'on appelle un SVM √† *hard margin classification*. Chaque individu doit √™tre d'un cot√© de la zone de d√©cision.\n",
    "\n",
    "Autrement dit on ne peut pas trouver une versicolore du cot√© d'une setosa\n",
    "\n",
    "**Mais cel√† pose 2 probl√®mes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons l'exemple du graphique N¬∞6.\n",
    "\n",
    "Il est impossible de r√©aliser une classification lin√©aire √† l'aide d'un SVM.\n",
    "L'outliers emp√™che de trouver une fronti√®re de d√©cision qui permettrait de classifier parfaitement nos 2 groupes.\n",
    "\n",
    "Dans le 2nd cas, visible sur le graphique N¬∞\n",
    "Ici l'outliers est tellement √©loign√© du point moyen de son groupe qu'il r√©duit drastiquement l'√©cart des marges du SVM.\n",
    "<br>\n",
    "La fronti√®re de d√©cision ne sera pas optimal et il sera compliqu√© de g√©n√©raliser ce mod√®le √† d'autres individus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Graphique N¬∞6 : Donn√©es non lin√©aire et SVM </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_6_svm_linear_problem.png?token=GHSAT0AAAAAABZOBGAS6PBYQVT6TKJJQGXIY2ZUTXQ\" alt=\"fig_6_svm_linear_problem\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour √©viter ce genre de probl√®mes, les statisticiens ont d√©velopp√©s un mod√®le plus flexible.\n",
    "Son objectif est de trouver un √©quilibre entre la maximisation des marges et le nombre de fois o√π l'on peut ignorer un point.\n",
    "(nombre de points du mauvais cot√©, mal classifi√©)\n",
    "\n",
    "Le mod√®le s'appelle le *soft margin classification* en opposition au *hard margin classification* vu plus haut.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ins√©rer meme enfin du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La soft margin classification fait intervenir un nouveau param√®tre dans le mod√®le. On l'appelle param√®tre de r√©gularisation $C$.\n",
    "Il est √† valeur dans $]0,\\infty[$.\n",
    "On parle ici d'hyperparam√®tre car sa valeur optimale n'est pas fix√©e mais d√©pend du jeu de donn√©es. C'est √† vous de le trouver pour optimiser votre mod√®le\n",
    "\n",
    "Regardons comment faire avec le code python suivant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Import Package\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "# Code\n",
    "\n",
    "#Standardiser nos donn√©es\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Entrainement mod√®le\n",
    "# Loss='hinge' permet de dire qu'on utile un SVM classique\n",
    "# Vous pouvez regarder la doc pour avoir + d'infos\n",
    "svm_clf= LinearSVC(loss='hinge', C=1)\n",
    "svm_clf.fit(X_scaled, y)\n",
    "\n",
    "#Prediction pour un nouveau point\n",
    "svm_clf.predict([[1,1]])\n",
    "#==> array([1.])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus on augmente la valeur de $C$ plus le mod√®le va avoir tendance √† produire des marges proches de la fronti√®re de d√©cision et √† l'inverse plus $C$ est petit plus la fronti√®re sera grande.\n",
    "<br>\n",
    "Pour mieux le comprendre, regardons le graphique N¬∞ . Cela repr√©sente 3 SVM entrain√©s avec 3 valeurs diff√©rentes de $C$ √† savoir 1, 50 et 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code suivant\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "svm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "svm_clf2 = LinearSVC(C=50, loss=\"hinge\", random_state=42)\n",
    "svm_clf3 = LinearSVC(C=500, loss=\"hinge\", random_state=42)\n",
    "\n",
    "scaled_svm_clf1 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf1),\n",
    "    ])\n",
    "scaled_svm_clf2 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf2),\n",
    "    ])\n",
    "\n",
    "scaled_svm_clf3 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf3),\n",
    "    ])\n",
    "\n",
    "scaled_svm_clf1.fit(X, y)\n",
    "scaled_svm_clf2.fit(X, y)\n",
    "scaled_svm_clf3.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞7 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_7_regularisation_critere.png?token=GHSAT0AAAAAABZOBGASPSRSPNQDAEM3QKSOY2ZS3KA\" alt=\"fig_7_regularisation_critere\" style=\"width:1400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les autres hyperparamtres fix√©s, une augmentation de C permet de diminuer la taille des marges. \n",
    "Plus la taille de la marge sera faible plus il sera compliqu√© de g√©n√©ralis√© pour le mod√®le\n",
    "<br>\n",
    "‚ÑπÔ∏è Le SVM lin√©aire est √©galement disponible √† travers la fonction **SGDClassifier** de sklearn. La diff√©rence essentiel provient de l'optimiseur utilis√©. Ici la fonction utilise une descente de gradient qui peut s'av√©rer utile dans le cas de dataset avec beaucoup de ligne (n grand)\n",
    "\n",
    "On l'utilise de la m√™mme mani√®re\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, loss='hinge', alpha=0.001))\n",
    "#alpha=1/(n*C)\n",
    "clf.fit(X, Y)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification non li√©naire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classfication lin√©aire est tr√®s utile et peut s'av√©rer pr√©cise pour de nombreux dataset. Malheuresement quand on se confronte √† des donn√©es r√©elles et non pas un dataset kaggle, les donn√©es sont rarement lin√©airement s√©parables.\n",
    "\n",
    "Les mod√®les li√©naires fournissent des performances assez faibles et ne permettent pas de r√©pondre √† votre probl√®me.\n",
    "Heuresement, il existe des technique pour faire √©voluer les SVM et traiter les cas o√π les donn√©es ne sont pas lin√©airement s√©parable.\n",
    "\n",
    "Prenons l'exemple intuitif suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞8 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_8_SVM_non_lineaire.png?token=GHSAT0AAAAAABZOBGASMJJWQECQ3ULPKKDGY2ZS37A\" alt=\"fig_8_SVM_non_lineaire\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce probl√®me est un cas r√©current en machine learning. Nous cherchons √† classifier des donn√©es mais les features disponibles ne permettent pas de le faire.\n",
    "\n",
    "C'est un cas d'√©cole de **feature engineering**.\n",
    "Ici il faut transformer nos donn√©es brutes de telle sorte qu'on puisse classifier nos donn√©es lin√©airement apr√®s transformation.\n",
    "\n",
    "Une transformation possible est d'ajouter un feature qui serait $X_2 = X_1¬≤$\n",
    "\n",
    "Regardons graphiquement le r√©sultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞9 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_9_svm_separation_lineaire.png?token=GHSAT0AAAAAABZOBGATM6V77DASKHUIZVZYY2ZS4HA\" alt=\"fig_9_svm_separation_lineaire\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant une transformation qui nous fait passer d'un probl√®me √† 1D √† 2D, on trouve un espace o√π nos donn√©es sont lin√©airement s√©parable.\n",
    "**Ce type de transformation est tr√®s utile pour les SVM mais s'applique √† tous les mod√®les de machine learning.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.red { background-color:#ff000020; border-radius: 5px; padding: 20px;}\n",
    "</style>\n",
    "<div class = \"blue\">\n",
    "\n",
    "Si vos donn√©es brutes offres des performances m√©diocres, pensez √† faire du feature engineering sur vos donn√©es.\n",
    "         \n",
    "- Appliquer des fonctions sur vos donn√©es pour en cr√©er des nouvelles (log, puissance, sigmoide, loi normale, ...)\n",
    "- Combiner des donn√©es $X_{new} = X_1*X_2$\n",
    "- Cr√©er de nouveau feature avec des mod√®les (ACP, ACM, AUTOENCODER, KNN)\n",
    "- Tester, soyez cr√©atif ;)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour impl√©menter ce type d'approche sklearn offre des fonctions toutes faites.\n",
    "Vous pouvez utiliser la fonction *PolynomiaFeatures* dans le module sklearn.preprocessing qui permet de faire des transformations polynomiales pour chaque features num√©riques.\n",
    "\n",
    "```Python\n",
    "#Import package\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#Function sklearn qui genere donn√©es en forme de lune\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "#Transformation polynomiale de nos features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_degr3 = poly.fit_transform(X)\n",
    "\n",
    "#Standardiser les donn√©es\n",
    "scaler = StandardScaler()\n",
    "X_degr3_scaled = scaler.fit_transform(X_degr3)\n",
    "\n",
    "#SVM classification\n",
    "\n",
    "polynomial_svm_clf_test= LinearSVC(C=1, loss='hinge', random_state=42)\n",
    "polynomial_svm_clf_test.fit(X_degr3_scaled,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞10 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_10_classification_non_lineaire.png?token=GHSAT0AAAAAABZOBGASDXBX7YBJM23ZMKN6Y2ZS4NA\" alt=\"fig_10_classification_non_lineaire\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'approche par transformation des features convient parfaitement pour des datasets de petite taille et peu complexe.\n",
    "En effet, une tranformation polynomiale avec des degr√©s faibles ne pourra pas s'adapter √† un dataset complexe et des degr√©s trop √©lev√©s cr√©eront un dataset trop grand pour √™tre entrainer dans un lapse de temps int√©ressant.\n",
    "\n",
    "Rappel :\n",
    "\n",
    "Plus on augmente la taille d'un dataset (features ou lignes) plus le mod√®le devient complexe √† entrainer et plus le temps de calcul sera long...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuresement, les math√©maticiens ont pens√© √† tout et il existe une astuce avec les SVM.\n",
    "On l'appelle le *kernel trick*. Cela permet d'obtenir les m√™mes r√©sultats qu'en ajoutant des *polynomial features* sans faire exploser la taille du dataset.\n",
    "\n",
    "Sans passer par la d√©monstration math√©matique, il faut retenir qu'utiliser le *kernel trick* permet d'avoir un mod√®le de compl√©xit√© $O(n^d)$ vs $O(n)$.\n",
    "\n",
    "Cette technique est impl√©ment√©e directement dans sklearn avec la fonction suivante\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel=\"poly\", degree=3, C=50,coef0=1)\n",
    "svm.fit(X, y)\n",
    "\n",
    "#Degree ==> Degre polinomiale\n",
    "#Kernel ==> Le type de noyau\n",
    "# C ==> Param√®tre de tol√©rance (r√©gularisation)\n",
    "# coef0 ==> Contr√¥le l'infulence des polynomes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞11 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_11_svm_no_lineaire_hyperpara.png?token=GHSAT0AAAAAABZOBGASCWXYK4BIUEU5PLXIY2ZS4WQ\" alt=\"fig_11_svm_no_lineaire_hyperpara\" style=\"width:1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La facon la plus simple de trouver les hyperpamratr√®s ad√©quats et de r√©aliser un *grid search*. Nous verrons en TD comment l'impl√©menter avec sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe de nombreux *kernel trick* utile pour trouver un espace √† plus grande dimension o√π nos donn√©es sont li√©nairement s√©parables.\n",
    "Vous pouvez aller voir celles qui sont impl√©ment√©s avec sklearn sur https://scikit-learn.org/stable/modules/svm.html#kernel-functions.\n",
    "\n",
    "La derni√®re que nous allons voir est une des plus populaires pour les SVM est la fonction de similarit√© *Gaussian Radial Basis Function*. Elle se d√©finit formellement de la fa√ßon suivante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation N¬∞\n",
    "\n",
    "$\\phi_\\gamma(x,x') = exp(-\\gamma\\vert\\vert x-x'\\vert\\vert¬≤)$\n",
    "\n",
    "o√π\n",
    "$x'$ = Un point rep√®re que nous choisisons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple :\n",
    "\n",
    "Prenons le cas du graphique 1D N¬∞...\n",
    "Nous prenons $x'$ = {-2,1} comme rep√®res et $x$ = -1 pour un $\\gamma =0.3$\n",
    "\n",
    "Nous obtenons donc les fonctions de similirat√©s suivantes pour 2 nouveaux features $x_2$ et $x_3$\n",
    "\n",
    "$x_2 = exp(-0.3*1¬≤) \\simeq  0.74 $\n",
    "\n",
    "$x_3 = exp(-0.3*2¬≤) \\simeq  0.3 $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞12 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_12_similarity_features.png?token=GHSAT0AAAAAABZOBGASD7UVYX4EJLOEUOFGY2ZS45A\" alt=\"fig_12_similarity_features\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les *polynomials features*, vous pouvez cr√©er √† la main vous m√™mes les features que vous souhaitez rajouter dans votre dataset avec cette technique.\n",
    "Choissiez autant de 'rep√®res' que vous avez de ligne dans votre dataset pour cr√©er de nouveaux features.\n",
    "\n",
    "Probl√®me, cette technique peut rapidement faire exploser la taille de votre Dataset si n est grand. Vous obtiendrez √† la fin un Dataframe de taille $n*n$ (en supposant que vous supprimez les features de bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuresement pour nous sklearn propose une impl√©mentation optimis√©e dans ses fonction pour cette approche. Exactement comme pour les *polynomial features*.\n",
    "\n",
    "On retrouve le *kernel trick* dans la fonction  SVC de sklearn avec comme noyau(kernel) 'rbf' pour Radial Basis Function kernel vu au dessus.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel=\"rbf\",  gamma=5, C=0.001)\n",
    "svm.fit(X, y)\n",
    "\n",
    "#gamma rend la distribution plus √©troite ce qui donne des fronti√®res de d√©cisions plus irr√©guli√®res\n",
    "#chaque observations influences plus la fronti√®re de d√©cision\n",
    "\n",
    "# C ici ne change pas. Il est toujours un cri√®tre de tol√©rance\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez √©galement que gamma comme C est un hyperparam√®tre permettant de r√©gulariser le mod√®le.\n",
    "Si votre mod√®le est en *overfitting* pensez √† r√©duire gamma/C et inversement s'il est en *underfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞12 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/13_rbf_kernel.png?token=GHSAT0AAAAAABZOBGAT6JHMTZ72HVIGNAP6Y2ZS5GA\" alt=\"13_rbf_kernel\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "div.blue { background-color: rgba(117, 190, 218, 0.5); border-radius: 5px; padding: 20px;}\n",
    "</style>\n",
    "<div class = \"blue\">\n",
    "\n",
    "Tips :\n",
    "Parfois vos donn√©es ne sont pas au format num√©rique et le *kernel trick* n√©cessite un noyau sp√©cifique.\n",
    "Il faut savoir qu'il existe des noyaux adapt√©s pour diff√©rentes structures de donn√©es :\n",
    "    \n",
    "- String kernel pour la classification de text par exemple ( cf *string subsequence kernel* ou *Levenshtein distance*)\n",
    "    \n",
    "    \n",
    "Comment choisir son *kernel trick* parmis ceux disponibles?\n",
    "Il n'y a pas de r√®gle √©crite qui permet de choisir directement. La meilleur r√©ponse est ca d√©pend de vos donn√©es.\n",
    "    \n",
    "Mais le sch√©ma suivant marche g√©n√©ralement bien:\n",
    "    \n",
    "- 1, commencer par un SVM lin√©aire dispo avec la fonction LinearSVC\n",
    "- 2, si le training set n'est tr√®s grand vous pouvez utiliser le noyau *Gaussian RBF kernel*\n",
    "- 3, si vous avez du temps tester d'autres noyau mais 1 et 2 est g√©n√©ralement suffisant pour voir si les SVM sont adapt√©s au probl√®me\n",
    "\n",
    "‚ö†Ô∏è Pensez bien √† tuner votre mod√®le avec un *gridsearch* et une *cross-validation* avant de comparer vos mod√®les avec votre test set\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Multiclass pour les SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreux mod√®les permettent nativement de r√©aliser des classifications multiclasse (Random Forest, SGBDclassifiers, Naives Bayes, ...).\n",
    "Malheuresement les SVM n'en font pas partie.\n",
    "\n",
    "Pour les utilier lors de classification multiclasse, nous devons trouver une parade!\n",
    "\n",
    "Imaginons qu'on nous donne un dataset contenant des images. Chaque image repr√©sente un chiffre manuscrit entre 0 et 9.\n",
    "On nous demande de cr√©er un mod√®le bas√© sur un SVM afin de classifier ces chiffres manuscrit (l'humain qui le fait habituellement en √† marre de le faire).\n",
    "\n",
    "Le data scientist en charge du projet √† bien compris la probl√©matique mais sait aussi qu'un SVM ne permet pas de faire de la classification multiclasse...\n",
    "Il cherche alors une strat√©gie pour r√©pondre parfaitement √† la commande.\n",
    "\n",
    "Sa premiere intuition est de d√©couper le probl√®me en 10 probl√®mes distincts.\n",
    "10 classification binaire o√π il va chercher √† identifier les 1 VS les autres puis les 2 VS les autres etc.\n",
    "Apr√®s les avoir entrain√©s, il obtiendra le score de d√©cision gr√¢ce √† sklearn et prendra celui qui le maximise.\n",
    "\n",
    "Un coll√®gue lui souffle √©galement une autre id√©e.\n",
    "R√©aliser des mod√®les par pair. Un mod√®le 1vs2, 1vs3, 1vs4,...2vs3,, ... etc\n",
    "Il essaye cette approche mais obtient 45 mod√®les diff√©rent pour ce probl√®me.\n",
    "\n",
    "\n",
    "C'est 2 approches sont appel√©s OVR(one versus rest) pour la premiere et OVO(one versus one) pour la seconde.\n",
    "Elles permettent d'approcher un probl√®me multiclasse avec une classification binaire.\n",
    "Leur principal d√©faut est de faire exploser le nombre de mod√®le √† entrainer\n",
    "\n",
    "OVR produit N mod√®les ou N est le nombre de classe\n",
    "<br>\n",
    "OVO produit $N\\times\\frac{N-1}{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'impl√©mentation avec sklearn est encore une fois chose facile\n",
    "##### OVR\n",
    "\n",
    "```python\n",
    "#Import OVO, OVR\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#Import SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "OvR_SVC_clf = OneVsRestClassifier(SVC())\n",
    "\n",
    "OvR_SVC_clf.fit(trainX, trainY)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "Pour exemple, nous pouvons utiliser ces strat√©gies pour le jeu de donn√©es MNIST.\n",
    "C'est un jeu de donn√©es c√©l√®bre qui comprend 70 000 images de chiffre √©crit √† la main. Chaque image a √©t√© classifi√© et le jeu de donn√©es est parfait pour s'entrainer √† ce type de donn√©es\n",
    "\n",
    "Pour le charger utilisez la commande suivante sur python\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#Import SVM\n",
    "from sklearn.svm import SVC\n",
    "##########\n",
    "#GET DATA#\n",
    "##########\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "# dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',\n",
    "# 'categories', 'url'])\n",
    "\n",
    "X , y = mnist[\"data\"] , mnist['target']\n",
    "X.shape\n",
    "#(70000, 784)\n",
    "# Chaque image contient 784 features qui correspodent √† la distribution de ses pixels en nuance de gris.\n",
    "# Sa valeur est entre 0 et 255\n",
    "\n",
    "#############\n",
    "#Train model#\n",
    "#############\n",
    "# Suivant votre quantit√© de RAM, attention √† combien de ligne vous prenez pour entrainer votre mod√®le!!\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000],y[60000:]\n",
    "\n",
    "\n",
    "OvR_SVC_clf = OneVsRestClassifier(SVC())\n",
    "OvO_SVC_clf = OneVsOneClassifier(SVC())\n",
    "\n",
    "OvR_SVC_clf.fit(X_train, y_train) \n",
    "OvO_SVC_clf.fit(X_train , y_train)    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞ 14:  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_14_mnist.png?token=GHSAT0AAAAAABZOBGAT3NHVR4YEHYDD47TQY2ZS5OQ\" alt=\"fig_14_mnist\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo avec le code 01_SVM_DEMO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM pour la r√©gression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derni√®re partie consacr√©e au SVM sera celle sur la r√©gression.\n",
    "Jusqu'√† maintenant nous avons vu les SVM pour la classification et il n'est pas n√©cessairement intuitif de voir comment l'appliqu√© √† la r√©gression (du moins th√©oriquement, ca vous prendra une ligne avec sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globalement les SVM pour la r√©gression sont aussi flexibles. Ils permettent de faire de la r√©gression *lin√©aire* et non *li√©naire* avec les m√™mes techniques vues pr√©c√©demment.\n",
    "La nuance est que nous devons inverser notre objectif! Ici on ne cherche plus √† maximiser la marge entre 2 classes tout en limitant le nombre de violations.\n",
    "\n",
    "Le mod√®le cherche √† inclure le maximun d'observation √† l'int√©rieur de ses marges tout en limitant le nombre d'observations √† l'ext√©rieur. La largeur de la fronti√®re de d√©cision sera control√©e par un nouveau hyperparam√®tre $\\epsilon$.\n",
    "\n",
    "Regardons comme il agit √† travers 2 exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>graphique N¬∞15 :  </u>\n",
    "<img src=\"https://raw.githubusercontent.com/Roulitoo/cours_iae/main/img/fig_15_svm_reg.png?token=GHSAT0AAAAAABZOBGATB75PQJFS25GWZEAIY2ZS5ZQ\" alt=\"fig_15_svm_reg\" style=\"width:700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez utilisez le code suivant pour l'impl√©menter sous python.\n",
    "La logique est la m√™me que pour la classfication ;)\n",
    "\n",
    "```python \n",
    "#SVM Regression li√©naire\n",
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)\n",
    "\n",
    "#SVM Regression non li√©naire\n",
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©capitulatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Summary\n",
    "\n",
    "**Advantages** of SVM\n",
    "It works well on a dataset having many features.\n",
    "It provides a clear margin of separation.\n",
    "It is very effective for the dataset where the number of features are greater than the data points.\n",
    "You can specify different kernel functions to make a proper decision boundary.\n",
    "\n",
    "**Disadvantages** of SVM\n",
    "It requires very high training time, hence not recommended for large datasets.\n",
    "It is very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 01_svm.ipynb to PDFviaHTML\n",
      "[NbConvertApp] Writing 852433 bytes to 01_svm.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter-nbconvert --to PDFviaHTML 01_svm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
