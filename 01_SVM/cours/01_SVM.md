<center><h1> SVM sous Python</h1></center>
<p align="center">
<img src="https://github.com/Roulitoo/cours_iae/blob/master/00_intro/img/Logo_IAE_horizontal.png" alt="Logo IAE.png" style="width:200px;"/>
</p>

#### Table of Contents
[1. Pr√©ambule](#1-pr%C3%A9ambule)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[1.1 Iris flower ](#11-iris-flower)<br>

[2. Support Vecteur Machine](#2-support-vecteur-machine)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[2.1 Presentation intuitive d'un SVM](#21-presentation-intuitive-dun-svm)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[2.2 Calcul de la marge ](#22-calcul-de-la-marge)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[2.3 Maximisation de la marge](#23-maximisation-de-la-marge)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[2.4 SVM LINEAIRE, HARDS MARGING VS SOFT MARGIN CLASSIFCATION](#24-svm-lineaire-hards-marging-vs-soft-margin-classifcation)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[2.5 SVM non li√©naire](#25-svm-non-li%C3%A9naire)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.5.1 Polynomial Kernel](#251-polynomial-kernel)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.5.2 Similarity Features](#252-similarity-features)<br>
&nbsp;&nbsp;&nbsp;&nbsp;[2.6 Classification Multiclass pour les SVM](#26-classification-multiclass-pour-les-svm)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.6.1 Application classication multiclass, dataset MNIST ](#261-application-classication-multiclass-dataset-mnist)<br>

[3. SVM pour la r√©gression](#3-svm-pour-la-r%C3%A9gression)<br>
[4. R√©capitulatif](#4-r%C3%A9capitulatif)<br>


## 1-Pr√©ambule

Connaissez-vous le fameux jeu de donn√©es IRIS produit par Ronald Fisher en 1936??  
Si non, nous allons y rem√©dier!  

Nous pouvons acc√©der √† ce dataset depuis sklearn avec le code suivant :

```python
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np

data = load_iris()

# Array to Pandas DataFrame
iris = pd.DataFrame(np.c_[data['data'],
                          data['target']
                         ],
                    columns = data.feature_names + ['species']
                   )

iris['species'] = iris['species'].astype('int')
```

Le jeu de donn√©es Iris est un jeu de donn√©es regroupant 3 esp√®ces de plantes :

- La Setosa
- La Versicolore
- La Virginica

Pour chaque plante nous avons mesur√© en cm 4 caract√©ristiques.
La longueur de ses p√©tales et s√©pales (visible sous la photo ci-dessous) ainsi que leur largeur.

## 1.1-Iris flower

    
![iris_photo](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)

On affiche le jeu de donn√©es Iris pour voir comment les informations sont repr√©sent√©es.  
Les 3 esp√®ces ont √©t√© recod√©es de la mani√®re suivante :

**0 = setos**

**1 = versicolor** 

**2 = virginica**

On compte 50 lignes par esp√®ces pour un jeu de donn√©es au format (150,5)


<u>Tableau N¬∞1 :  5 premieres lignes du jeu de donn√©es IRIS </u>

| sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | species |
|------------------:|-----------------:|------------------:|-----------------:|--------:|
|               5.1 |              3.5 |               1.4 |              0.2 |       0 |
|               4.9 |              3.0 |               1.4 |              0.2 |       0 |
|               4.7 |              3.2 |               1.3 |              0.2 |       0 |
|               4.6 |              3.1 |               1.5 |              0.2 |       0 |
|               5.0 |              3.6 |               1.4 |              0.2 |       0 |
    

Le tableau comporte 4 features(colonnes) ainsi que l'esp√®ce de la fleur.  

Regardons √©galement sur le graphique N¬∞1 la dispersion de la longueur des p√©tales en fonction de leur largeur.

<u>Graphique N¬∞1 :  R√©partition des Iris en fonction de la longueur et largueur de ses p√©tales</u>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_1_iris_scatter_y3.png" style="width:600px;"/>

On remarque que les points rouges ont une distribution tr√®s diff√©rente des autres.  
Pour les points gris et oranges on voit √©galement qu'ils appartiennent √† 2 distributions distinctes mais la fronti√®re entre les 2 est plus mince

<center><h1>2-Support Vecteur Machine</h1></center>

## 2.1-Presentation intuitive d'un SVM


Prenons un exemple de classification binaire avec le data set Iris.  
Pour faire simple nous utiliserons seulement 2 features (plus simple √† repr√©senter)

Nous d√©cidons de conserver la longueur du petal et la largeur du petal( colonnes 3 & 4) 

ü§î  
Imaginons maintenant qu'on nous demande de tracer une fonction permettant de s√©parer nos 2 nuages de points par une fronti√®re.  
**Comment feriez-vous?**



Nous cherchons une fonction de la forme :<br>
$f(x) = X^T\beta = \beta_0 + x_1\beta_1 + x_2\beta_2 + ... + x_n\beta_n =0$

Cette fonction de permettre de d√©terminer :

$f(x)>0 : classe: 1$<br>
$f(x)<0 : classe:0$

<u>Graphique N¬∞2 :Fronti√®re de d√©cision pour classification binaire</u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_2_intuition_svm.png" style="width:600px;"/>

Pour ce probl√®me, il existe une **infinit√© de solutions**. Comment en d√©terminer une optimale??

Une premi√®re piste est qu'en machine learning, notre objectif est de r√©aliser une pr√©diction.  
Pour cela, il faut donc que notre mod√®le soit **g√©n√©ralisable**. Un point qui ne figure pas dans les donn√©es d'entrainement, devra √™tre bien classifi√©.


Nous allons donc apporter une contrainte √† notre fronti√®re de d√©cision $f(x)$.
Elle ne devra pas √™tre trop proche des points en distance.  
Plus elle sera proche plus on aura de chance qu'un nouveau point issu de la distribution 0 ou 1 soit mal classifi√©.

**Autrement dit, nous devons trouver une contrainte √† notre fonction afin d'apporter une solution unique.**




<u>Graphique N¬∞3 : Calculer une distance </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/03_calcul_distance.png" alt="03_calcul_distance" style="width:400px;"/>

Pour rappel la distance entre un point et une droite se calcule de la mani√®re suivante pour un espace √† 2 dimensions

<u>Equation N¬∞1 : Distance dans d'un point √† une droite dans R¬≤</u><br>

La distance entre une droite $D$ d'√©quation $ax+by+c = 0$ et un point $P$ de coordonn√©es $(x1,y1)$ est

$\normalsize d(D,P) = \frac{\vert{ax+by+c}\vert}{\sqrt{a¬≤+b¬≤}}$

On peut g√©n√©raliser ce calcul de distance pour trouver les marges du SVM.

On va **chercher le ou les points** qui permettent de **maximiser l'√©cart entre la marge et la fronti√®re de d√©cision** (hyperplan) tout en **minimisant l'√©cart entre la marge √† un des points d'entra√Ænement**

Si on le formalise cela donne la formule suivante:


## 2.2-Calcul de la marge

<u>Equation N¬∞2 : Formule de calcul SVM </u>

Si on transpose notre exemple de R¬≤ √† notre probl√®me de SVM on obtient :

L'√©quation de la fronti√®re de d√©cision not√© $H$ est : $f(x) = \lt\beta.x \gt= \beta^Tx+b$ 

L'√©cart entre la fronti√®re de d√©cision et une marge est alors not√©e: $\large\frac{(\beta^Tx+b)}{\vert\vert\beta\vert\vert_2}$<br>
Comme il y a 2 marges on obtient la formule suivante :
<br>
$\normalsize Marge = 2d(x,H) =2\frac{(\omega^T\beta+b)}{\vert\vert\omega\vert\vert_2}$


o√π :<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\beta$ : param√®tre du mod√®le <br>
&nbsp;&nbsp;&nbsp;&nbsp; $\vert\vert\beta\vert\vert_2$ d√©signe la norme euclidienne de $\beta$ : $\sqrt{\beta_1¬≤+\beta_2¬≤+\beta_3¬≤+...+\beta_n¬≤}$ 


üí°Pour rappel, la marge est la distance minimale de l‚Äôhyperplan √† un des points d‚Äôentra√Ænement.


## 2.3-Maximisation de la marge

On veut trouver l‚Äôhyperplan de support qui permet de maximiser cette marge, c‚Äôest-√†-dire qu‚Äôon veut trouver un hyperplan avec la plus grande marge possible.
<br>
Puisque l‚Äôon cherche l‚Äôhyperplan qui maximise la marge, on cherche l‚Äôunique hyperplan dont les param√®tres $(\beta,b)$ sont donn√©s par la formule‚ÄØ:
<br>

$\large arg max_{\beta,b} min_k \frac{l_k(\beta^Tx+b)}{\vert\vert\beta\vert\vert_2}$

o√π $l_k$ est le label de la donn√©es

<u>Graphique N¬∞4 : Fronti√®re de d√©cision et marges, SVM lin√©aire</u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_4_visualisation_svm_lineaire.png" alt="fig_4_visualisation_svm_lineaire" style="width:600px;"/>
    
Sur le graphique N¬∞4, on peut observer les points fronti√®res qui maximisent l'√©cart entre la marge et la fronti√®re de d√©cision.
Ici on peut dire que la fronti√®re de d√©cision est bonne.  
Elle est suffisamment large pour que la probabilit√© qu'un point soit mal classifi√© est peu probable.

üìù  **Pour info**
- On appelle les points prochent des marges ==> **Points edges** ou **points supports**
- La fonction qui s√©pare nos 2 ensembles de points est une **fronti√®re de d√©cision**
- Les droites proche des points edges sont les **marges**

‚ùì**Questions**

Que se passe-t-il si je rajoute une observation? 
Ma fronti√®re de d√©cision change-t-telle?

‚ö†Ô∏è**Attention**

Notez qu'il est important de toujours standardiser vos donn√©es lorsque vous utilisez des SVM.  
De mani√®re g√©n√©rale quand vous utilisez des mod√®les avec calcul de distance, pensez √† standardiser vos donn√©es.
   
   Pour cela vous avez plusieurs op√©rations math√©matiques pour mettre vos donn√©es √† la m√™me √©chelle, voici les principaux √† retenir :
   
   - **Min Max Scaling**
   - **Normalization** 
   

### Standardisation
Prenons le jeu de donn√©es suivant avec $x_1$ et $x_2$ des features associ√© √† un label {0,1}.  
On a √©galement rajout√© la standardisation de nos 2 features $x_1scaled$ et $x_2scaled$

Nous lan√ßons un SVM lin√©aire pour tenter de classifier les labels 0 et 1 en fonction de ces features.
Un premier avec les features brutes et l'autre avec les features standardis√©s

Regardons maintenant sur le graphique N¬∞5 comment la standardisation impact la fronti√®re de d√©cision.

<u>Tableau N¬∞2 : Feature scaling</u>
<br>
| x1 | x2 | x1_scaled | x2_scaled | label |
|----|----|-----------|-----------|-------|
| 1  | 50 | -1.507    | -0.115    | 0     |
| 5  | 20 | 0.904     | -1.5010   | 0     |
| 3 | 80  | -0.301    | 1.270     | 1     |
| 5  | 60 | 0.904     | 0.346     | 1     |

<u>Graphique N¬∞5 : Influence de l'√©chelle des donn√©es sur le mod√®le </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_5_scaling_data.png" alt="scaling_features_5" style="width:600px;"/>

<style>
div.red { background-color:#ff000020; border-radius: 5px; padding: 20px;}
</style>
<div class = "red">
üìù  <strong>Pour info</strong>
    
Les SVM sont sensibles √† l'√©chelle des donn√©es. Il est important de standardiser ses donn√©es avant d'entrainer le mod√®le.

$\large Xscale = \frac{X-\mu}{\sigma}\$

</div>

## 2.4-SVM LINEAIRE, HARDS MARGING VS SOFT MARGIN CLASSIFCATION

A ce stade tout se passe bien et la classification d'un SVM semble √™tre parfaite pour notre jeu de donn√©es.

Cependant il est tr√®s rare qu'un jeu de donn√©es soit lin√©airement s√©parable... pour ainsi dire jamais avec des donn√©es d'entreprise.
Pour l'instant nous avons vu ce qu'on appelle un SVM √† *hard margin classification*. Chaque individu doit √™tre d'un cot√© de la zone de d√©cision.

Autrement dit, on ne peut pas trouver une versicolore du cot√© d'une setosa

**Mais cel√† pose 2 probl√®mes**



Prenons l'exemple du graphique N¬∞6.

Il est impossible de r√©aliser une classification lin√©aire √† l'aide d'un SVM.
L'outlier emp√™che de trouver une fronti√®re de d√©cision qui permettrait de classifier parfaitement nos 2 groupes.

Dans le 2nd cas, visible sur le graphique N¬∞6, l'outlier est tellement √©loign√© du point moyen de son groupe qu'il r√©duit drastiquement l'√©cart des marges du SVM.
<br>
La fronti√®re de d√©cision ne sera pas optimale et il sera compliqu√© de g√©n√©raliser ce mod√®le √† d'autres individus.


<u>Graphique N¬∞6 : Donn√©es non lin√©aire et SVM </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_6_svm_linear_problem.png" alt="fig_6_svm_linear_problem" style="width:800px;"/>

Pour √©viter ce genre de probl√®me, les statisticiens ont d√©velopp√© un mod√®le plus flexible.
Son objectif est de trouver un √©quilibre entre la maximisation des marges et le nombre de fois o√π l'on peut ignorer un point.
(nombre de points du mauvais cot√©, mal classifi√©)

Le mod√®le s'appelle le *soft margin classification* en opposition au *hard margin classification* vu plus haut.



### Soft Margin

La soft margin classification fait intervenir un nouveau param√®tre dans le mod√®le. On l'appelle param√®tre de r√©gularisation $C$.
Il est √† valeurs dans $]0,\infty[$.
On parle ici d'hyperparam√®tre car sa valeur optimale n'est pas fix√©e mais d√©pend du jeu de donn√©es. C'est √† vous de le trouver pour optimiser votre mod√®le

Regardons comment faire avec le code python suivant:

```python
#Import Package
import numpy as np
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)] # petal length, petal width
y = (iris["target"] == 2).astype('int32') # Iris virginica

# Code

#Standardiser nos donn√©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#Entrainement mod√®le
# Loss='hinge' permet de dire qu'on utile un SVM classique
# Vous pouvez regarder la doc pour avoir + d'infos
svm_clf= LinearSVC(loss='hinge', C=1)
svm_clf.fit(X_scaled, y)

#Prediction pour un nouveau point
svm_clf.predict([[1,1]])
#==> array([1.])

```

**Plus on augmente la valeur de $C$ plus le mod√®le va avoir tendance √† produire des marges proches de la fronti√®re de d√©cision et √† l'inverse plus $C$ est petit plus la fronti√®re sera grande.**
<br>
Pour mieux le comprendre, regardons le graphique N¬∞7 . Cela repr√©sente 3 SVM entrain√©s avec 3 valeurs diff√©rentes de $C$ √† savoir 1, 50 et 100

Code suivant
```python
scaler = StandardScaler()
svm_clf1 = LinearSVC(C=1, loss="hinge", random_state=42)
svm_clf2 = LinearSVC(C=50, loss="hinge", random_state=42)
svm_clf3 = LinearSVC(C=500, loss="hinge", random_state=42)

scaled_svm_clf1 = Pipeline([
        ("scaler", scaler),
        ("linear_svc", svm_clf1),
    ])
scaled_svm_clf2 = Pipeline([
        ("scaler", scaler),
        ("linear_svc", svm_clf2),
    ])

scaled_svm_clf3 = Pipeline([
        ("scaler", scaler),
        ("linear_svc", svm_clf3),
    ])

scaled_svm_clf1.fit(X, y)
scaled_svm_clf2.fit(X, y)
scaled_svm_clf3.fit(X, y)
```

<u>Graphique N¬∞7 :Influence du crit√®re de regularisation </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_7_regularisation_critere.png" alt="fig_7_regularisation_critere" style="width:1400px;"/>

Pour les autres hyperparamtres fix√©s, une augmentation de C permet de diminuer la taille des marges. 
Plus la taille de la marge sera faible plus il sera compliqu√© de g√©n√©raliser pour le mod√®le
<br>
<br>
‚ÑπÔ∏è Le SVM lin√©aire est √©galement disponible √† travers la fonction **SGDClassifier** de sklearn. La diff√©rence essentielle provient de l'optimiseur utilis√©. Ici la fonction utilise une descente de gradient qui peut s'av√©rer utile dans le cas de dataset avec beaucoup de ligne (n grand)

On l'utilise de la m√™me mani√®re

```python

import numpy as np
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
Y = np.array([1, 1, 2, 2])
# Always scale the input. The most convenient way is to use a pipeline.
clf = make_pipeline(StandardScaler(),
                    SGDClassifier(max_iter=1000, loss='hinge', alpha=0.001))
#alpha=1/(n*C)
clf.fit(X, Y)
#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html

```

## 2.5-SVM non li√©naire

La classification lin√©aire est tr√®s utile et peut s'av√©rer pr√©cise pour de nombreux dataset. Malheureusement quand on se confronte √† des donn√©es r√©elles et non pas un dataset kaggle, les donn√©es sont rarement lin√©airement s√©parables.

Les mod√®les lin√©aires fournissent des performances assez faibles et ne permettent pas de r√©pondre √† votre probl√®me.
Heureusement, il existe des techniques pour faire √©voluer les SVM et traiter les cas o√π les donn√©es ne sont pas lin√©airement s√©parables.

Prenons l'exemple intuitif suivant :

<u>Graphique N¬∞8 : Donn√©es non lin√©airement s√©parables  </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_8_SVM_non_lineaire.png" alt="fig_8_SVM_non_lineaire" style="width:800px;"/>

Ce probl√®me est un cas r√©current en machine learning. Nous cherchons √† classifier des donn√©es mais les features disponibles ne permettent pas de le faire.

C'est un cas d'√©cole de **feature engineering**.
Ici il faut transformer nos donn√©es brutes de telle sorte qu'on puisse classifier nos donn√©es lin√©airement apr√®s transformation.

Une transformation possible est d'ajouter un feature qui serait $X_2 = X_1¬≤$

Regardons graphiquement le r√©sultat

<u>Graphique N¬∞9 : Astuce pour donn√©es non lin√©airement s√©parables </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_9_svm_separation_lineaire.png" alt="fig_9_svm_separation_lineaire" style="width:800px;"/>

En utilisant une transformation qui nous fait passer d'un probl√®me √† 1D √† 2D, on trouve un espace o√π nos donn√©es sont lin√©airement s√©parables.
**Ce type de transformation est tr√®s utile pour les SVM mais s'applique √† tous les mod√®les de machine learning.**


<style>
div.red { background-color:#ff000020; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Si vos donn√©es brutes offres des performances m√©diocres, pensez √† faire du feature engineering sur vos donn√©es.
         
- Appliquer des fonctions sur vos donn√©es pour en cr√©er des nouvelles (log, puissance, sigmoide, loi normale, ...)
- Combiner des donn√©es $X_{new} = X_1*X_2$
- Cr√©er de nouveau feature avec des mod√®les (ACP, ACM, AUTOENCODER, KNN)
- Tester, soyez cr√©atif ;)
</div>

Pour impl√©menter ce type d'approche sklearn offre des fonctions toutes faites.
Vous pouvez utiliser la fonction *PolynomiaFeatures* dans le module sklearn.preprocessing qui permet de faire des transformations polynomiales pour chaque feature num√©rique.

```Python
#Import package
from sklearn.datasets import make_moons
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import LinearSVC
#Function sklearn qui genere donn√©es en forme de lune
X, y = make_moons(n_samples=100, noise=0.15)
polynomial_svm_clf = Pipeline([
("poly_features", PolynomialFeatures(degree=3)),
("scaler", StandardScaler()),
("svm_clf", LinearSVC(C=10, loss="hinge"))
])
polynomial_svm_clf.fit(X, y)

```

```python
from sklearn.datasets import make_moons
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import LinearSVC
X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

#Transformation polynomiale de nos features
poly = PolynomialFeatures(degree=3)
X_degr3 = poly.fit_transform(X)

#Standardiser les donn√©es
scaler = StandardScaler()
X_degr3_scaled = scaler.fit_transform(X_degr3)

#SVM classification

polynomial_svm_clf_test= LinearSVC(C=1, loss='hinge', random_state=42)
polynomial_svm_clf_test.fit(X_degr3_scaled,y)
```

<u>Graphique N¬∞10 : SVM transformation polynomiale </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_10_classification_non_lineaire.png" alt="fig_10_classification_non_lineaire" style="width:800px;"/>

### 2.5.1-Polynomial Kernel

L'approche par transformation des features convient parfaitement pour des datasets de petite taille et peu complexe.
En effet, une transformation polynomiale avec des degr√©s faibles ne pourra pas s'adapter √† un dataset complexe et des degr√©s trop √©lev√©s cr√©eront un dataset trop grand pour √™tre entrain√© dans un laps de temps int√©ressant.

Rappel :

Plus on augmente la taille d'un dataset (features ou lignes) plus le mod√®le devient complexe √† entrainer et plus le temps de calcul sera long...


Heureusement, les math√©maticiens ont pens√© √† tout et il existe une astuce avec les SVM.
On l'appelle le *kernel trick*. Cela permet d'obtenir les m√™mes r√©sultats qu'en ajoutant des *polynomial features* sans faire exploser la taille du dataset.

Sans passer par la d√©monstration math√©matique, il faut retenir qu'utiliser le *kernel trick* permet d'avoir un mod√®le de complexit√© $O(n^d)$ vs $O(n)$.

Cette technique est impl√©ment√©e directement dans sklearn avec la fonction suivante

```python
from sklearn.svm import SVC

svm = SVC(kernel="poly", degree=3, C=50,coef0=1)
svm.fit(X, y)

#Degree ==> Degre polinomiale
#Kernel ==> Le type de noyau
# C ==> Param√®tre de tol√©rance (r√©gularisation)
# coef0 ==> Contr√¥le l'infulence des polynomes
```

<u>Graphique N¬∞11 : SVM non lin√©aire influence des hyperparam√®tres </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_11_svm_no_lineaire_hyperpara.png" alt="fig_11_svm_no_lineaire_hyperpara" style="width:1000px;"/>

La facon la plus simple de trouver les hyperparam√®tres ad√©quats et de r√©aliser un *grid search*.<br>
Nous verrons en TD comment l'impl√©menter avec sklearn

### 2.5.2-Similarity Features

Il existe de nombreux *kernel trick* utile pour trouver un espace √† plus grande dimension o√π nos donn√©es sont lin√©airement s√©parables.
Vous pouvez aller voir celles qui sont impl√©ment√©es avec [sklearn](https://scikit-learn.org/stable/modules/svm.html#kernel-functions)

La derni√®re que nous allons voir est une des plus populaires pour les SVM est la fonction de similarit√© *Gaussian Radial Basis Function*. Elle se d√©finit formellement de la fa√ßon suivante

<u>Equation N¬∞ 3  : kernel trick</u>

$\phi_\gamma(x,x') = exp(-\gamma\vert\vert x-x'\vert\vert¬≤)$

o√π
$x'$ = Un point rep√®re que nous choisisons 

Exemple :

Prenons le cas du graphique 1D N¬∞12
Nous prenons $x'$ = {-2,1} comme rep√®res et $x$ = -1 pour un $\gamma =0.3$

Nous obtenons donc les fonctions de similirat√©s suivantes pour 2 nouveaux features $x_2$ et $x_3$

$x_2 = exp(-0.3*1¬≤) \simeq  0.74$

$x_3 = exp(-0.3*2¬≤) \simeq  0.3$





<u>Graphique N¬∞12 : Similarity feature construction </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_12_similarity_features.png" alt="fig_12_similarity_features" style="width:800px;"/>

Comme pour les *polynomials features*, vous pouvez cr√©er √† la main vous-m√™mes les features que vous souhaitez rajouter dans votre dataset avec cette technique.
Choissier autant de 'rep√®res' que vous avez de ligne dans votre dataset pour cr√©er de nouveaux features.

**Probl√®me**, cette technique peut rapidement **faire exploser la taille de votre Dataset si $n$ est grand**. Vous obtiendrez √† la fin un Dataframe de taille $n*n$ (en supposant que vous supprimez les features de bases)

Heureusement pour nous sklearn propose une impl√©mentation optimis√©e dans ses fonctions pour cette approche. Exactement comme pour les *polynomial features*.

On retrouve le *kernel trick* dans la fonction  SVC de sklearn avec comme noyau(kernel) 'rbf' pour Radial Basis Function kernel vu au-dessus.


```python

from sklearn.svm import SVC

svm = SVC(kernel="rbf",  gamma=5, C=0.001)
svm.fit(X, y)

#gamma rend la distribution plus √©troite ce qui donne des fronti√®res de d√©cisions plus irr√©guli√®res
#chaque observations influences plus la fronti√®re de d√©cision

# C ici ne change pas. Il est toujours un cri√®tre de tol√©rance


```

Notez √©galement que gamma comme C est un hyperparam√®tre permettant de r√©gulariser le mod√®le.<br>
Si votre mod√®le est en *overfitting* pensez √† r√©duire gamma/C et inversement s'il est en *underfitting*

<u>Graphique N¬∞13 : Fronti√®re de d√©cision et similarity features </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/13_rbf_kernel.png" alt="13_rbf_kernel" style="width:800px;"/>

<style>
div.blue { background-color: rgba(117, 190, 218, 0.5); border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

Tips :
Parfois vos donn√©es ne sont pas au format num√©rique et le *kernel trick* n√©cessite un noyau sp√©cifique.
Il faut savoir qu'il existe des noyaux adapt√©s pour diff√©rentes structures de donn√©es :
    
- String kernel pour la classification de text par exemple ( cf *string subsequence kernel* ou *Levenshtein distance*)
    
    
Comment choisir son *kernel trick* parmis ceux disponibles?
Il n'y a pas de r√®gle √©crite qui permet de choisir directement. La meilleur r√©ponse est ca d√©pend de vos donn√©es.
    
Mais le sch√©ma suivant marche g√©n√©ralement bien:
    
- 1, commencer par un SVM lin√©aire dispo avec la fonction LinearSVC
- 2, si le training set n'est tr√®s grand vous pouvez utiliser le noyau *Gaussian RBF kernel*
- 3, si vous avez du temps, testez d'autres noyaux mais 1 et 2 est g√©n√©ralement suffisant pour voir si les SVM sont adapt√©s au probl√®me

‚ö†Ô∏è Pensez bien √† *tuner* votre mod√®le avec un *gridsearch* et une *cross-validation* avant de comparer vos mod√®les avec votre test set
    

</div>

## 2.6-Classification Multiclass pour les SVM


De nombreux mod√®les permettent nativement de r√©aliser des classifications multiclasse (Random Forest, SGBDclassifiers, Naives Bayes, ...).
Malheureusement les SVM n'en font pas partie.

Pour les ustiliser lors de classifications multiclasse, nous devons trouver une parade!

### Exemple, classification de chiffre manuscrit

#### Contexte
Imaginons qu'on nous donne un dataset contenant des images. Chaque image repr√©sente un chiffre manuscrit entre 0 et 9.
On nous demande de cr√©er un mod√®le bas√© sur un SVM afin de classifier ces chiffres manuscrits (l'humain qui le fait habituellement en √† marre de le faire).

Le data scientist en charge du projet √† bien compris la probl√©matique mais sait aussi qu'un SVM ne permet pas de faire de la classification multiclasse...
Il cherche alors une strat√©gie pour r√©pondre parfaitement √† la commande.

#### Solution
Sa **premiere intuition** est de **d√©couper le probl√®me en 10 probl√®mes distincts.**
10 classifications binaires o√π il va chercher √† identifier les 1 VS les autres puis les 2 VS les autres etc.
Apr√®s les avoir entrain√©s, il obtiendra le score de d√©cision gr√¢ce √† sklearn et prendra celui qui le maximise.

Un coll√®gue lui souffle √©galement **une autre id√©e.**
**R√©aliser des mod√®les par pair. Un mod√®le 1vs2, 1vs3, 1vs4,...2vs3,, ... etc**
Il essaye cette approche mais obtient **45 mod√®les** diff√©rents pour ce probl√®me.


C'est 2 approches sont appel√©s **OVR(one versus rest)** pour la premiere et **OVO(one versus one)** pour la seconde.
Elles permettent d'approcher un probl√®me multiclasse avec une classification binaire.
Leur principal d√©faut est de faire exploser le nombre de mod√®les √† entrainer.

- **One versus rest** : On ram√®ne un probl√®me avec $N_{class}$ √† $N$ classification binaire. On compare une classe √† $N-1$ classe.<br>
<br>

- **One versus one** : On ram√®ne le probl√®me avec $N_{class}$ √† $N\times\frac{N-1}{2}$ classification binaire. Cette fois on teste toutes les combinaisons de N possibles divis√©es par 2. (Classification $N_1 | N_2$ est la m√™me que $N_2 | N_1$)



L'impl√©mentation avec sklearn est encore une fois chose facile
#### OVR

```python
#Import OVO, OVR
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multiclass import OneVsOneClassifier
#Import SVM
from sklearn.svm import SVC

OvR_SVC_clf = OneVsRestClassifier(SVC())

OvR_SVC_clf.fit(trainX, trainY)

```

### 2.6.1-Application classication multiclass, dataset MNIST

Pour exemple, nous pouvons utiliser ces strat√©gies pour le jeu de donn√©es MNIST.
C'est un jeu de donn√©es c√©l√®bre qui comprend 70 000 images de chiffre √©crit √† la main. Chaque image a √©t√© classifi√© et le jeu de donn√©es est parfait pour s'entrainer √† ce type de donn√©es

Pour le charger utilisez la commande suivante sur python

```python
from sklearn.datasets import fetch_openml
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multiclass import OneVsOneClassifier
#Import SVM
from sklearn.svm import SVC
##########
#GET DATA#
##########
mnist = fetch_openml('mnist_784', version=1)
mnist.keys()
# dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',
# 'categories', 'url'])

X , y = mnist["data"] , mnist['target']
X.shape
#(70000, 784)
# Chaque image contient 784 features qui correspodent √† la distribution de ses pixels en nuance de gris.
# Sa valeur est entre 0 et 255

#############
#Train model#
#############
# Suivant votre quantit√© de RAM, attention √† combien de ligne vous prenez pour entrainer votre mod√®le!!
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000],y[60000:]


OvR_SVC_clf = OneVsRestClassifier(SVC())
OvO_SVC_clf = OneVsOneClassifier(SVC())

OvR_SVC_clf.fit(X_train, y_train) 
OvO_SVC_clf.fit(X_train , y_train)    

```

<u>Graphique N¬∞ 14: Visualisation chiffres manuscrits </u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_14_mnist.png" alt="fig_14_mnist" style="width:600px;"/>

<center>************************Demo avec le code 01_SVM_DEMO************************</center>

## 3-SVM pour la r√©gression 

La derni√®re partie consacr√©e au SVM sera celle sur la r√©gression.
Jusqu'√† maintenant nous avons vu les SVM pour la classification et il n'est pas n√©cessairement intuitif de voir comment l'appliquer √† la r√©gression (du moins th√©oriquement, ca vous prendra une ligne avec sklearn)


Globalement les SVM pour la r√©gression sont aussi flexibles. Ils permettent de faire de la r√©gression *lin√©aire* et non *li√©naire* avec les m√™mes techniques vues pr√©c√©demment.
La nuance est que nous devons inverser notre objectif! Ici on ne cherche plus √† maximiser la marge entre 2 classes tout en limitant le nombre de violations.

Le mod√®le cherche √† inclure le maximum d'observation √† l'int√©rieur de ses marges tout en limitant le nombre d'observations √† l'ext√©rieur. La largeur de la fronti√®re de d√©cision sera contr√¥l√©e par un nouvel hyperparam√®tre $\epsilon$.

Regardons comme il agit √† travers 2 exemples

<u>Graphique N¬∞15 : Exemple SVM r√©gression</u>
<br>
<img src="https://github.com/Roulitoo/cours_iae/blob/master/01_SVM/img/fig_15_svm_reg.png" alt="fig_15_svm_reg" style="width:700px;"/>

Vous pouvez utilisez le code suivant pour l'impl√©menter sous python.
La logique est la m√™me que pour la classfication üòâ

```python 
#SVM Regression li√©naire
from sklearn.svm import LinearSVR
svm_reg = LinearSVR(epsilon=1.5)
svm_reg.fit(X, y)

#SVM Regression non li√©naire
from sklearn.svm import SVR
svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)
svm_poly_reg.fit(X, y)



```

## 4-R√©capitulatif

<u>Tableau N¬∞3 : R√©sum√© des fonction utilis√©es</u>

| Sklearn class | Scaling | Kernel trick | Hyperparameter                                          | Computational complexity | Multiclass                                | Sklearn import                                 |
|--------------:|---------|-------------:|---------------------------------------------------------|-------------------------:|-------------------------------------------|------------------------------------------------|
| SVC           | OUI     | OUI          | kernel,C                                            |          O(m^3n)         | OneVsRestClassifier<br>OneVsOneClassifier | from sklearn.svm import SVC                    |
| LinearSVC     | OUI     | NON          | loss ='hinge',<br>C=> Tol√©rance                         |          O(m*n)          | OneVsRestClassifier<br>OneVsOneClassifier | from sklearn.svm import LinearSVC              |
| SGDClassifier | OUI     | NON          | loss = 'hinge',<br>max_iter,<br>Alpha ==> Learning rate |          O(m*n)          | OneVsRestClassifier<br>OneVsOneClassifier | from sklearn.linear_model import SGDClassifier |
| LinearSVR     | OUI     | NON          |  Epsilon , C                                                      |          O(m*n)          |                                           | from sklearn.svm import LinearSVR              |
| SVR           | OUI     | OUI          |Epsilon,  C , kernel                                                        |          O(m^3n          |                                           | from sklearn.svm import SVR                    |

**Avantages des SVM**<br>
- Ils marchent relativement bien sur les dataset avec peu de features<br>
- Ils fournissent une s√©paration clair de vos donn√©es. On peut interpr√©ter le mod√®le.<br>
- Ils s'adaptent tr√®s bien au dataset avec plus de features que de data points.<br>
- On peut sp√©cifier diff√©rent $kernel$ pour trouver une meilleur fronti√®re de d√©cision<br>

**D√©savantages des SVM** 
- Leur entrainement n√©cessite beaucoup de temps de calul et de ressource.<br> 
- Ce n'est pas recommand√© de les utilis√© si vous avez un dataset grand (millions de lignes)<br>
- Ils sont tr√®s sensibles aux outliers<br>
- Ne supportent pas nativement le multiclass<br>


> Si vous souhaitez un suppl√©ment de cours plus math√©matique vous pouvez consulter le lien [suivant](https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-app-svm.pdf)

> La page wikip√©dia de [Vapnik](https://fr.wikipedia.org/wiki/Vladimir_Vapnik) l'inventeur du SVM

> Une video avec Vladmir Vapnik sur le [l'apprentissage automatique](https://www.youtube.com/watch?v=STFcvzoxVw4&ab_channel=LexFridman) 
