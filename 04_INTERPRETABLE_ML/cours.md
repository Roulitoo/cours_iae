## Interpretable Machine Learning

R√©sum√© :  Ce cours est √† destination des √©tudiants du master 2 ECAP de l'IAE Nantes. Il vise √† r√©sumer les m√©thodes d'interpretation de mod√®le de machine learning qualifi√© de "Black Box" et ainsi rendre le mod√®le explicable.

La premiere partie s'attardera sur la notion d'explicabilit√© et les diff√©rents cas o√π il s'av√©re n√©ccessaire de rendre la d√©cision d'un mod√®le de machine learning humainement interpretable.

Apr√®s avoir explor√© les diff√©rents concepts d'interpratibilit√© nous √©tudierons bri√®vement les mod√®les interpr√©table par nature ( concept d√©ja maitr√Æs√© dans votre cursus) puis nous recentrerons le cours sur les "models agnostic methods" afin d'interpr√©ter les mod√®les boites noirs.

Nous verrons pour chaque m√©thode leur explication th√©roqie et comment elles sont construires et leur impl√©mentation avec le langage Python.

De plus nous concluerons syst√©matiquement par les avantages et inconv√©nient de chaque m√©thode cit√©e.

Le cours est aujourd'hui traiter pour interpr√©ter des donn√©es tabulaires ce qui correspond aux donn√©es que vous traitez majoritarement dans le cadre de votre formation mais il existe √©galement des m√©thodes d'interp√©tation pour **des donn√©es non tabulaire!**

### Qu'est ce que le machine learning

Avant d'entammer une d√©finition pr√©cise de l'interpr√©tabilit√© et explicabilit√© des mod√®les d'apprentissage automatique. il convient de bien d√©finir l'apprentissage automatique ou Machine Learning.

Parfois, il est confondu avec la notion d'algorithme.

Illustration d'un algorithme VS Machine Learning

![1735222089703](image/cours/1735222089703.png)

**Un algorithme** : Est un ensemble de r√®gle d√©finie par un humain qui sont √©x√©cut√©es par une machine afin d'atteindre un but pr√©d√©f√©nis.

On peut le voir comme un processus qui d√©finit des intputs et pr√©voit tous les √©tapes permettant de transformer nos inputs en output d√©sir√©s.

```python
def celsius_to_fahrenheit(celsius):
    """
    Convertit une temp√©rature de degr√©s Celsius en Fahrenheit.
    Inputs:
        - celsius (float): temp√©rature en degr√©s Celsius
    Output:
        - float: temp√©rature en Fahrenheit
    """
    # √âtape 1 : Appliquer la formule de conversion
    fahrenheit = (celsius * 9/5) + 32

    # √âtape 2 : Retourner le r√©sultat
    return fahrenheit

# Exemple d'utilisation
print(celsius_to_fahrenheit(25))  # Output : 77.0

```

L'utilisation d'un algorithme ici est idoine. On connait les instructions qu'on souhaite r√©aliser et comment transformer nos intput pour obtenir l'output d√©sir√©.

**Machine Learning :** C'est une m√©thode qui permet √† un programme d'apprendre √† partir de donn√©es afin de r√©aliser et optimiser une pr√©diction. C'est un changement de paradigm de la *programmation normale* o√π on d√©finit explicitement nos √©tapes et nos r√®gles √† une *programmation indirecte* o√π les r√®gles elles-m√™mes √©manement de la Data.

#### Machine Learning VS Statistical Learning

L'approche statistique(√©conometrique) s'attarque √† comprendre le processus g√©n√©rateur d'un ph√©nom√®ne Y en se basant sur les co-informations X.

```mermaid
graph LR
    X[Input: X] -->|R√©gression lin√©aire / R√©gression logistique| Process[Mod√®le]
    Process --> Y[Output: Y]

```

Le machine learning quant √† lui cherche √† approximer Y √† l'aide d'une fonction f(x) sans s'attarder sur les relations entre Y et f(x).

```mermaid
graph LR
    X[Input: X] --> Process[Bo√Æte noire]
    Process --> Y[Output: Y]

    X-.-> ML[Random Forest<br> XGBoost<br> Neural Network]
    ML-.-> Y


  

```

Avec un mod√®le de machine complexe nous sommes en incapacti√© d'explicit√© le r√©sultat pour une pr√©diction individuelle.

Pourquoi dois-je refuser un pr√™t pour ce client? La seul r√©ponse que je je peux apporter est "parce-que le mod√®le me le dit"

### De l'importance de l'interpretability :

Il n'existe pas de d√©finition math√©matique formelle de l'interpr√©tabilit√© mais nous pouvons repondre la d√©finition donn√©es par Miller(2017):

> L'interpretabilit√© d'un mod√®le de Machine Learning est la capacit√© d'un humain √† comprendre les causes d'une d√©cision du mod√®le.

Plus un interpr√©tabilit√© du mod√®le sera forte plus un humain sera en mesure de comprendre les d√©cisions/crit√®res influencant la pr√©diction du mod√®le.

Dans ce cours nous distinguerons √©galement une nuance entre Interpretabilit√© d'un mod√®le et Explicabilit√©.

**üí°Explicabilit√©** : Explication de pr√©diction individuelle

**üìñ Interpr√©tabilit√©** : Compr√©hension g√©n√©rale du mod√®le et comment sont r√©alis√©es les pr√©dictions

#### Compromis entre interpr√©tabilit√© et pouvoir pr√©dictif

Lors de l'entrainement  d'un mod√®le d'apprentissage automatique vous aurez syst√©matiquement √† choisir entre le "**Pourquoi" et le "Quoi".**

Un mod√®le permettant de comprendre facilement **"Pourquoi"** je r√©alise tel ou tel pr√©diction offre g√©n√©ralement de performance moins bonne, de fait un **"quoi"** plus faible.

---

**Exemple du Customer Churn :**

Un client souhaite partir de votre enseigne et votre mod√®le de ML d√©tecte avec une probabilit√© de 98% qu'il va quitter votre enseigne. Cette information est importante car elle priorise de client pour agir tout de suite afin de le retenir.

En revanche, cela ne vous aucune information de comment le retenir

Qu'est ce qui pousse mon client √† partir? Cette question n'est pas r√©pondue.

- Le positionnement de mon prix?
- Une insatifaction ?

Ce compromis doit √™tre guid√© par l'objectif que vous recherchez.                                                                                                             G√©n√©ralement le Pourquoi l'importe dans les cas suivants :

- Recherche scientifique afin de comprendre un ph√©nom√®ne ==> Pourquoi
- Quand le probl√®me est d√©ja tr√®s bien cadr√© et d√©ja r√©solu ==> What Optical charact√®re recognition
- Authorit√© r√©gulatrice besoin de comprendre le mod√®le

```markdown
> [!NOTE]
Suivant la probl√©matique que vous souhaitez r√©soudre vos aurez √† choisir entre maximiser l'interpr√©tabilit√© de votre mod√®le ou son pouvoir explicatif.

D'o√π l'importance de cadrer pr√©cisement le probl√®me que vous souhaitez adresser
```

#### Taxonomie des interpr√©tations de mod√®les

Le premier niveau d'interpr√©tabilit√© porte sur la **capacit√© intrins√®que** d'un mod√®le √† √™tre interpr√©table ou alors √™tre **interpr√©table post hoc.**

**Capacit√© intrins√®que :**

On l'obtient en contraignant le mod√®le en restreignant sa compl√©xit√© (nombre de features) afin de le comprendre ais√©ment contre  des m√©thodes utilis√©es pour interpr√©ter le mod√®le post entrainement.

L'interpr√©tabilit√© intrins√®que se refere √† des mod√®les simle comme (R√©gression lin√©aire, Logistique,  Arbre de regression/classification simple, SVM)

**Post hoc:**

L'interpretabilit√© se ref√®re √† des mod√®les plus complexes qui sont interpr√©tables post-entrainement grace √† des m√©thodes ext√©rieurs aux mod√®les (Feature importance)

> üí°Le post Hoc interpretabilit√© peut aussi s'utiliser sur des mod√®les interpretable intrins√®que

Globalement, il existe 5 m√©thodes d'interpr√©tation qu'on peut diviser de la fa√ßon suivante :

- **Feature summary statistic** : Statistique par feature permettant d'interpr√©ter son r√¥le dans le mod√®le (Feature Importante)
- **Feature summary visualization** : Repr√©sentation visuels des statistiques en visualisation quand le nombre de statistiques rend difficilie l'interpration une √† une (Partial Dependance Plot)
- **Model internal** : Param√®tre interne du mod√®le permettant l'interpr√©tation des r√©sultats ( Poids du mod√®les Reg li√©naire, Structure arbre)
- **Data point** : L'interpr√©ation par individu sp√©cifique du jeu de donn√©es, on se concentre sur un invidividu sp√©cifique afin d'expliquer sa pr√©diction (Counter factual pr√©diction)
- Intrinsically interpretable model : Approximer un mod√®le Black Box par un mod√®le interpr√©table localement ou Globalement (ref model internal)

> üí°On parle de mod√®le sp√©cifique quand l'interpr√©tation est propre √† un type de mod√®le et de mod√®le agnostic quand la m√©thode s'applique √† tout type de mod√®le.

### Les diff√©rents niveaux d'interpr√©tabilit√©s

#### L'interpr√©tabilit√© Global ou Hoslitique

Un mod√®le est globalement interpr√©table si l'on peut comprendre **l'ensemble de son fonctionnement** d'un seul coup d'≈ìil ou avec une vue d'ensemble compl√®te. Il faut √™tre capable de :

* Comment le mod√®le effectue ses pr√©dictions (les m√©canismes internes).
* L'importance des variables/features.
* Les interactions entre les variables.
* La distribution des sorties (cible) en fonction des caract√©ristiques d'entr√©e.

Il est tr√®s rare de pouvoir atteindre ce niveau de connaissance d'un mod√®le quand on d√©passe 3 Features. D√®s lors qu'on d√©passe des repr√©sentations √† 3 dimensions, il est impossible pour un humain de se repr√©senter les int√©ractions.

#### L'interpr√©tabilit√© Global √† un niveau modulaire

Comprendre un mod√®le entier, comme un Naive Bayes avec des centaines de variables, est pratiquement impossible. Cela n√©cessiterait de m√©moriser tous les poids et d‚Äô√©valuer la distribution conjointe des variables, une t√¢che irr√©aliste.

Plut√¥t que de chercher √† comprendre tout le mod√®le, on peut analyser certaines parties sp√©cifiques :

Pour les mod√®les lin√©aires on peut interpr√©ter ses poids toutes choses √©tant √©gale par ailleurs. Cela signifie que les autres param√®tres sont inchang√©s pour intr√©pter l'effet d'une variable.

Dans les faits, il est rare qu'une variable varie alors que les autres sont constantes.

#### **Interpr√©tabilit√© locale pour une pr√©diction unique**

Comprendre pourquoi un mod√®le a fait une pr√©diction particuli√®re pour une instance donn√©e.

√Ä un niveau local, le comportement d‚Äôun mod√®le complexe peut devenir plus simple. Par exemple :

Une relation non lin√©aire entre la taille et le prix d‚Äôune maison peut se comporter de mani√®re lin√©aire pour une maison de 100 m¬≤ si l‚Äôon observe uniquement cette instance.

On peut tester cela en simulant des modifications de la taille (+ ou - 10 m¬≤) et en observant l‚Äôimpact sur la pr√©diction.

**Avantage** : Les explications locales sont souvent plus pr√©cises que les explications globales, car elles se concentrent sur un sous-ensemble restreint de donn√©es.

**M√©thodes disponibles** : Les techniques ind√©pendantes des mod√®les ( **model-agnostic methods** ) permettent de rendre les pr√©dictions individuelles plus interpr√©tables.

**Conclusion** : Approfondir une instance sp√©cifique permet de mieux comprendre les d√©cisions du mod√®le, m√™me lorsqu‚Äôil est complexe au niveau global.

#### Interpr√©tabilit√© locale pour un groupe de pr√©dictions

Comprendre pourquoi le mod√®le a fait des pr√©dictions sp√©cifiques pour un groupe d‚Äôinstances.

**M√©thodes disponibles** :

1. **Approches globales** : Appliquer des m√©thodes d'interpr√©tation globale, mais en consid√©rant le groupe comme s'il s'agissait de l'ensemble complet des donn√©es.
2. **Approches locales** : Utiliser des explications locales pour chaque instance individuelle, puis les combiner ou les agr√©ger pour le groupe.

### Qu'est ce qu'une explication humainement compr√©hensible

Une explication est une r√©ponse √† une question formul√©e avec un "Pourquoi" (Miller 2017)

- Pourquoi mon client va-t-il arr√©ter son contrat?
- Pourquoi mon pr√™t √† √©t√© rejett√©?

Le fait de donner une bonne explication √† √©t√© √©tudi√©e par Lipton en 1990.

Un humain ne souhaite pas saisir l'ensemble des causes d√©terminants une pr√©diction mais plut√¥t comprendre la pr√©diction a √©t√© r√©alis√©e plut√¥t qu'une autre.

Nous avons tendance √† penser √† des contres exemples pour comprendre une pr√©diction.

Combien serait estim√©e le prix de ma maison si j'augmente le nombre de pi√®ces de 1?

Si je demande un pr√™t √† la banque, je ne cherche pas √† comprendre tous les facteurs qui ont entrain√© mon rejet mais seulement ceux sur lequel je peux agir!

**Transpos√© au machine learning : Cela signifie**

Les humains pr√©f√®rent les explications contrastives, qui comparent une pr√©diction √† une autre situation hypoth√©tique ou r√©elle. Ces explications doivent √™tre adapt√©es au **contexte** et au **destinataire**, en choisissant un point de r√©f√©rence pertinent (par exemple, une maison similaire pour expliquer une pr√©diction de prix immobilier).

Les explications doivent √©galement √™tre courtes et s√©lectionn√©es : les gens attendent 1 √† 3 causes principales plut√¥t qu‚Äôune liste exhaustive. Ce ph√©nom√®ne, connu sous le nom **d‚Äôeffet Rashomon**, illustre qu‚Äôun √©v√©nement peut avoir plusieurs explications valables (chaine d'infos √† la TV). Les m√©thodes comme LIME, qui fournissent des explications simples et compr√©hensibles, sont bien adapt√©es √† cet objectif.

```mermaid

graph TD
    A[Interpretable Models For Machine Learning] --> B[Model-Agnostic Methods]
  
    B --> C[Global Model Agnostic]
    B --> D[Local Model Agnostic]
  
    C --> E1[Partial Dependence Plots]
    C --> E2[Accumulated Local Effects]
    C --> E3[Feature Interaction]
    C --> E4[Feature Importance]

    D --> F1[Individual Conditional Expectation]
    D --> F2[Local surogate AKA LIME]
    D --> F3[Shapley Values]
    D --> F4[Shapley Additive]


  
```

### Mod√®le Lin√©aire et interpr√©tation

R√©aliser un exemple avec une r√©gression li√©naire sur la vente de v√©lo

### Global Model Agnostic

Les m√©thodes globales d√©crivent le comportement **moyen** de votre mod√®le de Machine Learning. Elles sont particuli√®rement utiles lorsuq'il s'agit de comprendre les m√©canismes g√©n√©raux de votre mod√®le et ainsi le valid√© ou l'invalid√©.

Dans ce cours nous √©tudierons les m√©thodes suivantes :

- Partial dependance plot :  Effet marginal d'une variable (qualitative ou quantitvative) sur la target
- Accumulated Local Effect : `<remplir>`
- Feature Interaction (H-statistic) : Quantifie les effets joints des variables
- Feature Importance : Mesure l'effet d'une feature sur la fonction de perte

#### Partial Depence plot

##### Th√©orie

Le partial depence plot ou (PDP) nous montre l'effet marginal d'une ou 2 variables sur la target que nous chercons √† pr√©dire. PDP peut donc nous montrer la nature de la relation existante entre une variable du mod√®le et la target que celle ci soit li√©naire ou non lin√©aire, monotone ou m√™me plus complexe.

$\hat{f}_S(x_S) = \mathbb{E}_{X_C} \left[ \hat{f}(x_S, X_C) \right] $

Avec :

* $x_S$ : les variables pour lesquelles on veut analyser l'effet sur la pr√©diction.
* $X_C$ : les autres variables participant √† votre mod√®le.
* $\hat{f}$ : fonction de d√©pendance partielle

---

**Traduction algorithmique :**

Le mod√®le ayant d√©j√† √©t√© construit, on le calcule de la mani√®re suivante pour une variable $x_S$ d‚Äôun ensemble de donn√©es de taille ‚Äò‚Äôn‚Äô‚Äô (qui peut √™tre l‚Äô√©chantillon d‚Äôapprentissage) :

a. D√©finir une grille de M valeurs ($V_m$) √©galement r√©partis entre $min(x_S)$  et $max(x_S)$
b. Pour chaque valeur $V_m$

1. Remplacer, dans la matrice des descripteurs X, les valeurs de $x_S$ par $V_m$
2. Appliquer le mod√®le sur cette matrice pour obtenir les probabilit√©s d‚Äôaffectation (œÄ)
   √† la classe cible
3. Calculer les moyennes de ces probabilit√©s (ùúãÃÖùëö)
   c. Les couples ($V_m$, ùúãÃÖùëö) constituent les points du graphique de d√©pendance partielle

c. Visualiser les r√©sultats du partial depence plot.

---

##### Exemple et impl√©mentation:

```python
 from sklearn.inspection import partial_dependence, PartialDependenceDisplay,

# D√©finir nos variables d'int√©r√™ts dans une liste
features = ["temp","hum","windspeed"]  # Index des caract√©ristiques
_, ax1 = plt.subplots(figsize = (12,6))
PartialDependenceDisplay.from_estimator(rf, # votre mod√®le
                                         X_train, # Jeu d'entrainement
                                         features, # features
                                         kind="average", # Pour obtenir une PDP
                                         grid_resolution=50, Nombre de points estim√©s pour le tracer de la courbe
                                         ax = ax1 # Param√®tre de matplotlib
   
                                         )
plt.suptitle("Partial Dependence Plots - random- forest")
plt.tight_layout()
plt.show()

```

Cela √† pour effet de tracer les courbes de d√©pendences partielles suivantes :

![1736197591464](image/cours/1736197591464.png)

> Note :  Les donn√©es ont √©t√© normalis√©es avec un min_max_scaler

Temp√©rature :

Plus la temp√©rature augmente plus la vente de v√©los semble importantes avec un palier.

Humit√© :

Plus l'humidit√© augmente plus la vente de v√©lo va diminuer

Vitesse du vent:

Jusqu'√† 35km/h la vente de v√©lo ne change pas √©normement

Cas avec des variables cat√©gorielles :

```python
#On passe nos variables OHE
features = ["weathersit_1","weathersit_2","weathersit_3"]  # Index des caract√©ristiques
_, ax1 = plt.subplots(figsize = (12,6))
PartialDependenceDisplay.from_estimator(rf, 
                                         X_train, 
                                         features,
                                         categorical_features=["weathersit_1","weathersit_2","weathersit_3"], # On sp√©cifie ici les variables cat√©gorielles
                                         kind="average",
                                         grid_resolution=50,
                                         ax = ax1,
                                         n_cols=4
   
                                         )
plt.suptitle("Partial Dependence Plots - random- forest")
plt.tight_layout()
plt.show()

```

R√©sulats :

![1736198927081](image/cours/1736198927081.png)

Pour la variable weathersit_2 on peut remarquer une diff√©rence importante entre la modalit√© 1 et 0.

Il semble qu'un temps avec peu de nuage semble bien plus int√©ressant qu'un temps sans nuage.

Dernier cas, on souhaite maintenant comparer des paires de features.

```python
# PDP pour tracer des features par paires. 
# Attention fonctionne unqiuement par paire de m√™me type quali/quali ou quanti/quanti
features = ["temp","hum",("temp","hum"),("season_1","season_2"),'hr']  # Index des caract√©ristiques
_, ax1 = plt.subplots(figsize = (12,6))
PartialDependenceDisplay.from_estimator(rf, 
                                         X_train, 
                                         features,
                                         categorical_features=["season_1","season_2","hr"],
                                         kind="average",
                                         grid_resolution=50,
                                         ax = ax1
                                         )
plt.suptitle("Partial Dependence Plots - random- forest")
plt.tight_layout()
plt.show()

```

![1736199337344](image/cours/1736199337344.png)

---

##### Avatanges :

Les PDP sont simlle √† comprendre et permettent d'interpr√©ter des relations lin√©aire ou non li√©naires simplement.

Elles sont simple √† impl√©menter et permettent de voir les effets joints de 2 variables sur notre Target.

Si votre Feature n'est pas corr√©el√©es avec les autres pr√©dicteurs marginalis√©s l'interpr√©tation se fait facilement.

---

##### D√©sanvatages:

Le **nombre de features maximun** pouvant √™tre interpr√©t√©s √† la fois est de 2. Cela ne signifie pas que les PDP ne peuvent pas en utiliser plus mais il devient humainement impossible d'interpr√©ter des relations en Dimension 3 ou plus.

Peut donner des relations falatieuse si on examine pas la r√©elle distribution r√©elle des donn√©es.

Les graphiques de d√©pendance partielle (PDP) supposent que les variables √©tudi√©es sont ind√©pendantes des autres, ce qui peut mener √† des r√©sultats irr√©alistes lorsqu'elles sont corr√©l√©es.

**Exemple :**
Pour analyser l'effet de la taille et du poids sur la vitesse de marche, un PDP pourrait inclure des combinaisons improbables comme une taille de 2 m√®tres avec un poids inf√©rieur √† 50 kg. Cela cr√©e des points dans des zones o√π la probabilit√© r√©elle est tr√®s faible, rendant les r√©sultats moins fiables.

#### Accumulated Loccal Effect

##### Th√©orie

Lorsque les variables sont corr√©l√©es entre elle une alternative existe. Elle permet √©galement d'√©xaminer l'influence d'une feature sur votre target tout en √©tant non biais√© et moins couteuse en temps de calcul (pas de calcul sur l'ensemble des donn√©es).

Intuition :

Prenons l'exemple d'un jeu de donn√©es o√π l'on cherche le prix d'une maison sur le march√©.

Pour cela nous avons des informations sur le bien comme , le nombre de pi√®ces, la superficie, le type de pi√®ce, ...

On lance un partial d√©pendance plot pour expliquer le prix du bien avec le nombre de pi√®ces disponible tout en fixant les autres variables.

üìè Une variable fix√©e est la superfie du bien. Disons que celui-ci √† pour valeur moyenne 40m¬≤ (on est √† paris)üìè

> Probl√®me nos features sont cor√©ll√©e et cela nous am√®ne dans un espace qui n'a pas sens dans la r√©alit√©. Par exemple un appartement de 10 pi√®ces qui feraient 40m¬≤...

Dans ce type de cas les PDP offrent une repr√©sentation irr√©aliste de votre jeu de donn√©es et de fait une interpr√©tation falacieuse.

Exemple : 2 Features corr√©l√©es X1 & X2

![1735853902866](image/cours/1735853902866.png)

Graphique de de l'ALE:

![1735854580515](image/cours/1735854580515.png)

> On divise l'espace en 5 intervals suivant X1. Pour chaque individu dans chaque interval, nous calculons la diff√©rence de pr√©diction en remplacant les valeurs de X1 par la borne inf et la borne max de l'interval.

Traduction algorithmique :

1. Diviser les valeurs de votre features en interval (quantile ou interval √©gaux)
2. Affecter chaque instance √† son interval et doubler les instance une pour le lower bond et l'autre pour le upper bon
3. Pour chaque instance calcule f(lower_bond) et le f(upper_bond)
4. Calculer la diff√©rence entre les 2 et moyenniser le r√©ultat
5. Tracer le ALE

**Traduction algorithmique : Accumulated Local Effects (ALE)**

---

√âtape 1 : D√©finir une grille d'intervalles ($I_k$) pour la feature $x_S$

1. Diviser les valeurs de $x_S$ en $K$ intervalles √©gaux ou bas√©s sur les quantiles.
   - $I_k = [b_{k-1}, b_k)$ avec $k \in [1, K]$.

---

√âtape 2 : Calculer les diff√©rences locales pour chaque intervalle

Pour chaque intervalle $I_k$ :

1. Identifier les instances $X_k$ dont la valeur de $x_S$ appartient √† $I_k$.
2. Pour chaque instance $i$ dans $X_k$ :

   - Remplacer $x_S$ par la borne inf√©rieure $b_{k-1}$ de $I_k$ et pr√©dire :
   - $f_{i,\text{lower}} = f(X_i | x_S = b_{k-1})$
   - Remplacer $x_S$ par la borne sup√©rieure $b_k$ de $I_k$ et pr√©dire :
   - $f_{i,\text{upper}} = f(X_i | x_S = b_k)$
   - Calculer la diff√©rence locale pour l'instance :
   - $\Delta f_i^k = f_{i,\text{upper}} - f_{i,\text{lower}}$
3. Moyenniser les diff√©rences locales pour l'intervalle :

   $\Delta f^k = \frac{1}{|X_k|} \sum_{i \in X_k} \Delta f_i^k$

> üí°Prendre 2 bornes petites permet de faire varifer votre feature dans les 2 sens et ainsi observer les effets d'un chagement d'une petite quantit√© et l'effet sur votre pr√©diction..On moyennise ensuite cet effet pour chaque borne.

---

√âtape 3 : Calculer les effets accumul√©s (ALE)

1. Initialiser $ALE_1 = 0$.
2. Pour chaque intervalle $I_k$ ($k > 1$) :

   - Accumuler les effets locaux :
     $ALE_k = ALE_{k-1} + \Delta f^k$
3. Optionnel : Centrer les ALE autour de z√©ro :

   $ALE_k = ALE_k - \frac{1}{K} \sum_{k=1}^K ALE_k$

> Note :  L'accumulation des effets permet d'interpr√©ter l'ALE comme une courbe continue. Si elle est monotone et croissante cela signifique que chaque borne influence positivement notre variable cible.

---

√âtape 4 : Visualiser les r√©sultats

1. Construire les couples $(x_S^k, ALE_k)$, o√π $x_S^k$ est le centre ou la borne sup√©rieure de chaque intervalle.
2. Tracer un graphique avec :
   - $x_S^k$ en abscisses,
   - $ALE_k$ en ordonn√©es.

---

##### Exemple et impl√©mentation:

Pour impl√©menter les ALE en python vous pouvez utiliser le package `ALIBI` celui-ci est nettement √©volu√© que son √©quivalent en dans le langage R [ALEPlot R](https://cran.r-project.org/web/packages/ALEPlot/index.html) ou [iml](https://cran.r-project.org/web/packages/iml/index.html) .

```python
from alibi.explainers import ALE, plot_ale

rf_ale = ALE(rf.predict, #Methode predict de votre mod√®le
             feature_names=features_names, # Liste des features o√π il faut calculer l'ALE
             target_names=["bike sell"] # Nom de la target
) 
#Calcul des ALE, attention il faut un format numpy arrray
rf_exp = rf_ale.explain(X_train.to_numpy()) 

#Plot pour l'interpr√©tation

_, ax1 = plt.subplots(figsize = (10,8))
plot_ale(rf_exp, #R√©sultats des ALE
	 features=["temp","hum","windspeed"], # Feature √† repr√©senter
	 ax= ax1, 
	 targets=[0] # Si classification mutliple, passer le nom de toutes les modalit√©s √† pr√©dire
)
```

![1736285515642](image/cours/1736285515642.png)

Ici l'interpr√©tation est essentiellement qualitative. On cherche √† √©xaminer l'int√©raction entre notre Feature et la target.

Pour rappel, la valeur de l'ALE en un point se lit de la fa√ßon suivante :

Une hausse de l'humidit√© sur l'interval [0.45,0.55] diminue la location de v√©lo de 5 unit√©s en en tenant compte de l'influence des autres variables. Cet effet n'est valable que pour l'interval 0.45,0.55

##### Avantages:

Les **ALE sont non biais√©s** en pr√©sence de features corr√©el√©es a la diff√©rence des PDP car marginaliseront des combinaisons improbables de donn√©es.

**ALE  sont plus rapide √† calculer que les** PDPs qui ont une compl√©xit√© O(n) alors que celui des ALE est $O(B_k)$

**Les ALE plot** **sont centr√©es** en 0 ce qui facilite leur intr√©p√©tation. La lecture se fait comprativement √† la moyenne des pr√©diction

##### D√©savantages :

Fixer son interval peut √™tre relativement compliqu√© et peut parfois produire des ALE plot tr√®s compliqu√© √† lire. Dans ce cas diminuer le nombre d'intervalles.

La moyennisation des effets ne permet pas de voir l'h√©t√©rog√©t√©s des pr√©dictions si elle existe.

L'impl√©mentation et la compr√©hension est moins intuitive que les PDP

M√™me si les trac√©s ALE ne sont pas biais√©s en cas de caract√©ristiques corr√©l√©es, l‚Äôinterpr√©tation reste difficile lorsque les caract√©ristiques sont fortement corr√©l√©es. Lors d'une tr√®s forte corr√©lation, il est logique d‚Äôanalyser l‚Äôeffet de la modification des deux caract√©ristiques ensemble et non isol√©ment. Cet inconv√©nient n‚Äôest pas sp√©cifique aux trac√©s ALE, mais constitue un probl√®me g√©n√©ral de caract√©ristiques fortement corr√©l√©es.

#### Feature interaction

##### Th√©orie

Quand nos features int√©ragissent entre elles dans un mod√®le notre pr√©diction ne peut √™tre exprim√©e comme une somme ind√©pendante de nos features. Car la valeur d'une feature d√©pendant directement de la valeur d'une autre.

> Exemple : Si $X_1$ repr√©sente l'√¢ge et $X_2$ repr√©sente le revenu, leur interaction pourrait d√©terminer la probabilit√© qu'un individu souscrive un pr√™t (par exemple, les jeunes avec un revenu √©lev√© pourraient √™tre plus enclins √† souscrire que les personnes √¢g√©es avec le m√™me revenu).

Si un mod√®le de machine learning r√©alise des pr√©dictions bas√©es sur 2 features. Nous pouvons d√©comper la pr√©diction en 4 termes :

- Une constante
- Un effet du premier feature
- Un effet du second feature
- L'effet combin√© des 2 features

Exemple d'un mod√®le pr√©disant la valeur d'un bien immobilier avec 2 features, taille de la maison (petit ou grand ) et la localisation (bien ou mauvais).

| localisation | taille | Prediction |
| -----------: | -----: | ---------: |
|         bien |  grand |    300,000 |
|         bien |  petit |    200,000 |
|     mauvaise |  grand |    250,000 |
|     mauvaise |  petit |    150,000 |

Ici, on d√©compose les pr√©dictions du mod√®le en:

- Terme constant : 150 000$
- taille : 100 000 $ si grand, 0 sinon
- localisation : 50 000 $ si bien, 0 sinon

La d√©composition est pleinement expliqu√©e ici, il n'y a pas d'effet d'int√©raction. L'effet indivuel des variables permet d'expliquer √† 100% votre mod√®le.

Maintenant un exemplea avec interaction:

| localisation | taille | Prediction |
| -----------: | -----: | ---------: |
|         bien |  grand |    400,000 |
|         bien |  petit |    200,000 |
|     mauvaise |  grand |    250,000 |
|     mauvaise |  petit |    150,000 |

On d√©compose la pr√©diction en :

- Un terme constant: 150 000$
- L'effet taille : 100 000$ si grand, 0 sinon
- L'effet localisation : 50 000$ si bien, 0 sinon
- L'effet interaction taille/localisation : 100 000 $  si grand et bien, 0sinon

Une mani√®re de mesurer cette int√©raction est de calculer de combien varie la pr√©diction de notre mod√®le bas√©e sur une l√©g√®re variation des effets d'int√©raction.

Cette m√©thode s'appelle le **Friedman's H-statistic**

##### R√©sum√© : H-statistic de Friedman

---

##### 2. D√©finition de base des interactions

- **Absence d'interaction entre deux features :**
  $PD_{jk}(x_j, x_k) = PD_j(x_j) + PD_k(x_k)$

  - $PD_{jk}$ : Partial Dependence Function (PDP) combin√©e des deux features.
  - $PD_j$, $PD_k$ : PDP de chaque feature s√©par√©ment.
- **Absence d'interaction entre une feature et toutes les autres :**
  $\hat{f}(x) = PD_j(x_j) + PD_{-j}(x_{-j})$

  - $\hat{f}(x)$ : Pr√©diction totale.
  - $PD_{-j}(x_{-j})$ : PDP combin√©e pour toutes les features sauf $j$.

---

###### 3. Calcul de la H-statistic

La H-statistic mesure la variance expliqu√©e par la diff√©rence entre le comportement observ√© (avec interactions) et celui sans interactions.

###### Interaction entre deux features $(H_{jk}^2)$ :

$H_{jk}^2 = \frac{\sum_{i=1}^n \big[ PD_{jk}(x_j^{(i)}, x_k^{(i)}) - PD_j(x_j^{(i)}) - PD_k(x_k^{(i)}) \big]^2}{\sum_{i=1}^n PD_{jk}^2(x_j^{(i)}, x_k^{(i)})}$

- **Num√©rateur :** Variance de la diff√©rence entre $PD_{jk}$ (PDP combin√©e) et $PD_j + PD_k$ (PDP individuelles).
- **D√©nominateur :** Variance totale de $PD_{jk}$.

###### Interaction entre une feature et toutes les autres $(H_j^2)$ :

$H_j^2 = \frac{\sum_{i=1}^n \big[ \hat{f}(x^{(i)}) - PD_j(x_j^{(i)}) - PD_{-j}(x_{-j}^{(i)}) \big]^2}{\sum_{i=1}^n \hat{f}^2(x^{(i)})}$

- **Num√©rateur :** Variance expliqu√©e par la diff√©rence entre $\hat{f}(x)$ (pr√©diction totale) et $PD_j + PD_{-j}$ (PDP sans interactions).
- **D√©nominateur :** Variance totale des pr√©dictions $\hat{f}(x)$.

---

###### 4. Interpr√©tation des r√©sultats

- $H = 0$ : Aucune interaction.
- $H = 1$ : L'effet des features provient uniquement des interactions (leurs PDP individuelles sont constantes).
- $H > 1$ : Rare, cela peut arriver si la variance des interactions d√©passe la variance totale, mais ce cas est difficile √† interpr√©ter.

---

###### 5. Probl√®mes pratiques du calcul

- **Complexit√© computationnelle :**  Le calcul de la statisique n√©cesside au mieux $2n^2$ pour calculer la H-statistic (j vs. k) et $3n^2$ pour la H-statistic (j vs. all).
- **√âchantillonnage :** R√©duit la complexit√©, mais peut rendre les r√©sultats instables.

---

##### Exemple et impl√©mentation:

- Utilisez la **H-statistic** pour d√©tecter et quantifier les interactions importantes.
- Si des interactions fortes sont d√©tect√©es :

  - Adaptez le mod√®le (e.g., mod√®les non lin√©aires, termes d'interaction explicites).
  - R√©alisez une analyse approfondie des interactions pour guider l'am√©lioration ou l'interpr√©tation du mod√®le.

```python
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.svm import LinearSVR

#Fichier de donn√©es contenant les ventes de v√©lo tous les 5 jours
bikes = fetch_openml("Bike_Sharing_Demand", version=2, as_frame=True)

X, y = bikes.data.copy(), bikes.target

# We use only a subset of the data to speed up the example.
X = X.iloc[::5, :]
y = y[::5]
```

```python
categorical_features = X.select_dtypes(include=['category']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features)
    ])

# Cr√©ation du pipeline complet
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=0, min_samples_leaf=10, max_depth=3,min_samples_split=5))
])

# Random Forest Regressor
model = pipeline.fit(X, y)


```

```python
# On restreint le nombre d'observations √† pr√©dire pour acc√©lerer le compute
random.seed(8)
X_exp = random.choices(X.to_numpy(), k=100)
X_exp = pd.DataFrame(X_exp, columns=X.columns)

h_stat = FriedmanHStatisticMethod()
h_stat.fit(model, X_exp)

```

```python
# Participation de votre feature et son interaction dans la variance globale
h_stat.plot(vis_type="bar_chart_ova")
```

> Ce graphique montre la pusisance d'int√©raction (H-statistic) pour chaque feature avec les autres . Ici les effets d'int√©ractions en entre les features son vraiment faibles (mois de 10% de la variance expliqu√©e par feature).

![1736288841144](image/cours/1736288841144.png)

Les effets d'int√©ractions :

```python
# Pair d'int√©raction et son intensit√©
h_stat.plot(vis_type="bar_chart", ,top_k=5 )
```

> On peut √©galement observer les effets d'int√©raction par paire de variables.

![1736288893942](image/cours/1736288893942.png)

##### Avantages :

- L'interpr√©tation se fait facilement, on repr√©sente la part de variance expliqu√©e par l'int√©raction de la feature et non pas son effet individuel.
- On peut comparer cette statistique d'un mod√®le √† l'autre
- Elle d√©tecte toute forme d'int√©raction

##### D√©savantages

- La statistique est tr√®s tr√®s couteuse en compute et nous force √† travailler sur des √©chantillons
- Pas de tests statistiques fournies pour emettre une hypoth√®se d'int√©raction ou non
- Pas de threshold pour d√©finir une int√©raction par exemple int√©raction >0.3

#### Permutation Feature importance

##### Th√©orie

‚ö†Ô∏è Connaitre la notion de feature importance est un pr√©-requis.

Le concept est tr√®s simple : nous mesurons l‚Äôimportance d‚Äôune feature en calculant l‚Äôaugmentation de l‚Äôerreur de pr√©diction du mod√®le apr√®s permutation des caract√®ristiques du feature.

Une caract√©ristique est **¬´ importante ¬ª** si le m√©lange de ses valeurs augmente l'erreur du mod√®le, car dans ce cas, le mod√®le s'est appuy√© sur la caract√©ristique pour la pr√©diction.

Une caract√©ristique est **¬´ sans importance ¬ª** si le m√©lange de ses valeurs laisse l‚Äôerreur du mod√®le inchang√©e, car dans ce cas, le mod√®le a ignor√© la caract√©ristique pour la pr√©diction.

###### Traduction algorithmique : Feature permutation importance

---

Input: Un mod√®le entrain√© $\hat{f}$, une matrice de vos features $X$, un vecteur contenant la target $y$, la mesure des erreurs de pr√©dictions $L(y,\hat{f})$.

1. Estimer les erreurs originelles du mod√®le $e_{orig} = L(y, \hat{f}(X))$  (i.e. mean squared error)
2. Pour chaque feature $j \in \{1,...,p\}$ faire:
   - G√©n√©rer une matrice de feature $X_{perm}$ en permutant  la feature j dans le jeu de donn√©es X. Cela aura pour effet de "casser" l'association entre la feature j et y.
   - Estimer l'erreur $e_{perm} = L(Y,\hat{f}(X_{perm}))$ bas√©e sur les pr√©dictions des donn√©es permut√©es.
   - Calculer la permutation feature importance comme un quotient $FI_j= e_{perm}/e_{orig}$ ou la diff√©rence $FI_j = e_{perm}- e_{orig}$
3. Ordonner les  features par desc FI.

---

##### Exemple et impl√©mentation :

```python
from sklearn.inspection import permutation_importance
random_permutation = permutation_importance(model, X, y,
                           			n_repeats=30,
                           			random_state=0)
```

![1736032999038](image/cours/1736032999038.png)

> Permuter la variable season conduit √† une augmentation du MSE de 0.13

##### Avantages :

- Facilter √† interpr√©ter : Le feature importance montre de combien on augmente l'erreur du mod√®le quand l'information est d√©truite
- La permutation ne n√©cessite pas de r√©entrainer le mod√®le !!
- La permutation supprime l'effet univari√©e de notre variable sur la target mais √©galement les effets joints avec la distribution des autres variables. Cela √† tendance √† surestimer l'effet de la variable car elle porte l'effet individuel et l'effet collectif

##### D√©savantages:

- Comme on permute la valeur des features de fa√ßon al√©atoire cela introduit un biais. Si on relance une seconde fois la permutation peut √™tre diff√©rent et donner des r√©sultats totalement diff√©rent. Pour diminuer cette effet il est conseiller de r√©aliser plusieurs fois cette permutation.
- Comme pour les PDP, si les features sont cor√©ll√©es ont peut biaiser les r√©sulats avec des valeurs n'existant pas
