### Mod√®le Lin√©aire et interpr√©tation

Pour illustrer ce cours nous allons utiliser le dataset Bike Rental Data. Celui-ci regroupe des informations journali√®res sur la location de v√©lo par une agence.

Le dataset contient des features sur :

- Donn√©es m√©t√©o :  Temp√©rature, humidit√©, vent, ...
- Dates : Heures, jours, jours f√©ri√©s, vacances, ...
- Le nombre de v√©lo lou√© par jour

Voici l'exemple d'une instance pour mieux comprendre :

| dteday     | season | yr | mnth | hr | holiday | weekday | workingday | weathersit | temp | atemp  | hum | windspeed |
| ---------- | ------ | -- | ---- | -- | ------- | ------- | ---------- | ---------- | ---- | ------ | --- | --------- |
| 2011-01-01 | 1      | 0  | 1    | 0  | 0       | 6       | 0          | 1          | 3.28 | 3.0014 | 81  | 0         |

On r√©alise une r√©gression sur ce mod√®le pour mieux comprendre les liens entre variables explicatives et la target (vente de v√©lo)

```python
from sklearn.linear_model import LinearRegression
from ucimlrepo import fetch_ucirepo 

# fetch dataset 
bike_sharing = fetch_ucirepo(id=275) 
  
# data (as pandas dataframes) 
X = bike_sharing.data.features 
y = bike_sharing.data.targets 


#Initialiser la r√©gression lin√©aire
lr = LinearRegression()
#Fit the model
lr.fit(X.drop('dteday',axis=1), y)

#On affiche la valeur des co√©fficients pour pouvoir les comparer
print(pd.DataFrame(np.abs(lr.coef_[0]),index=X.drop('dteday',axis=1).columns,columns=['abs(coef)']))
```

Les donn√©es √©tant centr√©es et r√©duites nous pouvons comparer les coefficients et leur ordre de grandeur. Ici les donn√©es sont en valeur absolue, on cherche uniquement √† voir celle qui affecte le plus le mod√®le.

| Variable   | abs(coef)            | coef                  |
| ---------- | -------------------- | --------------------- |
| season     | 19.899338            | 19.899338             |
| yr         | 81.087156            | 81.087156             |
| mnth       | 0.008648             | -0.008648             |
| hr         | 7.670597             | 7.670597              |
| holiday    | 21.879216            | -21.879216            |
| weekday    | 1.878354             | 1.878354              |
| workingday | 3.939225             | 3.939225              |
| weathersit | 3.432098             | -3.432098             |
| temp       | 78.149780            | 78.149780             |
| atemp      | **233.157087** | 233.157087            |
| hum        | **198.184681** | -**198.184681** |
| windspeed  | 41.565215            | 41.565215             |

On remarque ici que les features atemp, hum et yr sont les trois variables les plus importantes du mod√®le. On peut compl√©ter ces informations en regardant √©galement le signe des co√©fficients.

On peut √©galement interpr√©ter la qualit√© de notre mod√®le avec son $RMSE = 141$  et son $R^2 = 0.38$

![1736371716827](image/global_model_agnostic/1736371716827.png)

**Conclusion :**

Les mod√®les lin√©aires offrent une capacit√© d'interpr√©tation simpliste. On comprend rapidement comment le mod√®le pr√©dit un individu et quels sont ses param√®tres internes.

En revanche, ces interpr√©tations se basent souvent sur des mod√®les offrant une capacit√© de g√©n√©ralisation bien faible (RMSE,R¬≤ faible) et ne parviennent pas √† capturer des donn√©es complexes comportant des relations non li√©naires.

### Global Model Agnostic

Les m√©thodes globales d√©crivent le comportement **moyen** de votre mod√®le de Machine Learning. Elles sont particuli√®rement utiles lorsqu'il s'agit de comprendre les m√©canismes g√©n√©raux de votre mod√®le et ainsi le valider ou l'invalider.

Dans ce cours nous √©tudierons les m√©thodes suivantes :

- **Partial dependance plot :**  Effet marginal d'une variable (qualitative ou quantitvative) sur la target
- **Accumulated Local Effect :** Effet marginal d'une variable par interval sur la target (quanti)
- **Feature Interaction (H-statistic) :** Quantifie les effets joints des variables
- **Feature Importance :** Mesure l'effet d'une feature sur la fonction de perte

#### Partial Depence plot

##### Th√©orie

Le partial depence plot ou (PDP) nous montre l'effet marginal d'une ou 2 variables sur la target que nous cherchons √† pr√©dire. PDP peut donc nous montrer la nature de la relation existante entre une variable du mod√®le et la target que celle-ci soit linaire ou non lin√©aire, monotone ou m√™me plus complexe.

$\hat{f}_S(x_S)=E_{X_C}\left[\hat{f}(x_S,X_C)\right]=\int\hat{f}(x_S,X_C)d\mathbb{P}(X_C)$

Avec :

* $x_S$ : les variables pour lesquelles on veut analyser l'effet sur la pr√©diction.
* $X_C$ : les autres variables participant √† votre mod√®le.
* $\hat{f}$ : fonction de d√©pendance partielle

Pour une unique variable on peut d√©finir la PDP comme suiit :

---

**Traduction algorithmique :**

Le mod√®le ayant d√©j√† √©t√© construit, on le calcule de la mani√®re suivante pour une variable $x_S$ d‚Äôun ensemble de donn√©es de taille ‚Äò‚Äôn‚Äô‚Äô (qui peut √™tre l‚Äô√©chantillon d‚Äôapprentissage) :

a. D√©finir une grille de M valeurs ($V_m$) √©galement r√©partis entre $min(x_S)$  et $max(x_S)$

b. Pour chaque valeur $V_m$

1. Remplacer, dans la matrice des descripteurs X, les valeurs de $x_S$ par $V_m$
2. Appliquer le mod√®le sur cette matrice pour obtenir les probabilit√©s d‚Äôaffectation (œÄ)

   √† la classe cible
3. Calculer les moyennes de ces probabilit√©s (ùúãÃÖùëö)

   c. Les couples ($V_m$, ùúãÃÖùëö) constituent les points du graphique de d√©pendance partielle

c. Visualiser les r√©sultats du partial depence plot.

---

##### Exemple et impl√©mentation:

```python

 from sklearn.inspection import partial_dependence, PartialDependenceDisplay


# D√©finir nos variables d'int√©r√™ts dans une liste

features = ["temp","hum","windspeed"]  # Index des caract√©ristiques

_, ax1 = plt.subplots(figsize = (12,6))

PartialDependenceDisplay.from_estimator(rf, # votre mod√®le

                                         X_train, # Jeu d'entrainement

                                         features, # features

                                         kind="average", # Pour obtenir une PDP

                                         grid_resolution=50, Nombre de points estim√©s pour le tracer de la courbe

                                         ax = ax1 # Param√®tre de matplotlib

   

                                         )

plt.suptitle("Partial Dependence Plots - random- forest")

plt.tight_layout()

plt.show()


```

Cela √† pour effet de tracer les courbes de d√©pendences partielles suivantes :

![1736367856892](image/cours/pdp_plot_hum_temp_wind.png)

**Temp√©rature :**

Plus la temp√©rature augmente plus la vente de v√©los semble importantes avec un palier.

**Humit√© :**

Plus l'humidit√© augmente plus la vente de v√©lo va diminuer

**Vitesse du vent:**

Jusqu'√† 35km/h la vente de v√©lo ne change pas √©normement

Cas avec des variables cat√©gorielles :

```python

#On passe nos variables OHE

features = ["weathersit_1","weathersit_2","weathersit_3"]  # Index des caract√©ristiques

_, ax1 = plt.subplots(figsize = (12,6))

PartialDependenceDisplay.from_estimator(rf, 

                                         X_train, 

                                         features,

                                         categorical_features=["weathersit_1","weathersit_2","weathersit_3"], # On sp√©cifie ici les variables cat√©gorielles

                                         kind="average",

                                         grid_resolution=50,

                                         ax = ax1,

                                         n_cols=4

   

                                         )

plt.suptitle("Partial Dependence Plots - random- forest")

plt.tight_layout()

plt.show()


```

R√©sulats :

![1736367984471](image/cours/pdp_univariate_categorie.png)

Pour la variable weathersit_2 on peut remarquer une diff√©rence importante entre la modalit√© 1 et 0.

Il semble qu'un temps avec peu de nuages semble bien plus int√©ressant qu'un temps sans nuage.

Dernier cas, on souhaite maintenant comparer des paires de features.

```python

# PDP pour tracer des features par paires. 

# Attention fonctionne unqiuement par paire de m√™me type quali/quali ou quanti/quanti

features = ["temp","hum",("temp","hum"),("season_1","season_2"),'hr']  # Index des caract√©ristiques

_, ax1 = plt.subplots(figsize = (12,6))

PartialDependenceDisplay.from_estimator(rf, 

                                         X_train, 

                                         features,

                                         categorical_features=["season_1","season_2","hr"],

                                         kind="average",

                                         grid_resolution=50,

                                         ax = ax1

                                         )

plt.suptitle("Partial Dependence Plots - random- forest")

plt.tight_layout()

plt.show()


```

![1736368054392](image/cours/pdp_bivariate.png)

---

##### Avatanges :

Les PDP sont simples √† comprendre et permettent d'interpr√©ter des relations lin√©aires ou non lin√©aires.

Elles sont simples √† impl√©menter et permettent de voir les effets joints de 2 variables sur notre Target.

Si votre Feature n'est pas corr√©l√©e avec les autres pr√©dicteurs marginalis√©s l'interpr√©tation est valide.

---

##### D√©sanvatages:

Le **nombre de features maximum** pouvant √™tre interpr√©t√©es √† la fois est de 2. Cela ne signifie pas que les PDP ne peuvent pas en utiliser plus mais il devient humainement impossible d'interpr√©ter des relations en Dimension 3 ou plus.

Peut donner des relations fallacieuses si on n'examine pas la r√©elle distribution r√©elle des donn√©es.

Les graphiques de d√©pendance partielle (PDP) supposent que les variables √©tudi√©es sont ind√©pendantes des autres, ce qui peut mener √† des r√©sultats irr√©alistes lorsqu'elles sont corr√©l√©es.

**Exemple :**

> Pour analyser l'effet de la taille et du poids sur la vitesse de marche, un PDP pourrait inclure des combinaisons improbables comme une taille de 2 m√®tres avec un poids inf√©rieur √† 50 kg. Cela cr√©e des points dans des zones o√π la probabilit√© r√©elle est tr√®s faible, rendant les r√©sultats moins fiables.

#### Accumulated Loccal Effect

##### Th√©orie

Lorsque les variables sont corr√©l√©es entre elles, une alternative existe. Cette alternative permet d'√©valuer l'influence d'une feature sur votre target tout en √©tant non biais√©e et moins co√ªteuse en temps de calcul (pas de calcul sur l'ensemble des donn√©es).

**Intuition :**

Prenons l'exemple d'un jeu de donn√©es o√π l'on cherche le prix d'une maison sur le march√©.

Pour cela nous avons des informations sur le bien comme, le nombre de pi√®ces, la superficie, le type de pi√®ce, ...

On lance un partial d√©pendance plot pour expliquer le prix du bien avec le nombre de pi√®ces disponibles tout en fixant les autres variables.

üìè Une variable fix√©e est la superfie du bien. Disons que celui-ci √† pour valeur moyenne 40m¬≤ (on est √† paris)üìè

> Probl√®me nos features sont corr√©l√©es et cela nous am√®ne dans un espace qui n'a pas sens dans la r√©alit√©. Par exemple un appartement de 10 pi√®ces qui feraient 40m¬≤...

Dans ce type de cas les PDP offrent une repr√©sentation irr√©aliste de votre jeu de donn√©es et de fait une interpr√©tation fallacieuse.

Exemple : 2 Features corr√©l√©es X1 & X2

![1735853902866](image/cours/pdp_distribution_probleme.png)

Graphique de de l'ALE:

![1735854580515](image/cours/ale_intuition.png)

> On divise l'espace en 5 intervalles suivant X1. Pour chaque individu dans chaque interval, nous calculons la diff√©rence de pr√©diction en remplacant les valeurs de X1 par la borne inf et la borne max de l'interval.

**Traduction intuitive:**

1. Diviser les valeurs de votre features en interval (quantile ou interval √©gaux)
2. Affecter chaque instance √† son interval et doubler les instance une pour le lower bond et l'autre pour le upper bon
3. Pour chaque instance calcule f(lower_bond) et le f(upper_bond)
4. Calculer la diff√©rence entre les 2 et moyenniser le r√©ultat
5. Tracer le ALE

**Traduction algorithmique : Accumulated Local Effects (ALE)**

---

√âtape 1 : D√©finir une grille d'intervalles ($I_k$) pour la feature $x_S$

1. Diviser les valeurs de $x_S$ en $K$ intervalles √©gaux ou bas√©s sur les quantiles.

   - $I_k = [b_{k-1}, b_k)$ avec $k \in [1, K]$.

---

√âtape 2 : Calculer les diff√©rences locales pour chaque intervalle

Pour chaque intervalle $I_k$ :

1. Identifier les instances $X_k$ dont la valeur de $x_S$ appartient √† $I_k$.
2. Pour chaque instance $i$ dans $X_k$ :

   - Remplacer $x_S$ par la borne inf√©rieure $b_{k-1}$ de $I_k$ et pr√©dire :
   - $f_{i,\text{lower}} = f(X_i | x_S = b_{k-1})$
   - Remplacer $x_S$ par la borne sup√©rieure $b_k$ de $I_k$ et pr√©dire :
   - $f_{i,\text{upper}} = f(X_i | x_S = b_k)$
   - Calculer la diff√©rence locale pour l'instance :
   - $\Delta f_i^k = f_{i,\text{upper}} - f_{i,\text{lower}}$
3. Moyenniser les diff√©rences locales pour l'intervalle :

   $\Delta f^k = \frac{1}{|X_k|} \sum_{i \in X_k} \Delta f_i^k$

> üí°Prendre 2 bornes petites permet de faire varier votre feature dans les 2 sens et ainsi observer les effets d'un chagement d'une petite quantit√© et l'effet sur votre pr√©diction..On moyennise ensuite cet effet pour chaque borne.

---

√âtape 3 : Calculer les effets accumul√©s (ALE)

1. Initialiser $ALE_1 = 0$.
2. Pour chaque intervalle $I_k$ ($k > 1$) :

   - Accumuler les effets locaux :

     $ALE_k = ALE_{k-1} + \Delta f^k$
3. Optionnel : Centrer les ALE autour de z√©ro :

   $ALE_k = ALE_k - \frac{1}{K} \sum_{k=1}^K ALE_k$

> Note :  L'accumulation des effets permet d'interpr√©ter l'ALE comme une courbe continue. Si elle est monotone et croissante cela signifique que chaque borne influence positivement notre variable cible.

---

√âtape 4 : Visualiser les r√©sultats

1. Construire les couples $(x_S^k, ALE_k)$, o√π $x_S^k$ est le centre ou la borne sup√©rieure de chaque intervalle.
2. Tracer un graphique avec :

   - $x_S^k$ en abscisses,
   - $ALE_k$ en ordonn√©es.

---

##### Exemple et impl√©mentation:

Pour impl√©menter les ALE en python vous pouvez utiliser le package `ALIBI` celui-ci est nettement moins √©volu√© que son √©quivalent en dans le langage R [ALEPlot R](https://cran.r-project.org/web/packages/ALEPlot/index.html) ou [iml](https://cran.r-project.org/web/packages/iml/index.html) .

```python

from alibi.explainers importALE, plot_ale


rf_ale = ALE(rf.predict, #Methode predict de votre mod√®le

             feature_names=features_names, # Liste des features o√π il faut calculer l'ALE

             target_names=["bike sell"] # Nom de la target

) 

#Calcul des ALE, attention il faut un format numpy arrray

rf_exp = rf_ale.explain(X_train.to_numpy()) 


#Plot pour l'interpr√©tation


_, ax1 = plt.subplots(figsize = (10,8))

plot_ale(rf_exp, #R√©sultats des ALE

     features=["temp","hum","windspeed"], # Feature √† repr√©senter

     ax= ax1, 

     targets=[0] # Si classification mutliple, passer le nom de toutes les modalit√©s √† pr√©dire

)

```

![1736368184409](image/cours/ale_plot.png)

Ici l'interpr√©tation est essentiellement qualitative. On cherche √† √©xaminer l'interaction entre notre Feature et la target.

Pour rappel, la valeur de l'ALE en un point se lit de la fa√ßon suivante :

Une hausse de l'humidit√© sur l'interval [0.45,0.55] diminue la location de v√©lo de 5 unit√©s en en tenant compte de l'influence des autres variables. Cet effet n'est valable que pour l'interval 0.45,0.55

##### Avantages:

Les **ALE sont non biais√©s** en pr√©sence de features corr√©el√©es √† la diff√©rence des PDP car marginaliseront des combinaisons improbables de donn√©es.

**ALE  sont plus rapides √† calculer que les** PDPs qui ont une complexit√© O(n) alors que celui des ALE est $O(B_k)$

Les ALE plot **sont centr√©es** en 0 ce qui facilite leur intr√©p√©tation. La lecture se fait comprativement √† la moyenne des pr√©diction

##### D√©savantages :

Fixer son intervalle peut √™tre relativement compliqu√© et peut parfois produire des ALE plots tr√®s compliqu√©s √† lire. Dans ce cas diminuer le nombre d'intervalles.

La moyennisation des effets ne permet pas de voir l'h√©t√©rog√©n√©it√©  des pr√©dictions si elle existe.

L'impl√©mentation et la compr√©hension sont moins intuitives que les PDP

M√™me si les trac√©s ALE ne sont pas biais√©s en cas de caract√©ristiques corr√©l√©es, l‚Äôinterpr√©tation reste difficile lorsque les caract√©ristiques sont fortement corr√©l√©es. Lors d'une tr√®s forte corr√©lation, il est logique d‚Äôanalyser l‚Äôeffet de la modification des deux caract√©ristiques ensemble et non isol√©ment. Cet inconv√©nient n‚Äôest pas sp√©cifique aux trac√©s ALE, mais constitue un probl√®me g√©n√©ral de caract√©ristiques fortement corr√©l√©es.

#### Feature interaction

##### Th√©orie

Quand nos features interagissent entre elles dans un mod√®le notre pr√©diction ne peut √™tre exprim√©e comme une somme ind√©pendante de nos features. Car la valeur d'une feature d√©pendant directement de la valeur d'une autre.

> Exemple : Si $X_1$ repr√©sente l'√¢ge et $X_2$ repr√©sente le revenu, leur interaction pourrait d√©terminer la probabilit√© qu'un individu souscrive un pr√™t (par exemple, les jeunes avec un revenu √©lev√© pourraient √™tre plus enclins √† souscrire que les personnes √¢g√©es avec le m√™me revenu).

Si un mod√®le de machine learning r√©alise des pr√©dictions bas√©es sur 2 features. Nous pouvons d√©comper la pr√©diction en 4 termes :

- Une constante
- Un effet du premier feature
- Un effet du second feature
- L'effet combin√© des 2 features

Exemple d'un mod√®le pr√©disant la valeur d'un bien immobilier avec 2 features, taille de la maison (petit ou grand ) et la localisation (bien ou mauvais).

| localisation | taille | Prediction |
| ------------ | ------ | ---------- |
| bien         | grand  | 300,000    |
| bien         | petit  | 200,000    |
| mauvaise     | grand  | 250,000    |
| mauvaise     | petit  | 150,000    |

Ici, on d√©compose les pr√©dictions du mod√®le en:

- Terme constant : 150 000$
- taille : 100 000 $ si grand, 0 sinon
- localisation : 50 000 $ si bien, 0 sinon

La d√©composition est pleinement expliqu√©e ici, il n'y a pas d'effet d'int√©raction. L'effet indivuel des variables permet d'expliquer √† 100% votre mod√®le.

Maintenant un exemplea avec interaction:

| localisation | taille | Prediction |
| ------------ | ------ | ---------- |
| bien         | grand  | 400,000    |
| bien         | petit  | 200,000    |
| mauvaise     | grand  | 250,000    |
| mauvaise     | petit  | 150,000    |

On d√©compose la pr√©diction en :

- Un terme constant: 150 000$
- L'effet taille : 100 000$ si grand, 0 sinon
- L'effet localisation : 50 000$ si bien, 0 sinon
- L'effet interaction taille/localisation : 100 000 $  si grand et bien, 0sinon

Une mani√®re de mesurer cette interaction est de calculer de combien varie la pr√©diction de notre mod√®le bas√©e sur une l√©g√®re variation des effets d'interaction.

Cette m√©thode s'appelle le **Friedman's H-statistic**

##### R√©sum√© : H-statistic de Friedman

---

##### 2. D√©finition de base des interactions

**Absence d'interaction entre deux features peut √™tre exprim√© de la fa√ßon suivante :**

  $PD_{jk}(x_j, x_k) = PD_j(x_j) + PD_k(x_k)$

- $PD_{jk}$ : Partial Dependence Function (PDP) combin√©e des deux features.
- $PD_j$, $PD_k$ : PDP de chaque feature s√©par√©ment.

**S'il n'y a pas d'int√©raction entre un feature et les autres, on peut exprimer une pr√©diction comme suit :**

  $\hat{f}(x) = PD_j(x_j) + PD_{-j}(x_{-j})$

- $\hat{f}(x)$ : Pr√©diction totale.
- $PD_{-j}(x_{-j})$ : PDP combin√©e pour toutes les features sauf $j$.

---

###### 3. Calcul de la H-statistic

La H-statistic mesure la variance expliqu√©e par la diff√©rence entre le comportement observ√© (avec interactions) et celui sans interactions.

###### Interaction entre deux features $(H_{jk}^2)$ :

$H_{jk}^2 = \frac{\sum_{i=1}^n \big[ PD_{jk}(x_j^{(i)}, x_k^{(i)}) - PD_j(x_j^{(i)}) - PD_k(x_k^{(i)}) \big]^2}{\sum_{i=1}^n PD_{jk}^2(x_j^{(i)}, x_k^{(i)})}$

-**Num√©rateur :** Variance de la diff√©rence entre $PD_{jk}$ (PDP combin√©e) et $PD_j + PD_k$ (PDP individuelles).

-**D√©nominateur :** Variance totale de $PD_{jk}$.

###### Interaction entre une feature et toutes les autres $(H_j^2)$ :

$H_j^2 = \frac{\sum_{i=1}^n \big[ \hat{f}(x^{(i)}) - PD_j(x_j^{(i)}) - PD_{-j}(x_{-j}^{(i)}) \big]^2}{\sum_{i=1}^n \hat{f}^2(x^{(i)})}$

-**Num√©rateur :** Variance expliqu√©e par la diff√©rence entre $\hat{f}(x)$ (pr√©diction totale) et $PD_j + PD_{-j}$ (PDP sans interactions).

-**D√©nominateur :** Variance totale des pr√©dictions $\hat{f}(x)$.

---

###### 4. Interpr√©tation des r√©sultats

- $H = 0$ : Aucune interaction.
- $H = 1$ : L'effet des features provient uniquement des interactions (leurs PDP individuelles sont constantes).
- $H > 1$ : Rare, cela peut arriver si la variance des interactions d√©passe la variance totale, mais ce cas est difficile √† interpr√©ter.

---

###### 5. Probl√®mes pratiques du calcul

- **Complexit√© computationnelle :**  Le calcul de la statisique n√©cesside au mieux $2n^2$ pour calculer la H-statistic (j vs. k) et $3n^2$ pour la H-statistic (j vs. all).
- **√âchantillonnage :** R√©duit la complexit√©, mais peut rendre les r√©sultats instables.

---

##### Exemple et impl√©mentation:

- Utilisez la **H-statistic** pour d√©tecter et quantifier les interactions importantes.
- Si des interactions fortes sont d√©tect√©es :

  - Adaptez le mod√®le (e.g., mod√®les non lin√©aires, termes d'interaction explicites).
  - R√©alisez une analyse approfondie des interactions pour guider l'am√©lioration ou l'interpr√©tation du mod√®le.

```python

from sklearn.datasets import fetch_openml

from sklearn.preprocessing import OneHotEncoder

from sklearn.pipeline import Pipeline

from sklearn.compose import ColumnTransformer

from sklearn.svm import LinearSVR


#Fichier de donn√©es contenant les ventes de v√©lo tous les 5 jours

bikes = fetch_openml("Bike_Sharing_Demand", version=2, as_frame=True)


X, y = bikes.data.copy(), bikes.target


# We use only a subset of the data to speed up the example.

X = X.iloc[::5, :]

y = y[::5]

```

```python

categorical_features = X.select_dtypes(include=['category']).columns


preprocessor = ColumnTransformer(

    transformers=[

        ('cat', categorical_transformer, categorical_features)

    ])


# Cr√©ation du pipeline complet

pipeline = Pipeline(steps=[

    ('preprocessor', preprocessor),

    ('regressor', RandomForestRegressor(random_state=0, min_samples_leaf=10, max_depth=3,min_samples_split=5))

])


# Random Forest Regressor

model = pipeline.fit(X, y)



```

```python

# On restreint le nombre d'observations √† pr√©dire pour acc√©lerer le compute

random.seed(8)

X_exp = random.choices(X.to_numpy(), k=100)

X_exp = pd.DataFrame(X_exp, columns=X.columns)


h_stat = FriedmanHStatisticMethod()

h_stat.fit(model, X_exp)


```

```python

# Participation de votre feature et son interaction dans la variance globale

h_stat.plot(vis_type="bar_chart_ova")

```

> Ce graphique montre la pusisance d'int√©raction (H-statistic) pour chaque feature avec les autres . Ici les effets d'int√©ractions en entre les features son vraiment faibles (mois de 10% de la variance expliqu√©e par feature).

![1736368229091](image/cours/h_statistic_ova.png)

Les effets d'int√©ractions :

```python

# Pair d'int√©raction et son intensit√©

h_stat.plot(vis_type="bar_chart", ,top_k=5 )

```

> On peut √©galement observer les effets d'int√©raction par paire de variables.

![1736368266791](image/cours/h_statistic_pairs.png)Avantages :

- L'interpr√©tation se fait facilement, on repr√©sente la part de variance expliqu√©e par l'int√©raction de la feature et non pas son effet individuel.
- On peut comparer cette statistique d'un mod√®le √† l'autre
- Elle d√©tecte toute forme d'int√©raction

##### D√©savantages

- La statistique est tr√®s tr√®s co√ªteuse en compute et nous force √† travailler sur des √©chantillons
- Pas de tests statistiques fournis pour √©mettre une hypoth√®se d'int√©raction ou non
- Pas de threshold pour d√©finir une interaction par exemple int√©raction >0.3

#### Permutation Feature importance

##### Th√©orie

‚ö†Ô∏è Connaitre la notion de feature importance est un pr√©-requis.

Le concept est tr√®s simple : nous mesurons l‚Äôimportance d‚Äôune feature en calculant l‚Äôaugmentation de l‚Äôerreur de pr√©diction du mod√®le apr√®s permutation des caract√®ristiques du feature.

Une caract√©ristique est **¬´ importante ¬ª** si le m√©lange de ses valeurs augmente l'erreur du mod√®le, car dans ce cas, le mod√®le s'est appuy√© sur la caract√©ristique pour la pr√©diction.

Une caract√©ristique est **¬´ sans importance ¬ª** si le m√©lange de ses valeurs laisse l‚Äôerreur du mod√®le inchang√©e, car dans ce cas, le mod√®le a ignor√© la caract√©ristique pour la pr√©diction.

###### Traduction algorithmique : Feature permutation importance

---

Input: Un mod√®le entrain√© $\hat{f}$, une matrice de vos features $X$, un vecteur contenant la target $y$, la mesure des erreurs de pr√©dictions $L(y,\hat{f})$.

1. Estimer les erreurs originelles du mod√®le $e_{orig} = L(y, \hat{f}(X))$  (i.e. mean squared error)
2. Pour chaque feature $j \in \{1,...,p\}$ faire:

   - G√©n√©rer une matrice de feature $X_{perm}$ en permutant  la feature j dans le jeu de donn√©es X. Cela aura pour effet de "casser" l'association entre la feature j et y.
   - Estimer l'erreur $e_{perm} = L(Y,\hat{f}(X_{perm}))$ bas√©e sur les pr√©dictions des donn√©es permut√©es.
   - Calculer la permutation feature importance comme un quotient $FI_j= e_{perm}/e_{orig}$ ou la diff√©rence $FI_j = e_{perm}- e_{orig}$
3. Ordonner les  features par desc FI.

---

##### Exemple et impl√©mentation :

```python

from sklearn.inspection import permutation_importance

random_permutation = permutation_importance(model, X, y,

                                    n_repeats=30,

                                    random_state=0)

```

![1736368325267](image/cours/permutation_feature_importance.png)

> Permuter la variable hour conduit √† une augmentation du MSE de 1,4

##### Avantages :

- Faciliter √† interpr√©ter : Le feature importance montre de combien on augmente l'erreur du mod√®le quand l'information est d√©truite
- La permutation ne n√©cessite pas de r√©entrainer le mod√®le !!
- La permutation supprime l'effet univari√© de notre variable sur la target mais √©galement les effets joints avec la distribution des autres variables. Cela √† tendance √† surestimer l'effet de la variable car elle porte l'effet individuel et l'effet collectif

##### D√©savantages:

- Comme on permute la valeur des features de fa√ßon al√©atoire cela introduit un biais. Si on relance une seconde fois la permutation peut √™tre diff√©rent et donner des r√©sultats totalement diff√©rent. Pour diminuer cette effet il est conseiller de r√©aliser plusieurs fois cette permutation.
- Comme pour les PDP, si les features sont cor√©ll√©es ont peut biaiser les r√©sulats avec des valeurs n'existant pas

#### R√©sum√© :

| M√©thode            | Pakage                                                                                | Condition d'utilisation                                  | Interpretation                                                        |
| ------------------- | ------------------------------------------------------------------------------------- | -------------------------------------------------------- | --------------------------------------------------------------------- |
| PDP                 | from sklearn.inspection import partial_dependence,<br />PartialDependenceDisplay,     | Features ind√©pendantes et non corr√©l√©es.              | Effet marginale de la feature pour les<br />autres features fixes     |
| ALE                 | from alibi.explainers importALE, plot_ale                                             | Feature ind√©pendantes et corr√©lation faible ou moyenne | Effet marginale local valable uniquement<br />sur une plage de valeur |
| H-statistic         | from artemis.interactions_methods.model_agnostic<br />import FriedmanHStatisticMethod | Travailler sur un sous-√©chantillon de donn√©es          | Effet d'int√©raction d'une variable avec les<br />autres.             |
| Permutation Feature | from sklearn.inspection import permutation_importance                                 | Variable ind√©pendante et peu d'effet joint              | Effet de la d√©sactivation d'une variable sur le<br />mod√®le         |
