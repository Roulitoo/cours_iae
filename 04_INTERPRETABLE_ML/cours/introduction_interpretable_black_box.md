## Interpretable Machine Learning

**RÃ©sumÃ©** :  Ce cours est Ã  destination des Ã©tudiants du master 2 ECAP de l'IAE Nantes. Il vise Ã  rÃ©sumer les mÃ©thodes d'interprÃ©tation de modÃ¨le de machine learning qualifiÃ© de "Black Box" et ainsi rendre le modÃ¨le explicable.

La premiÃ¨re partie s'attardera sur la notion d'explicabilitÃ© et les diffÃ©rents cas oÃ¹ il s'avÃ¨re nÃ©cessaire de rendre la dÃ©cision d'un modÃ¨le de machine learning humainement interprÃ©table.

AprÃ¨s avoir explorÃ© les diffÃ©rents concepts d'interpratibilitÃ© nous Ã©tudierons briÃ¨vement les modÃ¨les interprÃ©table par nature ( concept dÃ©ja maÃ®trÃ®sÃ© dans votre cursus) puis nous recentrerons le cours sur les "models agnostic methods" afin d'interprÃ©ter les modÃ¨les boites noirs.

Nous verrons pour chaque mÃ©thode leur explication thÃ©orique et comment elles sont construites et leurs implÃ©mentations avec le langage Python.

De plus nous conclurons systÃ©matiquement par les avantages et inconvÃ©nients de chaque mÃ©thode citÃ©e.

Le cours est aujourd'hui traitÃ© pour interprÃ©ter des donnÃ©es tabulaires ce qui correspond aux donnÃ©es que vous traitez majoritairement dans le cadre de votre formation mais il existe Ã©galement des mÃ©thodes d'interprÃ©tation pour **des donnÃ©es non tabulaire!**

---

### Qu'est ce que le machine learning

Avant d'entamer une dÃ©finition prÃ©cise de l'interprÃ©tabilitÃ© et explicabilitÃ© des modÃ¨les d'apprentissage automatique. Il convient de bien dÃ©finir l'apprentissage automatique ou Machine Learning.

Parfois, il est confondu avec la notion d'algorithme.

Illustration d'un algorithme VS Machine Learning

![ml_vs_algo](image/cours/ml_vs_algo.png)

**Un algorithme** : Est un ensemble de rÃ¨gles dÃ©finies par un humain qui sont exÃ©cutÃ©es par une machine afin d'atteindre un but prÃ©dÃ©fini.

On peut le voir comme un processus qui dÃ©finit des inputs et prÃ©voit toutes les Ã©tapes permettant de transformer nos inputs en outputs dÃ©sirÃ©s.

```python

defcelsius_to_fahrenheit(celsius):

    """

    Convertit une tempÃ©rature de degrÃ©s Celsius en Fahrenheit.

    Inputs:

        - celsius (float): tempÃ©rature en degrÃ©s Celsius

    Output:

        - float: tempÃ©rature en Fahrenheit

    """

    # Ã‰tape 1 : Appliquer la formule de conversion

    fahrenheit = (celsius *9/5) +32


    # Ã‰tape 2 : Retourner le rÃ©sultat

    return fahrenheit


# Exemple d'utilisation

print(celsius_to_fahrenheit(25))  # Output : 77.0


```

L'utilisation d'un algorithme ici est idoine. On connait les instructions qu'on souhaite rÃ©aliser et comment transformer nos intputs pour obtenir l'output dÃ©sirÃ©.

**Machine Learning :** C'est une mÃ©thode qui permet Ã  un programme d'apprendre Ã  partir de donnÃ©es afin de rÃ©aliser et optimiser une prÃ©diction. C'est un changement de paradigme de la *programmation normale* oÃ¹ on dÃ©finit explicitement nos Ã©tapes et nos rÃ¨gles Ã  une *programmation indirecte* oÃ¹ les rÃ¨gles elles-mÃªmes Ã©manent de la Data.

#### Machine Learning VS Statistical Learning

L'approche statistique(Ã©conometrique) s'attarde Ã  comprendre le processus gÃ©nÃ©rateur d'un phÃ©nomÃ¨ne Y en se basant sur les co-informations X.

```mermaid

graph LR

    X[Input: X] -->|RÃ©gression linÃ©aire / RÃ©gression logistique| Process[ModÃ¨le]

    Process --> Y[Output: Y]


```

Le machine learning quant Ã  lui cherche Ã  approximer Y Ã  l'aide d'une fonction f(x) sans s'attarder sur les relations entre Y et f(x).

```mermaid

graph LR

    X[Input: X] --> Process[BoÃ®te noire]

    Process --> Y[Output: Y]


    X-.-> ML[Random Forest<br> XGBoost<br> Neural Network]

    ML-.-> Y



  


```

Avec un modÃ¨le de machine complexe nous sommes en incapactiÃ© d'expliciter le rÃ©sultat pour une prÃ©diction individuelle.

Pourquoi dois-je refuser un prÃªt pour ce client? La seule rÃ©ponse que je je peux apporter est "parce que le modÃ¨le me le dit"

### De l'importance de l'interpretability :

Il n'existe pas de dÃ©finition mathÃ©matique formelle de l'interprÃ©tabilitÃ© mais nous pouvons repondre Ã  la dÃ©finition donnÃ©e par Miller(2017):

> L'interpretabilitÃ© d'un modÃ¨le de Machine Learning est la capacitÃ© d'un humain Ã  comprendre les causes d'une dÃ©cision du modÃ¨le.

Plus un interprÃ©tabilitÃ© du modÃ¨le sera forte plus un humain sera en mesure de comprendre les dÃ©cisions/critÃ¨res influenÃ§ant la prÃ©diction du modÃ¨le.

Dans ce cours nous distinguerons Ã©galement une nuance entre InterpretabilitÃ© d'un modÃ¨le et ExplicabilitÃ©.

**ðŸ’¡ExplicabilitÃ©** : Explication de prÃ©diction individuelle

**ðŸ“– InterprÃ©tabilitÃ©** : ComprÃ©hension gÃ©nÃ©rale du modÃ¨le et comment sont rÃ©alisÃ©es les prÃ©dictions

#### Compromis entre interprÃ©tabilitÃ© et pouvoir prÃ©dictif

Lors de l'entraÃ®nement  d'un modÃ¨le d'apprentissage automatique vous aurez systÃ©matiquement Ã  choisir entre le "**Pourquoi" et le "Quoi".**

Un modÃ¨le permettant de comprendre facilement **"Pourquoi"** je rÃ©alise telle ou telle prÃ©diction offre gÃ©nÃ©ralement de performance moins bonne, de fait un **"quoi"** plus faible.

---

**Exemple du Customer Churn :**

Un client souhaite partir de votre enseigne et votre modÃ¨le de ML dÃ©tecte avec une probabilitÃ© de 98% qu'il va quitter votre enseigne. Cette information est importante car elle priorise ce client pour agir tout de suite afin de le retenir.

En revanche, cela ne vous donne aucune information sur comment le retenir

Qu'est-ce qui pousse mon client Ã  partir? Cette question n'est pas rÃ©pondue.

- Le positionnement de mon prix? :ðŸ’µ
- Une insatifaction ?ðŸ˜¡

Ce compromis doit Ãªtre guidÃ© par l'objectif que vous recherchez.                                                                                                             

**GÃ©nÃ©ralement le Pourquoi l'importe dans les cas suivants :**

- Recherche scientifique afin de comprendre un phÃ©nomÃ¨ne ==> **Pourquoi**
- Quand le problÃ¨me est dÃ©ja trÃ¨s bien cadrÃ© et dÃ©ja rÃ©solu ==> **Quoi** Optical charactÃ¨re recognition
- AutoritÃ© rÃ©gulatrice besoin de comprendre le modÃ¨le

```markdown

> [!NOTE]

Suivant la problÃ©matique que vous souhaitez rÃ©soudre, vous aurez Ã  choisir entre maximiser l'interprÃ©tabilitÃ© de votre modÃ¨le ou son pouvoir explicatif.


D'oÃ¹ l'importance de cadrer prÃ©cisement le problÃ¨me que vous souhaitez adresser

```

#### Taxonomie des interprÃ©tations de modÃ¨les

Le premier niveau d'interprÃ©tabilitÃ© porte sur la **capacitÃ© intrinsÃ¨que** d'un modÃ¨le Ã  Ãªtre interprÃ©table ou alors Ãªtre **interprÃ©table post hoc.**

**CapacitÃ© intrinsÃ¨que :**

On l'obtient en contraignant le modÃ¨le en restreignant sa complexitÃ© (nombre de features) afin de le comprendre aisÃ©ment.

L'interprÃ©tabilitÃ© intrinsÃ¨que se rÃ©fÃ¨re Ã  des modÃ¨les simples comme (RÃ©gression linÃ©aire, Logistique,  Arbre de rÃ©gression/classification , SVM)

**Post hoc:**

L'interprÃ©tabilitÃ© se rÃ©fÃ¨re Ã  des modÃ¨les plus complexes qui sont interprÃ©tables post-entraÃ®nement grace Ã  des mÃ©thodes extÃ©rieures aux modÃ¨les (Feature importance)

> ðŸ’¡Le post Hoc interpretabilitÃ© peut aussi s'utiliser sur des modÃ¨les interpretable intrinsÃ¨que

Globalement, il existe 5 mÃ©thodes d'interprÃ©tation qu'on peut diviser de la faÃ§on suivante :

- **Feature summary statistic** : Statistique par feature permettant d'interprÃ©ter son rÃ´le dans le modÃ¨le (Feature Importante)
- **Feature summary visualization** : ReprÃ©sentation visuels des statistiques en visualisation quand le nombre de statistiques rend difficilie l'interpration une Ã  une (Partial Dependance Plot)
- **Model internal** : ParamÃ¨tre interne du modÃ¨le permettant l'interprÃ©tation des rÃ©sultats ( Poids du modÃ¨les Reg liÃ©naire, Structure arbre)
- **Data point** : L'interprÃ©ation par individu spÃ©cifique du jeu de donnÃ©es, on se concentre sur un invidividu spÃ©cifique afin d'expliquer sa prÃ©diction (Counter factual prÃ©diction)
- Intrinsically interpretable model : Approximer un modÃ¨le Black Box par un modÃ¨le interprÃ©table localement ou Globalement (ref model internal)

> ðŸ’¡On parle de modÃ¨le spÃ©cifique quand l'interprÃ©tation est propre Ã  un type de modÃ¨le et de modÃ¨le agnostic quand la mÃ©thode s'applique Ã  tout type de modÃ¨le.

### Les diffÃ©rents niveaux d'interprÃ©tabilitÃ©s

#### L'interprÃ©tabilitÃ© Global ou Hoslitique

Un modÃ¨le est globalement interprÃ©table si l'on peut comprendre **l'ensemble de son fonctionnement** d'un seul coup d'Å“il ou avec une vue d'ensemble complÃ¨te. Il faut Ãªtre capable de :

* Comment le modÃ¨le effectue ses prÃ©dictions (les mÃ©canismes internes).
* L'importance des variables/features.
* Les interactions entre les variables.
* La distribution des sorties (cible) en fonction des caractÃ©ristiques d'entrÃ©e.

Il est trÃ¨s rare de pouvoir atteindre ce niveau de connaissance d'un modÃ¨le quand on dÃ©passe 3 Features. DÃ¨s lors qu'on dÃ©passe des reprÃ©sentations Ã  3 dimensions, il est impossible pour un humain de se reprÃ©senter les interactions.

#### L'interprÃ©tabilitÃ© Global Ã  un niveau modulaire

Comprendre un modÃ¨le entier, comme un Naive Bayes avec des centaines de variables, est pratiquement impossible. Cela nÃ©cessiterait de mÃ©moriser tous les poids et dâ€™Ã©valuer la distribution conjointe des variables, une tÃ¢che irrÃ©aliste.

PlutÃ´t que de chercher Ã  comprendre tout le modÃ¨le, on peut analyser certaines parties spÃ©cifiques :

Pour les modÃ¨les linÃ©aires on peut interprÃ©ter ses poids toutes choses Ã©tant Ã©gales par ailleurs. Cela signifie que les autres paramÃ¨tres sont inchangÃ©s pour intrÃ©pter l'effet d'une variable.

> âœ‹Dans les faits, il est rare qu'une variable varie alors que les autres sont constantes.

#### **InterprÃ©tabilitÃ© locale pour une prÃ©diction unique**

Comprendre pourquoi un modÃ¨le a fait une prÃ©diction particuliÃ¨re pour une instance donnÃ©e.

ðŸ”½Ã€ un niveau local, le comportement dâ€™un modÃ¨le complexe peut devenir plus simple. Par exemple :

> Une relation non linÃ©aire entre la taille et le prix dâ€™une maison peut se comporter de maniÃ¨re linÃ©aire pour une maison de 100 mÂ² si lâ€™on observe uniquement cette instance.

On peut tester cela en simulant des modifications de la taille (+ ou - 10 mÂ²) et en observant lâ€™impact sur la prÃ©diction.

**Avantage** : Les explications locales sont souvent plus prÃ©cises que les explications globales, car elles se concentrent sur un sous-ensemble restreint de donnÃ©es.

**MÃ©thodes disponibles** : Les techniques indÃ©pendantes des modÃ¨les ( **model-agnostic methods** ) permettent de rendre les prÃ©dictions individuelles plus interprÃ©tables.

**Conclusion** : Approfondir une instance spÃ©cifique permet de mieux comprendre les dÃ©cisions du modÃ¨le, mÃªme lorsquâ€™il est complexe au niveau global.

#### InterprÃ©tabilitÃ© locale pour un groupe de prÃ©dictions

Comprendre pourquoi le modÃ¨le a fait des prÃ©dictions spÃ©cifiques pour un groupe dâ€™instances.

**MÃ©thodes disponibles** :

1.**Approches globales** : Appliquer des mÃ©thodes d'interprÃ©tation globale, mais en considÃ©rant le groupe comme s'il s'agissait de l'ensemble complet des donnÃ©es.

2.**Approches locales** : Utiliser des explications locales pour chaque instance individuelle, puis les combiner ou les agrÃ©ger pour le groupe.

### Qu'est ce qu'une explication humainement comprÃ©hensible

Une explication est une rÃ©ponse Ã  une question formulÃ©e avec un "Pourquoi" (Miller 2017)

- Pourquoi mon client va-t-il arrÃªter son contrat?
- Pourquoi mon prÃªt a Ã©tÃ© rejettÃ©?

Le fait de donner une bonne explication a Ã©tÃ© Ã©tudiÃ© par Lipton en 1990.

Un humain ne souhaite pas saisir l'ensemble des causes dÃ©terminant une prÃ©diction mais plutÃ´t comprendre la prÃ©diction a Ã©tÃ© rÃ©alisÃ©e plutÃ´t qu'une autre.

Nous avons tendance Ã  penser Ã  des contre-exemples pour comprendre une prÃ©diction.

Combien serait estimÃ©e le prix de ma maison si j'augmente le nombre de piÃ¨ces de 1?

Si je demande un prÃªt Ã  la banque, je ne cherche pas Ã  comprendre tous les facteurs qui ont entraÃ®nÃ© mon rejet mais seulement ceux sur lesquels je peux agir!

**TransposÃ© au machine learning : Cela signifie**

Les humains prÃ©fÃ¨rent les explications contrastives, qui comparent une prÃ©diction Ã  une autre situation hypothÃ©tique ou rÃ©elle. Ces explications doivent Ãªtre adaptÃ©es au **contexte** et au **destinataire**, en choisissant un point de rÃ©fÃ©rence pertinent (par exemple, une maison similaire pour expliquer une prÃ©diction de prix immobilier).

Les explications doivent Ã©galement Ãªtre courtes et sÃ©lectionnÃ©es : les gens attendent 1 Ã  3 causes principales plutÃ´t quâ€™une liste exhaustive. Ce phÃ©nomÃ¨ne, connu sous le nom **dâ€™effet Rashomon**, illustre quâ€™un Ã©vÃ©nement peut avoir plusieurs explications valables (chaine d'infos Ã  la TV). Les mÃ©thodes comme LIME, qui fournissent des explications simples et comprÃ©hensibles, sont bien adaptÃ©es Ã  cet objectif.

```mermaid

graph TD

    A[Interpretable Models For Machine Learning] --> B[Model-Agnostic Methods]

  

    B --> C[Global Model Agnostic]

    B --> D[Local Model Agnostic]

  

    C --> E1[Partial Dependence Plots]

    C --> E2[Accumulated Local Effects]

    C --> E3[Feature Interaction]

    C --> E4[Feature Importance]


    D --> F1[Individual Conditional Expectation]

    D --> F2[Local surogate AKA LIME]

    D --> F3[Shapley Values]

    D --> F4[Shapley Additive]



  

```
