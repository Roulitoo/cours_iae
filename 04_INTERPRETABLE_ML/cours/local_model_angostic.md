## Local model-Agnostic M√©thods

Dans ce chapitre nous verrons les diff√©rentes m√©thodes permettant d'analyser le r√©sultat d'une instance ou un groupe d'instances.

Les m√©thodes √©tudi√©es seront les suivantes :

- Individual conditional curves (ICE) bas√©e sur les Partial Depence Plot(PDP) mais par instance cette fois
- Local surrogate models (LIME) expliquer une pr√©diction en remplacant un mod√®le **black box** avec un **mod√®le white box localement**
- Shapley values : m√©thode d'attribution des pr√©dictions bas√©es sur les caract√©ristiques individuelles
- SHAP : D√©riv√©e des shapley values avec √©galement avec des global model agnostic (m√©thode la plus populaire)

### Individual Conditional Expectation (ICE)

Les Individual Conditional Expectation (ICE) plots permettent de tracer sur un graphique une ligne par instance qui montre comment la pr√©diction d'une instance est impact√©e si on fait varier la valeur d'une feature.

Cette m√©thode est bas√©e sur les PDP, la PDP repr√©sente l'effet moy√©nniser de toutes les courbes ICE. La m√©thode est donc exactement la m√™me, on fait varier notre feature d'int√©r√™t tout en fixant √† leur valeur moyenne les autres features.

L'ICE permet d'obtenir pour une pr√©diction individuelle l'effet du changement de la feature et surtout de mettre en avant s'il existe de l'h√©t√©rog√©n√©it√© dans la mani√®re dont la feature affecte un individu.

En effet, le PDP repr√©sente la valeur moyennis√©e mais cela ne nous dit pas si il y a une h√©t√©rog√©n√©it√©!!

### Exemple et impl√©mentation

L'impl√©mentation se r√©alise avec la m√™me fonction que PDP dans sklearn mais il faut modifier un param√®tre de la fonction afin d'obtenir les pr√©dictions individuelles et non la moyenne

```python
# ICE pour plusieurs caract√©ristiques quantitatives
features = ["temp","hum","windspeed"]  # Index des caract√©ristiques
_, ax1 = plt.subplots(figsize = (12,6))
PartialDependenceDisplay.from_estimator(rf,  #votre mod√®le
                                         X_train, 
                                         features,
                                         kind="individual", # On modifie ICI
                                         ax = ax1,
                                         grid_resolution=10, # Nombre de points estim√©s pour le tracer de la courbe
                                         n_cols=3,
                                         subsample=0.05, # 5% du jeu de donn√©es
                                         centered=True
                                         )
plt.suptitle("Partial Dependence Plots - random- forest")
plt.tight_layout()
plt.show()
```

![1736457872797](image/local_model_angostic/ice_plots_quanti.png)

Dans la grande majorit√© des cas les courbes ont la m√™me allure et donc l'effet semble similaire chez nos individus. Le PDP peut donc √™tre un bon r√©sum√© des relations entre nos features et la variable √† pr√©dire.

**Avantages :**

- Contraitement aux PDP les ICE peuvent r√©v√©ler des int√©ractions h√©t√©rog√®nes dans nos variables.

**D√©savantages:**

- Les courbes ICE ne peuvent afficher qu'une seule feature √† la fois. Il serait trop compliqu√© de lire une superposition de surfaces comme dans l'exemple 2D de PDP pour des pr√©dictions individuelles.
- La **corr√©lation** reste encore un probl√®me ici. Si on fixe les autres variables √† leur valeur, cela peut produire des combinaisons irr√©alistes.
- Le Graphique peut vite √™tre surcharg√© si on ajuste pas le nombre de lignes ou la transparence.

### Local surogate (Lime)

L'algorithme LIME( Local interpretable model-explanation )  est une m√©thode d'explicabilit√© con√ßue pour interpr√©ter des mod√®les black box.

L'id√©e est d'expliquer **localement** la pr√©diction d'un mod√®le en utilisant un mod√®le **plus simple** et compr√©hensible dans un voisinage sp√©cifique de l'exemple √† expliquer (Reg lin√©aire, arbre, Lasso, ...)

Prenons l'exemple d'un mod√®le Black Box tr√®s complexe, vous poss√©dez les caract√©ristiques d'un unique individu. Votre objectif est de comprendre pourquoi le mod√®le de machine learning pr√©dit une valeur pour cet individu.

Pour cela vous allez g√©n√©rer un dataset √† partir de cette observation et dupliquer cette observation en introduisant de l√©ger changement dans les donn√©es.

Ces perturbations vous permettront de comprendre pour chaque feature l'apport dans la pr√©diction. Les perturbations du dataset peuvent repr√©senter une variation plus ou moins importante des caract√©ristiques de nos individus.

L'ordre de grandeur de ces variations sont importantes pour estimer suite √† une variation l'impact sur la variable $y$.

Afin de comprendre les effets de toutes ces variations, nous utilisons un mod√®le lin√©aire avec ce dataset perturb√© et les valeurs pr√©dites comme √©tant le nouveau $y$.

Ce mod√®le nous permettra de comprendre pour un individu l'effet des variables sur sa pr√©diction.

![1736460507851](image/local_model_angostic/1736460507851.png)

> Fronti√®re de d√©cision d'un mod√®le complexe, interpr√©t√© localement par une r√©gression lin√©aire pour une pr√©diction

#### Th√©orie :

---

1. Mod√®le √† expliquer $\hat{f(x)}$ : Le classifieur ou mod√®le de r√©gression.
2. Nombre d'√©chantillons (N) : Nombre de points √† g√©n√©rer qui seront perturb√©s pour cr√©er l'explication
3. Instance cible (x) : L'instance √† expliquer, ainsi que sa version interpr√©table (**x'**).
4. **Noyau de similarit√© $œÄ_x$** : Une fonction mesurant la proximit√© entre les √©chantillons g√©n√©r√©s et l'instance cible.
5. **Nombre de feature explicatives (K)** : Le nombre maximal de caract√©ristiques utilis√©es dans l'explication.

---

#### √âtapes de l'algorithme :

1. **Initialisation** :

   - Cr√©ez un ensemble vide $Z$ pour stocker les donn√©es g√©n√©r√©es.
2. **G√©n√©ration d'√©chantillons** :

   - R√©p√©tez **N** fois :
     - G√©n√©rer un √©chantillon interpr√©table (**z'**) autour de **x'**.
     - Associer √† **z** :
       - La pr√©diction du mod√®le **f(z)**.
       - La similarit√© entre **z** et **x**, not√©e **œÄx(z)**.
       - Pond√©ration, plus similarit√© entre **z** et **x** plus **x'** sera pond√©r√©.
     - Ajouter ces informations √† l'ensemble `Z`.
3. **Apprentissage d'un mod√®le local** :

   - Utilisez une r√©gression Lasso avec r√©gularisation (**K-Lasso**) pour s√©lectionner au plus **K** caract√©ristiques dans **Z**.
   - Les caract√©ristiques s√©lectionn√©es et leurs poids forment l'explication locale.

---

#### Exemple et impl√©mentation :

```python
import lime
import lime.lime_tabular

#Cr√©ation de l'explainer
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.to_numpy(),  # Attention au format numpy array obligatoire
                                                   feature_names=features_names , #Nom des features du mod√®le
                                                   class_names=['cnt']  , #Variable √† predire
                                                   verbose=True,
                                                     mode='regression' # Classification ou r√©gression
                                                     )
# Index de l'individu √† pr√©dire
i = 3018
# R√©sultat de l'explainer
exp = explainer.explain_instance(X_train.loc[i], rf.predict, num_features=5)
```

Une fois r√©alis√© vous obtenez un explainer par individu qui sera le r√©sultat d'un mod√®le local. Ce mod√®le vous fournira sa valeur pr√©dite vs la valeur r√©elle issue du mod√®le black-box.

üí°Notez √©galement que LIME fournit une interpr√©tation qualitative, chaque variable est discr√©tis√©e pour faciliter l'interpr√©tation, selon les auteurs du papier de recherche :

- Les variables continues centr√©es sont trop compliqu√©es √† interpr√©ter
- Le double effet n√©gatif engendre √©galement une difficult√© de compr√©hension

Chaque variable continue est alors discr√©tis√©e √† l'aide de ses quantiles.

```pyth
#Afficher lime explainer in notebook
exp.show_in_notebook(show_table=True)
```

**Exemple de sortie de Lime :**

![1737400566755](image/local_model_angostic/lime_plot.png)

**A gauche**, on peut lire la valeur pr√©dite par notre mod√®le local ( LIME stock √©galement la valeur pr√©dite par le mod√®le black box).

**Au centre**, on peut lire l'effet des variables discr√©tis√©es sur la pr√©diction.

**A droite**, la valeur de chaque feature avant la discr√©tisation.

**Avantages :**

- Interpr√©tation facile car elle vous permet de mobiliser vos connaissances des mod√®les lin√©aires
- Possibilit√© de r√©duire le nombre de variables explicatives en utilisant la m√©thode du Lasso
- Fonctionne avec les donn√©es tabulaires, textes et images!

**D√©savantages :**

- Il peut relativement √™tre compliqu√© de cr√©er des instances similaires surtout pour les donn√©es tabulaires

### Shapley Values :

Cette partie sera consacr√©e aux valeurs de shapley issues de la th√©orie des jeux √† leur utilisation dans l'interpr√©tation des mod√®les black box avec les shapleys additive exPlanations AKA SHAP.

Exemple intuitif : Qui paiera le taxi?

Supposons un jeu de coop√©ratif o√π nous avons 3 joueurs et nous voulons savoir comment r√©partir le prix d'un taxi en fonction de leurs caract√©ristiques.

Nos 3 joueurs Alice, Bob & Charlie forment une coalition et recoivent un montant sp√©cifique lors du paiement du taxi ( ici le paiement est n√©gatif).

L'objectif est de d√©terminer pour chaque joueur un prix √©quitable du taxi. Nous posons alors aux joueurs et ce de fa√ßon al√©atoire le prix qu'ils sont pr√™ts  √† payer en fonction d'avec qui ils seront dans le taxi.

- Alice seule payera 15‚Ç¨
- Alice et Bob vivent ensemble mais Bob veut toujours prendre une voiture haut de gamme donc ce sera 25‚Ç¨ (15 ‚Ç¨ pour Alice et 10‚Ç¨ de majoration pour le standing)
- Charlie, Alice et Bob paieront 51‚Ç¨ de taxi car Charlie habite tr√®s loin.

Voici l'ensemble des combinaisons possibles :

| Passengers            | Cost | Note                                               |
| --------------------- | ---- | -------------------------------------------------- |
| ‚àÖ                    | ‚Ç¨0  | Pas de taxi                                        |
| {Alice}               | ‚Ç¨15 | Alice seul                                         |
| {Bob}                 | ‚Ç¨25 | Bob seul car il aime le luxe                       |
| {Charlie}             | ‚Ç¨38 | Charlie seul et qui habite loin                    |
| {Alice, Bob}          | ‚Ç¨25 | Bob veut toujours le luxe                          |
| {Alice, Charlie}      | ‚Ç¨41 | On d√©pose Alice en premier, ensuite Charlie       |
| {Bob, Charlie}        | ‚Ç¨51 | D√©poser le luxeux¬†Bob en premier, apr√®s Charlie |
| {Alice, Bob, Charlie} | ‚Ç¨51 | Les 3 prennent le taxi                             |

‚àÖ est l'ensemble vide, personne ne prend le taxi.

Ce tableau permet de donner une vague id√©e de combien chaque passager contribue √† une course en taxi. Mais il repr√©sente le co√ªt √† payer pour une coalition.

#### Calcul de la contribution marginal de chaque passager

La contribution marginale d'un joueur est le montant qu'un individu ajoute au co√ªt total lorsqu'il rejoint un groupe d√©ja form√©.

- Si Alice monte seule dans le taxi, elle paie l'int√©gralit√© du co√ªt
- Si Alice rejoint le taxi avec Bob, elle paie uniquement le surco√ªt qu'elle apporte en plus de la pr√©sence de Bob

Si on compare la coalition {Alice, Bob} avec la coalition {Bob} seul, on peut en d√©river la valeur marginale de Alice √† la coalition {Bob}.

Cela donne 25 Bob seul et 25 Bob et Alice soit 0‚Ç¨. Le co√ªt marginal de Alice est de 0 dans cette coalition.

A l'inverse le cout marginal de Bob avec la coalition Alice on obtient 25 - 15 = 10‚Ç¨. Cela veut dire que Bob paie 10 dollars le prix.

On calcule donc tous les co√ªts marginaux possibles pour chaque coalition :

| Ajout           | Coalition existante | Co√ªt avant | Co√ªt aprp√®s | Contribution marginale |
| --------------- | ------------------- | ----------- | ------------- | ---------------------- |
| Alice ‚àÖ        | ‚àÖ                  | 0           | 15            | 15‚Ç¨                   |
| Alice           | {Bob}               | 25          | 25            | 0‚Ç¨                    |
| Alice           | {Charlie}           | 38          | 41            | 3‚Ç¨                    |
| Alice           | {Bob, Charlie}      | 51          | 51            | 0‚Ç¨                    |
| Bob ‚àÖ          | ‚àÖ                  | 0           | 25            | 25‚Ç¨                   |
| Bob             | {Alice}             | 15          | 25            | 10‚Ç¨                   |
| Bob             | {Charlie}           | 38          | 51            | 13‚Ç¨                   |
| Bob             | {Alice, Charlie}    | 41          | 51            | 10‚Ç¨                   |
| Charlie ‚àÖ      | ‚àÖ                  | 0           | 38            | 38‚Ç¨                   |
| Charlie {Alice} | {Alice}             | 15          | 41            | 26‚Ç¨                   |
| Charlie         | {Bob}               | 25          | 51            | 26‚Ç¨                   |
| Charlie         | {Alice, Bob}        | 25          | 51            | 26‚Ç¨                   |

Nous avons les co√ªts marginaux de chaque coalition, il reste maintenant √† calculer la contribution marginale de chaque passager.

On pourrait assigner une pond√©ration √©quitable √† chaque contribution marginale et moy√©nniser mais la meilleur fa√ßon de le faire est de consid√©rer toutes les permutations possibles et pond√©rer la moyenne en fonction de ces permutations.

- Alice, Bob, Charlie
- Alice, Charlie, Bob
- Bob, Alice, Charlie
- Charlie, Alice, Bob
- Bob, Charlie, Alice
- Charlie, Bob, Alice

Math√©matiquement cela correspond √† 3! = 3 * *2* * 1 permutations possibles

#### Moyenniser la contribution marginale

Dans 2 cas Alice est ajout√©e √† un taxi vide, dans un cas elle est rajout√©e ave Bob, ...

En pond√©rant les contributs marginal on obtient le calcul suivant pour la contribution moyenne :

$\frac{1}{6} ( \underbrace{2* 15‚Ç¨ }_\textrm{Alice to ‚àÖ } + \underbrace{1* 0‚Ç¨ }_\textrm{Alice to Bob  } + \underbrace{1* 3‚Ç¨ }_\textrm{Alice to Charlie  } + \underbrace{2* 0‚Ç¨ }_\textrm{Alice to Bob,Charlie  } ) = 5.5‚Ç¨$

Pour Bob :

$\frac{1}{6} ( \underbrace{2* 25‚Ç¨ }_\textrm{Bob to ‚àÖ } + \underbrace{1* 10‚Ç¨ }_\textrm{Bob to Alice  } + \underbrace{1* 13‚Ç¨ }_\textrm{Bob to Charlie  } + \underbrace{2* 10‚Ç¨ }_\textrm{Bob to Alice,Charlie  } ) = 15.5‚Ç¨$

Pour Charlie :

$\frac{1}{6} ( \underbrace{2* 38‚Ç¨ }_\textrm{Charlie to ‚àÖ } + \underbrace{1* 26‚Ç¨ }_\textrm{Charlie to Bob  } + \underbrace{1* 26‚Ç¨ }_\textrm{Charlie to Alice  } + \underbrace{2* 26‚Ç¨ }_\textrm{Charlie to Alice,Bob  } ) = 30‚Ç¨$

La contribution individuelle de chacun nous donne bien 5.5 + 15.5 + 30 = 51‚Ç¨.

La Shapley value est donc : La valeur moyenne pond√©r√©e de la contribution marginale des joueurs.

#### Des valeurs de Shapley √† son usage pour le machine learning

Prenons l'exemple de machine learning suivant :

Une personne souhaite pr√©dire le prix d'un appartement en fonction de sa proximit√© √† un parc, de la taille de l'appartement, son √©tage et la possibilit√© d'y avoir un chat.

La pr√©diction moyenne pour le prix d'un appartement dans cette ville est de 310 000‚Ç¨

![1736635549728](image/local_model_angostic/exemple_shapley.png)

C'est un cas d'usage classique en apprentissage supervis√© mais quel est le lien avec notre th√©orie des jeux? O√π est le gain , les joueurs et la coalition? Ici :

- Le gain  = E(f(x) - Pr√©diction du mod√®le ==> 310 000 - 300 000
- Les joueurs sont les features utilis√©es qui collaborent pour r√©aliser une pr√©diction
- La coalition, l'ensemble des features activ√©es pour pr√©dire le r√©sultat

Pour calculer les shapley value sur des features il faut proc√©der de la mani√®re suivante :

On suppose qu'on souhaite √©tudier la feature `chat interdit` quand il est ajout√© √† la coalition `parc-proche & 50m¬≤`. Probl√®me pour √©tudier cette coalition on oublie la variable indiquant `L'√©tage` .

Quand on √©tudie une coalition comme {parc-proche, 50m¬≤} et l'ajout de {chat interdit} on suppose que toutes les autres features deviennent des variables al√©atoires.

Exemple :

![1736686816140](image/local_model_angostic/shap_calcul2.png)

Pour calculer la valeur de shapley il nous faut l'ensemble des coalitions possibles et l'ajout de l'effet de l'ajout de la variable chat pour chacun.

* ‚àÖ
* `parc-proche`
* `50m¬≤`
* `2nd etage`
* `parc-proche`+`50m¬≤`
* `parc-proche`+`2nd √©tage`
* `50m¬≤`+`2nd √©tage`
* `parc-proche`+`50 m¬≤`+`2nd √©tage`.

‚ö†Ô∏è On observe que plus nous avons de feature d'int√©ret pour le temps de calcul sera long pour le calcul de shapley value.

#### Formalisation

**Objectif :**

Calculer la valeur de Shapley pour la valeur de la \( j \)-√®me caract√©ristique.

#### **Entr√©es requises :**

- \( M \) : Nombre d'it√©rations.
- \( x \) : Instance d'int√©r√™t.
- \( j \) : Indice de la feature.
- \( X \) : Matrice des donn√©es.
- \( f \) : Mod√®le de machine learning utilis√© pour les pr√©dictions.

---

**√âtapes de l'algorithme :**

1. **Pour chaque it√©ration $( m = 1, \dots, M )$ :**

   - Tirer une instance al√©atoire $( z )$ de la matrice des donn√©es $( X )$.
     - Choisir une permutation al√©atoire $( o )$ des indices des caract√©ristiques.
     - R√©ordonner l'instance $( x )$ selon cette permutation :
       $x_o = (x_{(1)}, \dots, x_{(j)}, \dots, x_{(p)})$
     - R√©ordonner √©galement l'instance \( z \) selon cette permutation :
       $z_o = (z_{(1)}, \dots, z_{(j)}, \dots, z_{(p)})$

   **Construire deux nouvelles instances :**

   - Instance **avec $( j )$** :
     $x_+j = (x_{(1)}, \dots, x_{(j-1)}, x_{(j)}, z_{(j+1)}, \dots, z_{(p)})$

     Ici on conserve les caract√©ristiques de j et on int√®gre les valeur al√©atoires des features non √©tudi√©s $z$
   - Instance **sans \( j \)** :
     $x_j = (x_{(1)}, \dots, x_{(j-1)}, z_{(j)}, z_{(j+1)}, \dots, z_{(p)})$

     Ici on retire les caract√©ristiques de j remplac√©e par une permutation al√©atoire de $z_{j}$

**Calculer la contribution marginale :**
	 	  $	\phi^m_j = f(x_+j) - f(x_-j)$

---

**Accumuler la contribution marginale sur toutes les it√©rations.**

---

#### **Calcul final :**

Une fois toutes les it√©rations effectu√©es, la valeur de Shapley pour la caract√©ristique \( j \) est donn√©e par la moyenne des contributions marginales :
$\phi_j(x) = \frac{1}{M} \sum_{m=1}^M \phi^m_j$

---

En r√©sum√© : Les shapley value mises en pratiquent sont simplement la mesure de l'effet de la d√©sactivation de la variable $(X_j)$ dans la pr√©diction de l'individu $n¬∞i$.

#### Exemple et impl√©mentation

L'impl√©mentation se fera avec la librairie python [shap](https://shap.readthedocs.io/en/stable/index.html). Celle-ci poss√®de 3 niveaux d'analyses :

- Explicabilit√© d'une pr√©diction √† travers ses graphiques Watterfall Plot et Force Plot
- Interpr√©tabilit√© globale avec la somme des explicabilit√©s individuelles : Beeswarm Plot
- Effet d'une feature : Interaction/Scatter plot

##### Explicabilit√©

Continuons notre exemple avec notre Random Forest.

```python
# Importer la librairie Shap
import shap 

# D√©finir le module explainer de Shap, attention ce n'est pas le m√™me si c'est un mod√®le bas√© sur des abres

explainer = shap.TreeExplainer(rf)

# On calcule les shap values, attention c'est tr√®s couteux en temps de calcul.
# On prend donc un √©chantillon

shap_values = explainer(X_train.iloc[1:100])
```

Une fois les valeurs shap calcul√©es on peut passer √† la cr√©ation d'un graphique.

Pour un individu on peut lancer le force plot.

```python
shap.initjs()
# visualize the first prediction's explanation
shap.waterfall_plot(shap_values[86])
```

![1736708099927](image/local_model_angostic/waterfall_plot.png)

L'interpr√©tation se fait de la mani√®re suivante :

- L'axe des Y montre les effets individuelles de chaque feature avec la valeur de l'instance
- Les features sont ordonn√©es par ordre d'importance
- L'axe des X nous donne la valeur de la shapley value
- Chaque bar repr√©sente la valeur de shapley pour la valeur de l'instance
- On voit sur l'axe des X $E[f(X)]$ qui est la valeur moyenne de notre target
- $f(x)$ est la valeur de la pr√©diction pour l'instance $x_i$
- La couleur bleu indique une contribution n√©gative de la feature et en rouge une contribution positive par rapport √† la target

> Interpretation : La valeur pr√©dite de 115.48 v√©los lou√©s pour l'instance 98 diff√©re de la moyenne de 74 v√©los.
>
> La variable heure qui est √©gale √† 19 heures ici contribue √† augmenter de 83 v√©los notre pr√©diction
>
> Quant-√†-elle la variable temp√©rature √† 3.28 ¬∞c diminue de 8. v√©lo la pr√©diction.

Une autre repr√©sentation de ce graphique applatie de ce graphique est le force_plot.

```python
shap.force_plot(shap_values[86])
```

![1736715579838](image/local_model_angostic/force_plot.png)

La lecture se fait de la m√™me mani√®re mais elle permet de distinguer plus facilement les features jouant un r√¥le positif et celles jouant un r√¥le n√©gative.

##### Interpr√©tabilit√©

Apr√®s avoir calcul√© les valeurs de shapley pour les instances on cherche maintenant √† avoir une vision d'ensemble du mod√®le. Une interpr√©tation globale de notre mod√®le.

Heuresement Shap nous offre cette possibilit√© en calculant les valeurs pour un ensemble de point( id√©alement tout le dataset). Le passage √† l'interpr√©tation globale se fait en **moyennisant les r√©sultats individuels.**

```python
#Graphique beeswarm, interpr√©tabilit√© globale 
shap.plots.beeswarm(shap_values)
```

![1736716943826](image/local_model_angostic/1736716943826.png)

L'interpr√©tation se fait de la mani√®re suivante :

- L'axe des X repr√©sente la valeur de Shapley
- L'axe des Y repr√©sente les features et en couleur la valeur des features (rouge valeur √©lev√©es, bleu valeur faible)
- Chaque ligne correspond √† une feature avec la distribution de valeur de shapley associ√©e √† chaque instance
- L'ordre des features est donn√©es par la valeur moyenne de Shapley des instances

La distribution de la couleur r√©v√®le √©galement la nature de la relation entre la feature et nos donn√©es. Pour la variable ` hr` des valeurs hautes et faibles s'entrem√®le ce qui signifie que la relation n'est pas monotone.

De la m√™me mani√®re on peut lire qu'une humidi√© avec de forte valeur impact la shapley value de facon n√©gative √† l'inverse une humidit√© faible offre une shapley value l√©g√®rement positive.

üí°De mani√®re g√©nr√©ral voici les cl√©s pour l'interpr√©tation :

1. Observer l'ordre des features, plus une feature est haute plus elle aura une influence sur la shapley value
2. Pour chaque feature d'int√©ret regarder
   1. Sa distribution et voir son influence. Plus la distribution sera large plus la variable aura une influence.
   2. Regarder la distribution des couleurs et voir s'il y a une relation monotone ou un pattern plus complexe

##### Effet d'une variable sur notre target

Shap offre un √©quivalent au Partial Depence Plot appel√© scatter. Il permet d'√©tudier la relation entre les valeurs de shapley et la distribution de la feature.

Cela nous permet de connaitre la relation entre notre variable d'int√©r√™t(la vente de v√©lo) et une unique variable.

```python
shap.plots.scatter(shap_values[:, 'hr'])
```

![1736718353757](image/local_model_angostic/scatter.png)

Interpr√©tation :

- L'axe des X repr√©sente la distrubution de la variable Hr
- L'axe des Y repr√©sente la valeur de Shapley
- C'est une projection du beeswarm pour une unique feature
- L'histogramme en gris indique la distribution de notre feature Hr

### Comment interpreter les shapley values apr√®s la standardization

Une transformation usuel d'un jeu de donn√©es afin de l'utiliser dans un mod√®le de machine learning est de standardiser les donn√©es.

Cela am√©liore g√©n√©ralement les performances des mod√®les et permet de comparer nos variables exprim√©es dans un m√™me ordre de grandeur mais celui nuit grandement √† l'interpr√©tation individuelle des variables.

On cherchera bien souvent √† se ramener aux valeurs d'origines pour l'interpr√©tation.

Pour cela on s'appuyera sur la propri√©t√© suivante des valeurs de Shapley :

> Si une transformation **univari√©e** (c‚Äôest-√†-dire appliqu√©e ind√©pendamment √† chaque caract√©ristique) est utilis√©e, comme la standardisation,  **les valeurs de Shapley restent inchang√©es** . Cela signifie qu'il est possible de calculer les SHAP values directement pour les caract√©ristiques standardis√©es, puis de les interpr√©ter en les ramenant √† leurs valeurs d'origine.

Prenons le dataset suivant

```python
import shap
from sklearn.model_selection import train_test_split
X, y = shap.datasets.adult()
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.2, random_state=1
)

print(X.head(2))

```

|   | Age  | Workclass | Education-Num | Marital Status | Occupation | Relationship | Race | Sex | Capital Gain | Capital Loss | Hours per week | Country |
| - | ---- | --------- | ------------- | -------------- | ---------- | ------------ | ---- | --- | ------------ | ------------ | -------------- | ------- |
| 0 | 39.0 | 7         | 13.0          | 4              | 1          | 0            | 4    | 1   | 2174.0       | 0.0          | 40.0           | 39      |
| 1 | 50.0 | 6         | 13.0          | 2              | 4          | 4            | 4    | 1   | 0.0          | 0.0          | 13.0           | 39      |

Entrainons une r√©gression logistique pour mod√©liser les personnes gagnants plus de 50 000$ par mois.

```python
# get standardized data
scaler = StandardScaler()
scaler.fit(X_train)
X_std = scaler.transform(X)

# train the linear model
lr = LogisticRegression()
model = lr.fit(X_std, y)

# explain the model's predictions using SHAP
explainer = shap.explainers.Linear(model, X_std)
shap_values = explainer(X_std)

# visualize the model's dependence on the first feature
shap.plots.scatter(shap_values[:,0])

```

Effet de la feature N¬∞0 sur les shapley value ( ici c'est l'√¢ge standardis√©e)

![1737495110919](image/local_model_angostic/1737495110919.png)

L'interpr√©tation est relativement compliqu√©e avec des variables standardis√©e.

Ici 0.8 veut dire que l'√¢ge d'un individu est 0.8 √©cart type au dessus de la moyenne.

```python
# On rajoute les noms des features dans les valeurs de shapley
for i, c in enumerate(X.columns):
    shap_values.feature_names[i] = c


# On remplace les valeurs de X_std par ses valeurs orginales.
## Car la transformation de standscaler est univari√©e!

shap_values.data = X.values

# visualize the model's dependence on the first feature again, now in the new original feature space
shap.plots.scatter(shap_values[:, 0])

```

![1737496038563](image/local_model_angostic/1737496038563.png)

#### Une mani√®re plus √©l√©gante de le faire est d'utiliser les pipelines de sklearn

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
import numpy as np

# Define the categorical and numerical features
cats = ['Workclass', 'Marital Status', 'Occupation',
'Relationship', 'Race', 'Sex', 'Country']

nums = ['Age', 'Education-Num', 'Capital Gain',
'Capital Loss', 'Hours per week']

# Define the column transformer
preprocessor = ColumnTransformer(
transformers=[
('cat', OneHotEncoder(), cats),
('num', StandardScaler(), nums)
])

# Define the pipeline
model = Pipeline([
('preprocessor', preprocessor),
('classifier', LogisticRegression(max_iter=10000))
])

import shap
from sklearn.model_selection import train_test_split
X, y = shap.datasets.adult()
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.2, random_state=1
)

# Fit the pipeline to the training data
model.fit(X_train, y_train)
X_sub = shap.sample(X_train, 100)
ex = shap.Explainer(model.predict_proba, X_sub)
shap_values = ex(X_test.iloc[0:100])


shap.plots.scatter(shap_values[:,0,1])
```

![1737496765137](image/local_model_angostic/1737496765137.png)

#### Shapley value et corr√©lation

Les shapley value sont √©galement sensibles aux corr√©lations de vos features!!

En effet, √† l'instar des Partial Depence Plot on "fixe" des variables tout en faisant varier la valeur des autres ce qui peut conduire √† des combinaisons irr√©alistes.

Ces combinaisons apparaissent lorsque des features sont corr√©l√©es, les solutions qui s'offrent √† vous pour r√©duire ce probl√®me sont les suivantes :

- M√©thode de feature selection qui √©limine les features corr√©l√©es
- Eliminer les features avec peu de variance
- Technique de r√©duction de dimension comme l'ACP (‚ö†Ô∏è attention cela complique l'interpr√©tabilit√©)
- Transformation de vos features pour casser le line lin√©aire (mise au carr√©, log, sqrt, ...)
- Combiner des features corr√©les
